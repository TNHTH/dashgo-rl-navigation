# train_cfg_v2.yaml
# [DashGo v5.0 Ultimate - 2026-01-25]
# 策略：低学习率(1.5e-4) + 自动课程(Auto-Curriculum) + 混合奖励(Hybrid Rewards)
# 特点：自动从3m扩展到8m，reach_goal 2000.0绝对主导，保留Dense奖励保底

seed: 42
device: 'cuda:0'

# 1. 运行环境设置
runner:
  policy_class_name: "ActorCritic"
  algorithm_class_name: "PPO"
  num_steps_per_env: 24       # 配合 4096 环境的大批量梯度更新
  max_iterations: 5000        # ✅ 5000轮：给足时间让课程从 3m 走到 8m 并收敛
  empirical_normalization: True # ✅ 必开：自动归一化观测数据
  save_interval: 100          # ✅ 每100轮保存，安全第一
  experiment_name: "dashgo_v5_auto"
  run_name: "v5_ultimate_auto"
  resume: False
  load_run: -1                # -1 表示加载最新的 checkpoint (如果 resume=True)
  checkpoint: -1

# 2. 策略网络 (Policy Network) - 模仿 NeuPAN 的复杂决策能力
policy:
  class_name: "ActorCritic"
  init_noise_std: 1.0         # ✅ 初始探索噪声（让机器人刚开始敢于乱动）
  actor_hidden_dims: [512, 256, 128] # ✅ [NeuPAN风格] 更深的网络，拟合复杂避障逻辑
  critic_hidden_dims: [512, 256, 128]
  activation: 'elu'           # ✅ [稳定性] 比 ReLU 更平滑，减少电机抖动指令

# 3. 算法超参数 (PPO Algorithm)
algorithm:
  class_name: "PPO"
  # 价值函数系数
  value_loss_coef: 1.0
  use_clipped_value_loss: True
  clip_param: 0.2

  # 熵系数 (Entropy) - 控制探索欲望
  # [架构师修正 2026-01-25] 再次降低熵系数，防止策略崩溃
  # 修改历史：0.02 → 0.01 → 0.005（减半，减少无意义随机抽搐）
  # 原理：让策略更专注，避免高频抖动
  entropy_coef: 0.005  # ⬇️ [架构师稳健版 2026-01-25] 再次减半（从 0.01 降到 0.005）

  # 学习参数
  num_learning_epochs: 5
  num_mini_batches: 4         # ⚠️ 显存如果不够(如<8GB)，可以调大这个数 (例如 8)
  learning_rate: 1.5e-4      # ⬇️ [架构师稳健版 2026-01-25] "慢就是快"（从 3e-4 降到 1.5e-4）
  schedule: "adaptive"        # ✅ [保险丝] 自适应调整学习率
  gamma: 0.99                 # 折扣因子 (关注长远利益)
  lam: 0.95
  desired_kl: 0.01            # 目标 KL 散度，超过这个值就降低学习率
  max_grad_norm: 1.0          # 梯度裁剪，防止梯度爆炸

# 4. 辅助设置
num_envs: 32                 # ✅ [v6.0修复] 统一配置（实际运行环境数，与命令行参数一致）