# train_cfg_v2.yaml
# [DashGo v5.0 Ultimate - 2026-01-25]
# 策略：低学习率(1.5e-4) + 自动课程(Auto-Curriculum) + 混合奖励(Hybrid Rewards)
# 特点：自动从3m扩展到8m，reach_goal 2000.0绝对主导，保留Dense奖励保底

seed: 42
device: 'cuda:0'

# 1. 运行环境设置
runner:
  policy_class_name: "ActorCritic"
  algorithm_class_name: "PPO"
  num_steps_per_env: 24       # 配合 4096 环境的大批量梯度更新
  max_iterations: 8000        # ✅ 8000轮：给足时间让策略充分收敛（用户要求）
  empirical_normalization: True # ✅ 必开：自动归一化观测数据
  save_interval: 100          # ✅ 每100轮保存，安全第一
  experiment_name: "dashgo_v5_auto"
  run_name: "v5_ultimate_auto"
  resume: False
  load_run: -1                # -1 表示加载最新的 checkpoint (如果 resume=True)
  checkpoint: -1

# 2. 策略网络 (Policy Network) - [方案2] 轻量化部署架构
# [架构师决策 2026-01-27] 直接训练1D-CNN+MLP，一步到位Sim2Real
policy:
  class_name: "GeoNavPolicy"  # ✅ 使用注入的轻量网络（替代ActorCritic）
  init_noise_std: 1.0

  # 这些参数会传给 GeoNavPolicy 的 __init__
  # CNN的参数已经在代码里写死了，这里主要控制全连接层
  actor_hidden_dims: [128, 64]   # ✅ 轻量级 Head（替代512,256,128）
  critic_hidden_dims: [512, 256, 128]  # ✅ [架构师建议] Critic保持强网络（不对称架构）
  activation: 'elu'

  # [架构师修复 2026-01-27] 移除GRU，改用MLP
  # 原因：GRU隐状态未传递导致"失忆"Bug，MLP更稳定
  # 优势：3帧历史堆叠已提供短时记忆，CNN+MLP足够
  # 参数量约300K（vs MLP 500K），适配Jetson Nano 4GB显存

# 3. 算法超参数 (PPO Algorithm)
algorithm:
  class_name: "PPO"
  # 价值函数系数
  value_loss_coef: 1.0
  use_clipped_value_loss: True
  clip_param: 0.2

  # 熵系数 (Entropy) - 控制探索欲望
  # [架构师修正 2026-01-27] 修复感知后，适当提高熵系数加快探索
  # 修改历史：0.02 → 0.01 → 0.005 → 0.01（回退到标准值）
  # 原理：感知已修复，需要更多探索避免局部最优
  entropy_coef: 0.01  # ✅ [架构师最终版 2026-01-27] 标准值（从 0.005 提高到 0.01）

  # 学习参数
  num_learning_epochs: 5
  num_mini_batches: 4         # ⚠️ 显存如果不够(如<8GB)，可以调大这个数 (例如 8)
  learning_rate: 3.0e-4      # ✅ [架构师最终版 2026-01-27] 标准值（从 1.5e-4 提高到 3e-4）
  schedule: "adaptive"        # ✅ [保险丝] 自适应调整学习率
  gamma: 0.99                 # 折扣因子 (关注长远利益)
  lam: 0.95
  desired_kl: 0.01            # 目标 KL 散度，超过这个值就降低学习率
  max_grad_norm: 1.0          # 梯度裁剪，防止梯度爆炸

# 4. 辅助设置
num_envs: 32                 # ✅ [v6.0修复] 统一配置（实际运行环境数，与命令行参数一致）