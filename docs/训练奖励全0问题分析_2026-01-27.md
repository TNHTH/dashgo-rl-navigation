# è®­ç»ƒå¥–åŠ±å…¨0é—®é¢˜åˆ†ææŠ¥å‘Š

> **åˆ›å»ºæ—¶é—´**: 2026-01-27 17:00:00
> **åˆ†æå¯¹è±¡**: DashGo RL Navigation - Geo-Distill V2.2
> **è®­ç»ƒè¿­ä»£**: ç¬¬44æ¬¡è¿­ä»£
> **ä¸¥é‡ç¨‹åº¦**: ğŸ”´ ä¸¥é‡ï¼ˆå¥–åŠ±ä¿¡å·ç¼ºå¤±å¯¼è‡´è®­ç»ƒæ— æ•ˆï¼‰
> **åˆ†æç›®æ ‡**: è¯†åˆ«å¥–åŠ±å…¨0çš„æ ¹æœ¬åŸå› ï¼Œæä¾›æ¶æ„å¸ˆæ”¹è¿›æ–¹æ¡ˆ

---

## 1. é—®é¢˜æè¿°

### 1.1 è®­ç»ƒæ—¥å¿—å¼‚å¸¸

**è§‚å¯Ÿåˆ°çš„ç°è±¡**ï¼ˆä»è®­ç»ƒæ—¥å¿— iteration 44ï¼‰ï¼š

```python
Episode_Reward/reach_goal: 0.0000        # âŒ åˆ°è¾¾ç›®æ ‡å¥–åŠ±ä¸º0
Episode_Reward/collision: 0.0000         # âŒ ç¢°æ’æƒ©ç½šä¸º0
Episode_Reward/undesired_contacts: 0.0000  # âŒ æ¥è§¦æƒ©ç½šä¸º0
Episode_Reward/unsafe_speed_penalty: 0.0000  # âŒ è¶…é€Ÿæƒ©ç½šä¸º0
Episode_Reward/alive_penalty: 0.0000     # âŒ ç”Ÿå­˜æƒ©ç½šä¸º0
Episode_Reward/log_distance: 0.0000      # âŒ è·ç¦»æ—¥å¿—ä¸º0
Episode_Reward/log_velocity: 0.0000      # âŒ é€Ÿåº¦æ—¥å¿—ä¸º0
Episode_Reward/out_of_bounds: 0.0000     # âŒ è¶Šç•Œæƒ©ç½šä¸º0

Terminations/reach_goal: 0.0000         # âŒ åˆ°è¾¾ç›®æ ‡ç»ˆæ­¢ä¸º0
Terminations/object_collision: 0.0000   # âŒ ç¢°æ’ç»ˆæ­¢ä¸º0
Terminations/out_of_bounds: 0.0000      # âŒ è¶Šç•Œç»ˆæ­¢ä¸º0
Terminations/base_height: 0.0000        # âŒ é«˜åº¦å¼‚å¸¸ä¸º0
Terminations/bad_velocity: 0.0000       # âŒ é€Ÿåº¦å¼‚å¸¸ä¸º0
```

**æ ¸å¿ƒé—®é¢˜**ï¼š
- **ç»å¤§å¤šæ•°å¥–åŠ±é¡¹ä¸º0.0**
- **åªæœ‰å°‘æ•°å¥–åŠ±é¡¹ï¼ˆå¦‚facing_targetï¼‰æœ‰éé›¶å€¼**
- **è®­ç»ƒä¿¡å·ä¸¥é‡ç¼ºå¤±**ï¼ŒPPOæ— æ³•å­¦ä¹ æœ‰æ•ˆç­–ç•¥

### 1.2 å½±å“è¯„ä¼°

**ä¸¥é‡æ€§åˆ†æ**ï¼š

| å½±å“ | ä¸¥é‡ç¨‹åº¦ | è¯´æ˜ |
|------|---------|------|
| ç­–ç•¥æ”¶æ•› | ğŸ”´ è‡´å‘½ | å¥–åŠ±ä¿¡å·ä¸è¶³ï¼ŒPPOæ— æ³•å­¦ä¹  |
| è®­ç»ƒæ•ˆç‡ | ğŸ”´ ä¸¥é‡ | å¤§é‡æ— æ•ˆè®¡ç®—ï¼Œæµªè´¹èµ„æº |
| è¡Œä¸ºå¯¼å‘ | ğŸ”´ ä¸¥é‡ | æœºå™¨äººå­¦ä¸åˆ°æ­£ç¡®è¡Œä¸º |
| Sim2Real | ğŸŸ¡ ä¸­ç­‰ | å³ä½¿è®­ç»ƒå®Œæˆï¼Œä¹Ÿæ— æ³•éƒ¨ç½² |

**ä¸ºä»€ä¹ˆå¿…é¡»ä¿®å¤**ï¼š
1. **å¥–åŠ±å‡½æ•°æ˜¯RLçš„"è€å¸ˆ"**ï¼šè€å¸ˆä¸è¯´è¯ï¼Œå­¦ç”Ÿå°±å­¦ä¸åˆ°ä¸œè¥¿
2. **ç¨€ç–å¥–åŠ±é—®é¢˜**ï¼šå¦‚æœåªæœ‰åˆ°è¾¾ç›®æ ‡æ‰æœ‰å¥–åŠ±ï¼Œæ¢ç´¢å¤ªéš¾
3. **è®­ç»ƒæ— æ•ˆ**ï¼šå½“å‰è®­ç»ƒåªæ˜¯æµªè´¹æ—¶é—´ï¼Œä¸ä¼šæ”¶æ•›

---

## 2. æ ¹æœ¬åŸå› åˆ†æ

### 2.1 é—®é¢˜åˆ†ç±»

å°†å…¨0å¥–åŠ±åˆ†ä¸º4ç±»ï¼š
1. **æƒé‡ä¸º0çš„é¡¹**ï¼ˆé…ç½®é—®é¢˜ï¼‰
2. **Headlessæ¨¡å¼é™åˆ¶**ï¼ˆç¯å¢ƒé—®é¢˜ï¼‰
3. **é˜ˆå€¼è¿‡äºä¸¥æ ¼**ï¼ˆå‚æ•°é—®é¢˜ï¼‰
4. **åŠŸèƒ½æœªå®ç°**ï¼ˆä»£ç é—®é¢˜ï¼‰

---

### 2.2 é…ç½®é—®é¢˜ï¼šæƒé‡ä¸º0.0

#### é—®é¢˜A: alive_penalty - æƒé‡ä¸º0

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:1306`

```python
# é…ç½®æ–‡ä»¶ä¸­çš„å®šä¹‰
alive_penalty = RewardTermCfg(func=reward_alive, weight=0.0)  # âŒ æƒé‡ä¸º0
```

**æ ¹æœ¬åŸå› **ï¼š
- é…ç½®ä¸­æ˜ç¡®è®¾ç½® `weight=0.0`
- å³ä½¿ `reward_alive()` å‡½æ•°è¿”å›-1.0ï¼Œä¹˜ä»¥0åä»ä¸º0
- **alive_penaltyå®Œå…¨å¤±æ•ˆ**

**å½±å“**ï¼š
- åŸæœ¬è®¾è®¡ï¼šæ¯å¸§æƒ©ç½š-1.0ï¼Œé€¼è¿«æœºå™¨äººåŠ¨èµ·æ¥
- å®é™…æ•ˆæœï¼šæœºå™¨äººå¯ä»¥åŸåœ°ä¸åŠ¨ï¼Œæ²¡æœ‰ç”Ÿå­˜å‹åŠ›

**å‡½æ•°å®ç°** (`dashgo_env_v2.py:729-731`):
```python
def reward_alive(env: ManagerBasedRLEnv) -> torch.Tensor:
    # è¿”å› -1.0 * æƒé‡
    return -REWARD_CONFIG["alive_penalty"] * torch.ones(env.num_envs, device=env.device)
```

**ä¸ºä»€ä¹ˆè®¾ç½®ä¸º0**ï¼Ÿ
- å¯èƒ½æ˜¯å†å²é—ç•™é…ç½®
- æˆ–è€…æ˜¯ä¸ºäº†è°ƒè¯•æŸä¸ªé—®é¢˜ä¸´æ—¶ç¦ç”¨
- å¿˜è®°æ¢å¤

---

#### é—®é¢˜B: log_velocity - æƒé‡ä¸º0

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:1318-1322`

```python
log_velocity = RewardTermCfg(
    func=log_linear_velocity,
    weight=0.0,  # âŒ æƒé‡ä¸º0
    params={"asset_cfg": SceneEntityCfg("robot")}
)
```

**æ ¹æœ¬åŸå› **ï¼š
- æ—¥å¿—é¡¹ä¸éœ€è¦æƒé‡ï¼ˆåªç”¨äºè®°å½•ï¼Œä¸å‚ä¸æ€»å¥–åŠ±ï¼‰
- ä½†weight=0.0å¯¼è‡´æ—¥å¿—ä¹Ÿä¸º0.0

**å½±å“**ï¼š
- æ— æ³•è§‚æµ‹é€Ÿåº¦å˜åŒ–
- æ— æ³•åˆ¤æ–­æœºå™¨äººæ˜¯å¦åœ¨ç§»åŠ¨

---

### 2.3 Headlessæ¨¡å¼é™åˆ¶ï¼šreward_collisionç¡¬ç¼–ç ä¸º0

#### é—®é¢˜C: collisionæƒ©ç½š - Headlessæ¨¡å¼ç¦ç”¨

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:674-686`

```python
# [4] é¿éšœæƒ©ç½š
# [å…¼å®¹] headless æ¨¡å¼ä¸‹ä¼ æ„Ÿå™¨ä¸å­˜åœ¨ï¼Œè·³è¿‡é¿éšœæƒ©ç½š
if sensor_cfg is not None:
    depth_radial = _get_corrected_depth(env, sensor_cfg)
    min_dist = torch.min(depth_radial, dim=-1)[0]
    safe_dist = REWARD_CONFIG["safe_distance"]
    reward_collision = torch.zeros_like(min_dist)
    mask_danger = min_dist < safe_dist
    reward_collision[mask_danger] = -REWARD_CONFIG["collision_penalty"] * torch.exp(
        REWARD_CONFIG["collision_decay"] * (safe_dist - min_dist[mask_danger])
    )
else:
    # headless æ¨¡å¼ï¼šæ²¡æœ‰ä¼ æ„Ÿå™¨æ•°æ®ï¼Œä½¿ç”¨é›¶é¿éšœæƒ©ç½š
    reward_collision = torch.zeros(forward_vel.shape, device=env.device)  # âŒ ç¡¬ç¼–ç ä¸º0
```

**æ ¹æœ¬åŸå› **ï¼š
- Headlessæ¨¡å¼ä¸‹æ²¡æœ‰ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆsensor_cfg is Noneï¼‰
- ä»£ç ç›´æ¥è¿”å› `torch.zeros(...)`
- **collisionæƒ©ç½šå®Œå…¨å¤±æ•ˆ**

**ä¸ºä»€ä¹ˆä¼ æ„Ÿå™¨ä¸ºNone**ï¼š
- Headlessæ¨¡å¼å¯èƒ½ç¦ç”¨äº†ç›¸æœº/æ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨
- æˆ–è€…ä¼ æ„Ÿå™¨é…ç½®æ–¹å¼ä¸åŒ

**å½±å“**ï¼š
- æœºå™¨äººæ²¡æœ‰é¿éšœæƒ©ç½šä¿¡å·
- å¯ä»¥éšæ„ç©¿è¶Šéšœç¢ç‰©è€Œä¸å—æƒ©ç½š
- **è®­ç»ƒå‡ºçš„ç­–ç•¥æ— æ³•éƒ¨ç½²åˆ°å®ç‰©**

---

### 2.4 é˜ˆå€¼è¿‡äºä¸¥æ ¼ï¼šæƒ©ç½šé¡¹éš¾ä»¥è§¦å‘

#### é—®é¢˜D: undesired_contacts - é˜ˆå€¼è¿‡ä½

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:432-467` (å‡½æ•°å®šä¹‰), `dashgo_env_v2.py:1286-1293` (é…ç½®)

```python
# å‡½æ•°å®šä¹‰
def penalty_undesired_contacts(env: ManagerBasedRLEnv, sensor_cfg: SceneEntityCfg, threshold: float = 0.1) -> torch.Tensor:
    """è½»å¾®æ¥è§¦æƒ©ç½š - ç¬¬äºŒå±‚é˜²å¾¡"""
    contact_data = env.scene[sensor_cfg.name].data.net_forces_w  # [N, num_bodies, 3]
    force_mag = torch.norm(contact_data, dim=-1).max(dim=1)[0]  # [N]
    has_contact = force_mag > threshold  # threshold=0.1N
    return -torch.where(has_contact, 1.0, 0.0)

# é…ç½®
undesired_contacts = RewardTermCfg(
    func=penalty_undesired_contacts,
    weight=-1.0,
    params={
        "sensor_cfg": SceneEntityCfg("contact_forces_base"),
        "threshold": 0.1  # âŒ 0.1ç‰›é¡¿ï¼Œæä½çš„é˜ˆå€¼
    }
)
```

**æ ¹æœ¬åŸå› **ï¼š
- **é˜ˆå€¼0.1Nå¤ªä½**ï¼Œå‡ ä¹ä¸ä¼šè§¦å‘
- æ­£å¸¸è¡Œèµ°æ—¶çš„æ¥è§¦åŠ›å¯èƒ½å°±è¶…è¿‡0.1N
- æˆ–è€…ç›¸åï¼Œæ¥è§¦åŠ›ä»æœªè¾¾åˆ°0.1Nï¼ˆä¼ æ„Ÿå™¨é—®é¢˜ï¼‰

**åˆ†æ**ï¼š
- 0.1N â‰ˆ 10å…‹é‡é‡
- æœºå™¨äººé‡é‡çº¦20kgï¼Œè½»å¾®ç¢°æ’åŠ›è¿œå¤§äº0.1N
- å¦‚æœä¸º0ï¼Œå¯èƒ½æ˜¯ï¼š
  1. æ¥è§¦åŠ›ä¼ æ„Ÿå™¨æœªæ­£å¸¸å·¥ä½œ
  2. ç›¸æœºæ•°æ®é—®é¢˜å¯¼è‡´æœªæ­£ç¡®æ£€æµ‹ç¢°æ’

---

#### é—®é¢˜E: unsafe_speed_penalty - è·ç¦»é˜ˆå€¼è¿‡ä¸¥

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:376-429` (å‡½æ•°å®šä¹‰), `dashgo_env_v2.py:1295-1304` (é…ç½®)

```python
# å‡½æ•°å®šä¹‰
def penalty_unsafe_speed(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg, min_dist_threshold: float = 0.25) -> torch.Tensor:
    """é€Ÿåº¦-è·ç¦» åŠ¨æ€çº¦æŸ"""
    # è·å–ç›¸æœºæ•°æ®
    d_front = env.scene["camera_front"].data.output["distance_to_image_plane"]
    d_left = env.scene["camera_left"].data.output["distance_to_image_plane"]
    d_back = env.scene["camera_back"].data.output["distance_to_image_plane"]
    d_right = env.scene["camera_right"].data.output["distance_to_image_plane"]

    # æ‹¼æ¥å¹¶å±•å¹³
    all_pixels = torch.cat([d_front, d_left, d_back, d_right], dim=1).view(batch_size, -1)
    all_pixels = torch.nan_to_num(all_pixels, posinf=12.0)
    min_dist = torch.min(all_pixels, dim=1)[0]

    # è·å–é€Ÿåº¦
    vel = env.scene[asset_cfg.name].data.root_lin_vel_b[:, 0]

    # è®¡ç®—æƒ©ç½šï¼šè·ç¦» < 0.25m æ—¶ï¼Œé™åˆ¶æœ€å¤§é€Ÿåº¦
    safe_vel_limit = torch.clamp(min_dist, max=0.5)
    overspeed = torch.clamp(vel - safe_vel_limit, min=0.0)
    return -overspeed

# é…ç½®
unsafe_speed_penalty = RewardTermCfg(
    func=penalty_unsafe_speed,
    weight=-5.0,
    params={
        "asset_cfg": SceneEntityCfg("robot"),
        "min_dist_threshold": 0.25  # âŒ 0.25må®‰å…¨è·ç¦»
    }
)
```

**æ ¹æœ¬åŸå› **ï¼š
- **éœ€è¦åŒæ—¶æ»¡è¶³**ï¼š
  1. è·ç¦» < 0.25mï¼ˆå¾ˆè¿‘ï¼‰
  2. é€Ÿåº¦ > 0.25m/sï¼ˆè¾ƒå¿«ï¼‰
- è¿™ç§æƒ…å†µå¯èƒ½å¾ˆå°‘å‘ç”Ÿï¼Œæ‰€ä»¥æƒ©ç½šä¸º0

**åˆ†æ**ï¼š
- æœºå™¨äººåŠå¾„çº¦0.2mï¼ŒåŠ ä¸Š0.05mä½™é‡ = 0.25m
- å‡ ä¹è´´åˆ°éšœç¢ç‰©æ‰ä¼šè§¦å‘
- å¯èƒ½æœºå™¨äººè®­ç»ƒç­–ç•¥å·²ç»é¿å¼€äº†è¿™ä¹ˆè¿‘çš„è·ç¦»

---

#### é—®é¢˜F: out_of_bounds - åœºåœ°è¾¹ç•Œè¿‡å¤§

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:1324-1328`

```python
out_of_bounds = RewardTermCfg(
    func=penalty_out_of_bounds,
    weight=-200.0,
    params={"threshold": 8.0, "asset_cfg": SceneEntityCfg("robot")}  # âŒ 8.0mè¾¹ç•Œ
)
```

**æ ¹æœ¬åŸå› **ï¼š
- **é˜ˆå€¼8.0mè¿‡å¤§**
- å¦‚æœåœºåœ°åŠå¾„åªæœ‰3-5mï¼Œæœºå™¨äººæ°¸è¿œä¸ä¼šè¶Šç•Œ
- æƒ©ç½šæ°¸è¿œä¸º0

**éœ€è¦éªŒè¯**ï¼š
- åœºåœ°å®é™…å°ºå¯¸æ˜¯å¤šå°‘ï¼Ÿ
- 8.0mæ˜¯å¦åˆç†ï¼Ÿ

---

### 2.5 åŠŸèƒ½æœªå®ç°/è°ƒè¯•é¡¹

#### é—®é¢˜G: reach_goal - é˜ˆå€¼è¿‡ä¸¥

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:1335-1342`

```python
reach_goal = TerminationTermCfg(
    func=check_reach_goal,
    params={
        "command_name": "target_pose",
        "threshold": 0.5,  # âŒ 0.5mé˜ˆå€¼
        "asset_cfg": SceneEntityCfg("robot")
    }
)
```

**æ ¹æœ¬åŸå› **ï¼š
- **0.5mé˜ˆå€¼å¯èƒ½è¿‡ä¸¥**
- å¯¹äºå±€éƒ¨å¯¼èˆªï¼Œ0.5må·²ç»éå¸¸è¿‘
- æœºå™¨äººå¯èƒ½éš¾ä»¥ç²¾ç¡®åˆ°è¾¾

**åˆ†æ**ï¼š
- å¦‚æœæœºå™¨äººåŠå¾„0.2mï¼Œç›®æ ‡ç‚¹åŠå¾„0.3mï¼Œå®é™…è·ç¦»è¦æ±‚0.5m
- å¯èƒ½éœ€è¦é™ä½é˜ˆå€¼åˆ°1.0m

---

#### é—®é¢˜H: log_distance - æƒé‡è¿‡å°

**ä»£ç ä½ç½®**: `dashgo_env_v2.py:1309-1316`

```python
log_distance = RewardTermCfg(
    func=log_distance_to_goal,
    weight=1e-6,  # âŒ æƒé‡è¿‡å°ï¼ˆ0.000001ï¼‰
    params={
        "command_name": "target_pose",
        "asset_cfg": SceneEntityCfg("robot")
    }
)
```

**æ ¹æœ¬åŸå› **ï¼š
- **æƒé‡1e-6å¤ªå°**
- å³ä½¿è·ç¦»10mï¼Œå¥–åŠ±ä¹Ÿåªæœ‰0.01
- å‡ ä¹æ²¡æœ‰å¼•å¯¼ä½œç”¨

**å½±å“**ï¼š
- æ—¥å¿—é¡¹ä¸å‚ä¸æ€»å¥–åŠ±ï¼Œæ‰€ä»¥weightå¾ˆå°æ˜¯æ­£å¸¸çš„
- ä½†å¦‚æœçœŸçš„è¦ä½œä¸ºå¼•å¯¼ä¿¡å·ï¼Œæƒé‡åº”è¯¥æ›´å¤§ï¼ˆå¦‚1.0ï¼‰

---

## 3. ç›¸å…³ä»£ç æ±‡æ€»

### 3.1 å¥–åŠ±å‡½æ•°é…ç½®ï¼ˆå…¨éƒ¨ï¼‰

| å¥–åŠ±é¡¹ | ä½ç½® | æƒé‡ | é˜ˆå€¼ | çŠ¶æ€ | é—®é¢˜ |
|--------|------|------|------|------|------|
| **ä¸»è¦å¥–åŠ±** ||||||
| progress_rewards | 1273 | 1.0 | - | âœ… æ­£å¸¸ | æ—  |
| distance_tracking | 1277 | 2.0 | - | âœ… æ­£å¸¸ | æ—  |
| facing_target | 1280 | 0.5 | - | âœ… æ­£å¸¸ | æ—  |
| **æƒ©ç½šé¡¹** ||||||
| collision | 674-686 | -2.0 | safe_dist=0.4 | âŒ **ä¸º0** | Headlessæ¨¡å¼ç¦ç”¨ |
| undesired_contacts | 1286 | -1.0 | **0.1N** | âŒ **ä¸º0** | é˜ˆå€¼è¿‡ä½ |
| unsafe_speed | 1295 | -5.0 | **0.25m** | âŒ **ä¸º0** | é˜ˆå€¼è¿‡ä¸¥ |
| alive_penalty | 1306 | **0.0** | - | âŒ **ä¸º0** | æƒé‡ä¸º0 |
| out_of_bounds | 1324 | -200.0 | **8.0m** | âŒ **ä¸º0** | è¾¹ç•Œè¿‡å¤§ |
| **æ—¥å¿—é¡¹** ||||||
| log_distance | 1309 | 1e-6 | - | âŒ **ä¸º0** | æƒé‡å¤ªå° |
| log_velocity | 1318 | **0.0** | - | âŒ **ä¸º0** | æƒé‡ä¸º0 |

### 3.2 ç»ˆæ­¢æ¡ä»¶é…ç½®ï¼ˆå…¨éƒ¨ï¼‰

| ç»ˆæ­¢é¡¹ | ä½ç½® | é˜ˆå€¼ | çŠ¶æ€ | é—®é¢˜ |
|--------|------|------|------|------|
| time_out | 1332 | - | âœ… æ­£å¸¸ | æ—  |
| reach_goal | 1335 | **0.5m** | âŒ **ä¸º0** | é˜ˆå€¼è¿‡ä¸¥ |
| object_collision | 1344 | **50N** | âŒ **ä¸º0** | é˜ˆå€¼è¿‡é«˜æˆ–ä¼ æ„Ÿå™¨é—®é¢˜ |
| out_of_bounds | 1348 | **8.0m** | âŒ **ä¸º0** | è¾¹ç•Œè¿‡å¤§ |
| base_height | 1349 | -0.5~1.0m | âŒ **ä¸º0** | æ­£å¸¸æƒ…å†µä¸è§¦å‘ |
| bad_velocity | 1350 | **50.0m/s** | âŒ **ä¸º0** | é˜ˆå€¼è¿‡é«˜ï¼ˆå¼‚å¸¸é«˜ï¼‰ |

---

## 4. æ½œåœ¨è§£å†³æ–¹æ¡ˆï¼ˆä¾›æ¶æ„å¸ˆå‚è€ƒï¼‰

### 4.1 ç«‹å³ä¿®å¤ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

#### æ–¹æ¡ˆ1: æ¢å¤alive_penaltyæƒé‡

**é—®é¢˜**: `alive_penalty = RewardTermCfg(func=reward_alive, weight=0.0)`

**å»ºè®®**:
```python
# ä¿®æ”¹å‰
alive_penalty = RewardTermCfg(func=reward_alive, weight=0.0)

# ä¿®æ”¹åï¼ˆç¤ºä¾‹ï¼‰
alive_penalty = RewardTermCfg(func=reward_alive, weight=-0.05)  # è½»å¾®ç”Ÿå­˜å‹åŠ›
```

**ç†ç”±**:
- é€¼è¿«æœºå™¨äººåŠ¨èµ·æ¥
- -0.05çš„æƒé‡ä¸ä¼šè¿‡å¼ºï¼Œé¿å…å…¶ä»–å¥–åŠ±è¢«æ·¹æ²¡

---

#### æ–¹æ¡ˆ2: ä¿®å¤Headlessæ¨¡å¼collisionæƒ©ç½š

**é—®é¢˜**: Headlessæ¨¡å¼ä¸‹ç¡¬ç¼–ç ä¸º0

**å»ºè®®æ–¹æ¡ˆ**ï¼ˆå¤šç§é€‰æ‹©ï¼‰:

**é€‰é¡¹A**: ä½¿ç”¨ç›¸æœºæ•°æ®æ›¿ä»£æ¿€å…‰é›·è¾¾
```python
# ä¿®æ”¹å‰
if sensor_cfg is not None:
    depth_radial = _get_corrected_depth(env, sensor_cfg)
    ...
else:
    reward_collision = torch.zeros(forward_vel.shape, device=env.device)  # âŒ

# ä¿®æ”¹åï¼ˆä¼ªä»£ç ï¼‰
if sensor_cfg is not None:
    depth_radial = _get_corrected_depth(env, sensor_cfg)
    ...
else:
    # Headlessæ¨¡å¼ï¼šä½¿ç”¨ç›¸æœºè·ç¦»æ•°æ®
    min_dist = get_min_distance_from_cameras(env)  # æ–°å‡½æ•°
    reward_collision = compute_collision_penalty(min_dist)
```

**é€‰é¡¹B**: ç§»é™¤sensor_cfgä¾èµ–
```python
# å§‹ç»ˆä½¿ç”¨ç›¸æœºæ•°æ®ï¼ˆæ— è®ºæ˜¯å¦headlessï¼‰
depth_radial = _get_camera_depth(env)  # ç»Ÿä¸€æ¥å£
reward_collision = compute_collision_penalty(depth_radial)
```

**é€‰é¡¹C**: ä¸´æ—¶ç¦ç”¨collisionæƒ©ç½šï¼ˆä¸æ¨èï¼‰
- å®Œå…¨ä¾èµ–terminationçš„collisionæ£€æµ‹
- é£é™©ï¼šç¼ºå°‘æ¢¯åº¦ä¿¡å·

---

#### æ–¹æ¡ˆ3: è°ƒæ•´undesired_contactsé˜ˆå€¼

**é—®é¢˜**: `threshold: 0.1` å¤ªä½

**å»ºè®®**:
```python
# ä¿®æ”¹å‰
undesired_contacts = RewardTermCfg(
    func=penalty_undesired_contacts,
    weight=-1.0,
    params={
        "sensor_cfg": SceneEntityCfg("contact_forces_base"),
        "threshold": 0.1  # âŒ å¤ªä½
    }
)

# ä¿®æ”¹åï¼ˆç¤ºä¾‹ï¼‰
undesired_contacts = RewardTermCfg(
    func=penalty_undesired_contacts,
    weight=-1.0,
    params={
        "sensor_cfg": SceneEntityCfg("contact_forces_base"),
        "threshold": 5.0  # â¬†ï¸ æé«˜åˆ°5Nï¼ˆè½»å¾®ä½†å¯æ£€æµ‹ï¼‰
    }
)
```

**ç†ç”±**:
- 5N â‰ˆ 500å…‹é‡é‡ï¼Œè¶³ä»¥æ£€æµ‹è½»å¾®ç¢°æ’
- ä¸ä¼šè¯¯è§¦æ­£å¸¸è¡Œèµ°éœ‡åŠ¨

---

### 4.2 å‚æ•°ä¼˜åŒ–ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

#### æ–¹æ¡ˆ4: æ”¾å®½reach_goalé˜ˆå€¼

**é—®é¢˜**: `threshold: 0.5` è¿‡ä¸¥

**å»ºè®®**:
```python
# ä¿®æ”¹å‰
reach_goal = TerminationTermCfg(
    func=check_reach_goal,
    params={"threshold": 0.5, ...}  # âŒ 0.5m
)

# ä¿®æ”¹åï¼ˆç¤ºä¾‹ï¼‰
reach_goal = TerminationTermCfg(
    func=check_reach_goal,
    params={"threshold": 1.0, ...}  # â¬†ï¸ 1.0mï¼ˆæ›´å®½æ¾ï¼‰
)
```

**ç†ç”±**:
- 1må¯¹äºå±€éƒ¨å¯¼èˆªå·²ç»è¶³å¤Ÿç²¾ç¡®
- é™ä½ä»»åŠ¡éš¾åº¦ï¼ŒåŠ é€Ÿè®­ç»ƒ

---

#### æ–¹æ¡ˆ5: è°ƒæ•´out_of_boundsè¾¹ç•Œ

**é—®é¢˜**: `threshold: 8.0` å¯èƒ½è¿‡å¤§

**å»ºè®®**:
- **é¦–å…ˆéªŒè¯**: åœºåœ°å®é™…åŠå¾„æ˜¯å¤šå°‘ï¼Ÿ
- **ç„¶åè°ƒæ•´**: å¦‚æœåœºåœ°åŠå¾„5mï¼Œè®¾ç½®è¾¹ç•Œä¸º6m

**ç¤ºä¾‹**:
```python
# ä¿®æ”¹å‰
out_of_bounds = RewardTermCfg(
    func=penalty_out_of_bounds,
    weight=-200.0,
    params={"threshold": 8.0, ...}  # âŒ å¯èƒ½è¿‡å¤§
)

# ä¿®æ”¹åï¼ˆæ ¹æ®åœºåœ°å®é™…å¤§å°ï¼‰
out_of_bounds = RewardTermCfg(
    func=penalty_out_of_bounds,
    weight=-200.0,
    params={"threshold": 6.0, ...}  # â¬‡ï¸ å¦‚æœåœºåœ°åŠå¾„5m
)
```

---

### 4.3 è°ƒè¯•å»ºè®®ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

#### æ–¹æ¡ˆ6: æ·»åŠ è°ƒè¯•æ—¥å¿—

**ç›®çš„**: ç¡®è®¤å„å‡½æ•°æ˜¯å¦è¢«è°ƒç”¨ï¼Œè¿”å›å€¼æ˜¯ä»€ä¹ˆ

**ç¤ºä¾‹**:
```python
def penalty_undesired_contacts(env, sensor_cfg, threshold=0.1):
    contact_data = env.scene[sensor_cfg.name].data.net_forces_w
    force_mag = torch.norm(contact_data, dim=-1).max(dim=1)[0]

    # [è°ƒè¯•] æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if env.common_step_counter % 100 == 0:
        print(f"[penalty_undesired_contacts] max_force: {force_mag.max().item():.4f}, mean_force: {force_mag.mean().item():.4f}, above_threshold: {(force_mag > threshold).sum().item()}/{len(force_mag)}")

    has_contact = force_mag > threshold
    return -torch.where(has_contact, 1.0, 0.0)
```

---

## 5. éªŒè¯æ–¹æ³•

### 5.1 ä¿®å¤åéªŒè¯æ¸…å•

```
â–¡ alive_penalty æƒé‡æ”¹ä¸ºé0å€¼
â–¡ Headlessæ¨¡å¼collisionæƒ©ç½šä¸å†ä¸º0
â–¡ undesired_contactsé˜ˆå€¼æé«˜ï¼ˆå¦‚5Nï¼‰
â–¡ reach_goalé˜ˆå€¼æ”¾å®½ï¼ˆå¦‚1.0mï¼‰
â–¡ out_of_boundsè¾¹ç•Œè°ƒæ•´ä¸ºåœºåœ°å®é™…å¤§å°
â–¡ å„å‡½æ•°æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œç¡®è®¤è¢«è°ƒç”¨
â–¡ é‡æ–°è®­ç»ƒï¼Œè§‚å¯Ÿå¥–åŠ±é¡¹æ˜¯å¦é0
â–¡ æ£€æŸ¥Episode_Rewardå„é¡¹æ˜¯å¦æ­£å¸¸
â–¡ æ£€æŸ¥Terminationså„é¡¹æ˜¯å¦è§¦å‘
â–¡ è®­ç»ƒæ˜¯å¦æ”¶æ•›ï¼ˆRewardæ›²çº¿ä¸Šå‡ï¼‰
```

### 5.2 è®­ç»ƒç›‘æ§æŒ‡æ ‡

**å…³é”®æŒ‡æ ‡**ï¼ˆä¿®å¤ååº”è¯¥çœ‹åˆ°ï¼‰ï¼š

| æŒ‡æ ‡ | ä¿®å¤å‰ | ä¿®å¤åé¢„æœŸ |
|------|--------|-----------|
| Episode_Reward/alive_penalty | 0.0000 | < -0.5ï¼ˆæ¯å¸§æƒ©ç½šï¼‰ |
| Episode_Reward/collision | 0.0000 | < -0.1ï¼ˆæ¥è¿‘éšœç¢æ—¶ï¼‰ |
| Episode_Reward/undesired_contacts | 0.0000 | < -1.0ï¼ˆç¢°æ’æ—¶ï¼‰ |
| Episode_Reward/unsafe_speed_penalty | 0.0000 | < -0.5ï¼ˆè¶…é€Ÿæ—¶ï¼‰ |
| Terminations/reach_goal | 0.0000 | > 0.0ï¼ˆæœ‰æ—¶èƒ½åˆ°è¾¾ï¼‰ |
| Terminations/object_collision | 0.0000 | > 0.0ï¼ˆç¢°æ’æ—¶é‡ç½®ï¼‰ |

---

## 6. æ€»ç»“

### 6.1 é—®é¢˜ä¸¥é‡æ€§è¯„ä¼°

| é—®é¢˜ç±»åˆ« | å½±å“èŒƒå›´ | ä¸¥é‡ç¨‹åº¦ | ä¿®å¤ä¼˜å…ˆçº§ |
|---------|---------|---------|-----------|
| alive_penaltyæƒé‡ä¸º0 | è®­ç»ƒæ•ˆç‡ | ğŸ”´ é«˜ | P0ï¼ˆç«‹å³ï¼‰ |
| Headless collisionç¦ç”¨ | é¿éšœèƒ½åŠ› | ğŸ”´ é«˜ | P0ï¼ˆç«‹å³ï¼‰ |
| undesired_contactsé˜ˆå€¼è¿‡ä½ | è¡Œä¸ºå¡‘é€  | ğŸŸ¡ ä¸­ | P1ï¼ˆå°½å¿«ï¼‰ |
| reach_goalé˜ˆå€¼è¿‡ä¸¥ | ä»»åŠ¡éš¾åº¦ | ğŸŸ¡ ä¸­ | P1ï¼ˆå°½å¿«ï¼‰ |
| out_of_boundsè¾¹ç•Œè¿‡å¤§ | åœºåœ°çº¦æŸ | ğŸŸ¢ ä½ | P2ï¼ˆå¯é€‰ï¼‰ |

### 6.2 æ ¸å¿ƒç»“è®º

**ä¸ºä»€ä¹ˆå¥–åŠ±å…¨0**ï¼š
1. **é…ç½®é—®é¢˜**: alive_penaltyå’Œlog_velocityæƒé‡ä¸º0
2. **Headlessé™åˆ¶**: collisionæƒ©ç½šåœ¨æ— GUIæ¨¡å¼ä¸‹ç¡¬ç¼–ç ä¸º0
3. **é˜ˆå€¼é—®é¢˜**: undesired_contactsã€unsafe_speedç­‰é¡¹é˜ˆå€¼è¿‡ä¸¥/è¿‡ä½ï¼Œéš¾ä»¥è§¦å‘
4. **è®¾è®¡é—®é¢˜**: out_of_boundsã€reach_goalç­‰é¡¹é˜ˆå€¼å¯èƒ½ä¸ç¬¦åˆå®é™…åœºæ™¯

**å»ºè®®ä¿®å¤é¡ºåº**ï¼š
1. **P0**ï¼ˆç«‹å³ä¿®å¤ï¼‰:
   - æ¢å¤alive_penaltyæƒé‡ï¼ˆ0â†’-0.05ï¼‰
   - ä¿®å¤Headlessæ¨¡å¼collisionæƒ©ç½šï¼ˆæ”¹ç”¨ç›¸æœºæ•°æ®ï¼‰

2. **P1**ï¼ˆå°½å¿«ä¿®å¤ï¼‰:
   - è°ƒæ•´undesired_contactsé˜ˆå€¼ï¼ˆ0.1Nâ†’5Nï¼‰
   - æ”¾å®½reach_goalé˜ˆå€¼ï¼ˆ0.5mâ†’1.0mï¼‰
   - é™ä½unsafe_speedè·ç¦»é˜ˆå€¼ï¼ˆ0.25mâ†’0.5mï¼‰

3. **P2**ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰:
   - è°ƒæ•´out_of_boundsè¾¹ç•Œï¼ˆæ ¹æ®åœºåœ°å®é™…å¤§å°ï¼‰
   - æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œç›‘æ§å„å‡½æ•°è°ƒç”¨

### 6.3 é£é™©æç¤º

**å¦‚æœå®Œå…¨ä¸ä¿®å¤**ï¼š
- âŒ è®­ç»ƒæ— æ³•æ”¶æ•›ï¼ˆå¥–åŠ±ä¿¡å·ç¼ºå¤±ï¼‰
- âŒ æµªè´¹è®¡ç®—èµ„æºï¼ˆæ— æ•ˆè®­ç»ƒï¼‰
- âŒ æ— æ³•éƒ¨ç½²åˆ°å®ç‰©ï¼ˆç­–ç•¥å­¦ä¸åˆ°é¿éšœï¼‰

**å¦‚æœåªä¿®å¤éƒ¨åˆ†**ï¼š
- ğŸŸ¡ alive_penalty: æœºå™¨äººä¼šåŠ¨èµ·æ¥ï¼Œä½†å¯èƒ½ä¹±æ’
- ğŸŸ¡ collision: æœºå™¨äººå­¦ä¼šé¿éšœï¼Œä½†å¯èƒ½åŸåœ°ä¸åŠ¨
- âœ… ä¸¤è€…éƒ½ä¿®å¤: æœºå™¨äººå­¦ä¼š"æ—¢åŠ¨åˆé¿"

---

## é™„å½•A: å®Œæ•´ä»£ç ä½ç½®ç´¢å¼•

### å¥–åŠ±å‡½æ•°å®ç°

| å‡½æ•°å | ä½ç½® | ç”¨é€” |
|--------|------|------|
| `reward_progress` | 632-696è¡Œ | å‰è¿›è¿›åº¦å¥–åŠ±ï¼ˆä¸»å¥–åŠ±ï¼‰ |
| `reward_distance_tracking_potential` | 699-710è¡Œ | åŠ¿èƒ½å·®å¼•å¯¼å¥–åŠ± |
| `reward_facing_target` | 713-726è¡Œ | å¯¹å‡†ç›®æ ‡å¥–åŠ± |
| `reward_alive` | 729-731è¡Œ | ç”Ÿå­˜æƒ©ç½š |
| `penalty_unsafe_speed` | 376-429è¡Œ | é€Ÿåº¦-è·ç¦»çº¦æŸ |
| `penalty_undesired_contacts` | 432-467è¡Œ | è½»å¾®æ¥è§¦æƒ©ç½š |
| `penalty_out_of_bounds` | ï¼ˆéœ€æŸ¥æ‰¾ï¼‰ | è¶Šç•Œæƒ©ç½š |
| `reward_near_goal` | 757+è¡Œ | åˆ°è¾¾ç›®æ ‡å¥–åŠ± |

### å¥–åŠ±é…ç½®

| é…ç½®é¡¹ | ä½ç½® | æƒé‡ |
|--------|------|------|
| `progress_rewards` | 1273è¡Œ | 1.0 |
| `distance_tracking` | 1277è¡Œ | 2.0 |
| `facing_target` | 1280è¡Œ | 0.5 |
| `undesired_contacts` | 1286è¡Œ | -1.0 |
| `unsafe_speed_penalty` | 1295è¡Œ | -5.0 |
| `alive_penalty` | 1306è¡Œ | **0.0** âŒ |
| `log_distance` | 1309è¡Œ | 1e-6 |
| `log_velocity` | 1318è¡Œ | **0.0** âŒ |
| `out_of_bounds` | 1324è¡Œ | -200.0 |

### ç»ˆæ­¢æ¡ä»¶é…ç½®

| é…ç½®é¡¹ | ä½ç½® | é˜ˆå€¼ |
|--------|------|------|
| `time_out` | 1332è¡Œ | - |
| `reach_goal` | 1335è¡Œ | **0.5m** â“ |
| `object_collision` | 1344è¡Œ | 50N |
| `out_of_bounds` | 1348è¡Œ | **8.0m** â“ |
| `base_height` | 1349è¡Œ | -0.5~1.0m |
| `bad_velocity` | 1350è¡Œ | 50m/s |

---

## é™„å½•B: æµ‹è¯•è„šæœ¬å»ºè®®

**æµ‹è¯•å„å¥–åŠ±é¡¹æ˜¯å¦å·¥ä½œ**ï¼š

```python
# ä¸´æ—¶æµ‹è¯•è„šæœ¬ï¼štest_rewards.py
import torch
from dashgo_env_v2 import *

# åˆ›å»ºç¯å¢ƒ
env = manager_based_rl_loader()

# é‡ç½®
obs, info = env.reset()

# æ‰§è¡Œ100æ­¥
for step in range(100):
    actions = env.action_manager.action.unsqueeze(0)  # é›¶åŠ¨ä½œ
    obs, reward, terminated, truncated, info = env.step(actions)

    # æ‰“å°å„é¡¹å¥–åŠ±
    if step % 10 == 0:
        print(f"Step {step}:")
        for key, value in info["episode"].items():
            if "Reward" in key or "Terminations" in key:
                print(f"  {key}: {value.item():.4f}")

    if terminated.any() or truncated.any():
        break
```

---

**æ–‡æ¡£ç»´æŠ¤**:
- **åˆ›å»ºè€…**: Claude Code AI Assistant
- **ç”¨é€”**: æ¶æ„å¸ˆå®¡æŸ¥å’Œæ”¹è¿›
- **çŠ¶æ€**: ç­‰å¾…æ¶æ„å¸ˆè¯„ä¼°
- **ä¸‹ä¸€æ­¥**: æ ¹æ®æ¶æ„å¸ˆå†³ç­–å®æ–½ä¿®å¤æ–¹æ¡ˆ
