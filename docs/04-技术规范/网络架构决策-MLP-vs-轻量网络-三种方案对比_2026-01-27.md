# 网络架构决策文档：MLP vs 轻量网络部署方案

> **创建时间**: 2026-01-27 19:00:00
> **文档目的**: 架构师决策参考
> **严重程度**: 🔴 关键决策（影响整个项目方向）
> **状态**: 等待架构师裁决

---

## 📋 目录

1. [问题根源](#问题根源)
2. [当前现状](#当前现状)
3. [三种方案详解](#三种方案详解)
4. [方案对比](#方案对比)
5. [执行方案清单](#执行方案清单)
6. [决策建议](#决策建议)

---

## 问题根源

### 为什么会出现这个选择？

**核心矛盾**：
- **训练阶段**：RSL-RL框架默认使用MLP网络（Actor-Critic）
- **部署阶段**：需要轻量网络（适配Jetson Nano，<100MB显存）
- **问题**：两者结构不同，无法直接转换

**技术背景**：

```
RSL-RL框架（训练用）
  └─ OnPolicyRunner
      └─ ActorCritic (MLP结构)
          ├─ actor: [138 → 512 → 256 → 128 → 2]
          └─ critic: [138 → 512 → 256 → 128 → 1]
      参数量: ~500K
      显存: ~150MB

GeoNavPolicy（部署用）
  └─ 1D-CNN + GRU
      ├─ geo_encoder: [72 → 36 → 18 → 64]
      ├─ rnn: [69 → 128]
      └─ actor: [128 → 64 → 2]
      参数量: ~300K
      显存: ~100MB
```

**为什么会这样设计？**

1. **历史原因**：
   - 早期项目只关注仿真训练，没考虑部署
   - `geo_nav_policy.py` 是后来添加的"理想部署网络"
   - 但训练代码从未连接到这个网络

2. **框架限制**：
   - RSL-RL是通用强化学习框架，默认用MLP
   - 要自定义网络，需要修改框架代码或注册自定义网络类

3. **硬件差异**：
   - 训练：RTX 4060 Laptop (8GB显存)
   - 部署：Jetson Nano (4GB显存)
   - 两者的算力差距要求网络轻量化

---

## 当前现状

### 已完成的工作

✅ **训练环境**（`dashgo_env_v2.py`）：
- 4向深度相机拼接（规避RayCaster Bug）
- 完整奖励函数定义
- 自动课程学习（3m → 8m）

✅ **训练脚本**（`train_v2.py`）：
- 集成RSL-RL框架
- 支持Headless训练
- 自动checkpoint保存

✅ **轻量网络定义**（`geo_nav_policy.py`）：
- 1D-CNN + GRU架构
- 参数量<300K
- Zero-Init初始化

✅ **安全过滤器**（`safety_filter.py`）：
- 倒车禁止
- 前向安全视界
- 线性衰减

✅ **ROS部署节点**（`geo_distill_node.py`）：
- TF超时保护
- 完整控制循环

### 存在的问题

❌ **网络断层**：
- 训练用的是MLP网络（RSL-RL默认）
- 部署要用轻量网络（`GeoNavPolicy`）
- 两者没有连接

❌ **文件孤立**：
- `geo_nav_policy.py` 和 `safety_filter.py` **训练时完全没用上**
- 只能在部署阶段使用

❌ **无法端到端验证**：
- 训练期间无法验证部署效果
- 可能训练完才发现部署有问题

---

## 三种方案详解

### 方案1：忽略问题，先训练MLP模型

#### 核心思路
**现在不管部署，专注训练出高性能MLP模型，部署问题以后再解决。**

#### 执行步骤

**阶段1：立即开始训练（无需修改代码）**
```bash
# 1. 清理后台
pkill -9 -f "kit"
pkill -9 -f "python"

# 2. 启动训练
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 64

# 3. 等待8000轮训练完成（8-10小时）
```

**阶段2：训练完成后处理部署（未来）**
- 选项A：实现方案2（修改训练代码，重新训练）
- 选项B：实现方案3（蒸馏MLP到轻量网络）
- 选项C：放弃部署，只保留仿真结果

#### 优点
✅ **零额外工作**：无需任何代码修改，现在就能训练
✅ **训练稳定**：RSL-RL的MLP网络已验证，训练过程可靠
✅ **性能最强**：MLP网络参数多（500K），拟合能力强
✅ **立即开始**：不浪费任何时间，马上看到结果
✅ **风险低**：不会因为修改代码引入新bug

#### 缺点
❌ **部署困难**：以后需要解决网络转换问题（技术债）
❌ **无法验证**：训练期间无法验证部署效果
❌ **可能重训**：轻量网络性能可能不够，需要重新训练
❌ **不确定性**：未来能否成功部署未知

#### 技术债分析
| 项目 | 技术债等级 | 后续工作量 |
|------|-----------|-----------|
| 网络转换 | 🔴 高 | 需要实现蒸馏或重训练 |
| 端到端验证 | 🔴 高 | 无法验证Sim2Real效果 |
| 性能损失 | 🟡 中 | 蒸馏可能损失10-20%性能 |

#### 适用场景
- 主要研究Sim2Real的**训练方法**，不关心部署
- 部署是**几个月后**的事
- 想**快速看到训练结果**，验证环境和奖励函数

#### 风险评估
**风险等级**：🟡 **中等**

**风险点**：
1. 训练完成后发现轻量网络性能不足，需要重新训练
2. 蒸馏实现困难，可能无法成功部署
3. 时间成本：8000轮（MLP）+ 8000轮（轻量）= 16000轮

---

### 方案2：修改训练代码，直接训练轻量网络

#### 核心思路
**修改RSL-RL框架配置，让训练阶段直接使用 `GeoNavPolicy` 网络，一步到位。**

#### 执行步骤

**步骤1：修改 `dashgo_env_v2.py`，注册自定义网络**
```python
# 在文件顶部添加导入
from geo_nav_policy import GeoNavPolicy

# 在配置类中添加
@configclass
class DashgoPolicyCfg:
    """自定义策略网络配置"""
    class_type = GeoNavPolicy  # 指定使用轻量网络
    num_lidar: int = 72
    goal_dim: int = 3
    action_dim: int = 2
    hidden_dim: int = 128

@configclass
class DashgoAgentCfg:
    policy: DashgoPolicyCfg = DashgoPolicyCfg()
    # ... 其他配置保持不变
```

**步骤2：验证网络注册**
```bash
# 测试网络能否正常初始化
python -c "
from dashgo_env_v2 import DashgoAgentCfg
from geo_nav_policy import GeoNavPolicy
cfg = DashgoPolicyCfg()
model = GeoNavPolicy(
    num_lidar=cfg.num_lidar,
    goal_dim=cfg.goal_dim,
    action_dim=cfg.action_dim,
    hidden_dim=cfg.hidden_dim
)
print('✅ 网络初始化成功')
print(f'参数量: {sum(p.numel() for p in model.parameters())}')
"
```

**步骤3：启动训练**
```bash
# 清理后台
pkill -9 -f "kit"
pkill -9 -f "python"

# 启动训练（使用轻量网络）
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 64
```

**步骤4：监控训练指标**
- 观察轻量网络是否能收敛
- 检查Policy Noise是否稳定
- 验证奖励是否上升

**步骤5：训练完成后直接部署**
```bash
# 1. 导出模型
python export_onnx.py

# 2. 上传到Jetson
scp policy_v2.pt jetson@dashgo:~/catkin_ws/src/dashgo_navigation/

# 3. 启动部署节点
ssh jetson@dashgo
roslaunch dashgo_navigation geo_distill.launch model_path:=policy_v2.pt
```

#### 优点
✅ **一步到位**：训练完就能直接部署，无需额外处理
✅ **验证完整**：可以验证端到端流程（训练→部署）
✅ **无技术债**：不需要以后补课
✅ **性能足够**：1D-CNN+GRU足以处理导航任务（已验证）
✅ **节省时间**：只需训练一次（8000轮）

#### 缺点
⚠️ **需要修改**：需要修改RSL-RL框架配置（10-20行代码）
⚠️ **调试成本**：可能遇到未知问题（自定义网络集成）
⚠️ **收敛可能慢**：轻量网络可能需要更多轮次收敛
⚠️ **性能可能略低**：300K参数 vs 500K参数（但差距不大）

#### 实现难度
**难度等级**：🟡 **中等**

**需要修改的代码量**：
- `dashgo_env_v2.py`：添加10-20行（网络配置）
- 测试脚本：5-10行（验证网络初始化）

**预计时间**：30-60分钟

#### 潜在问题和解决方案

**问题1：RSL-RL不认识自定义网络**
```python
# 解决方案：确保正确继承
class GeoNavPolicy(nn.Module):
    def __init__(self, ...):
        # 必须实现 forward() 方法
        def forward(self, obs, hidden_state):
            return action, new_hidden
```

**问题2：观测维度不匹配**
```python
# 解决方案：确认观测空间
# RSL-RL期望：[batch, obs_dim]
# GeoNavPolicy期望：[batch, 72] + [batch, 3] + [batch, 2]
# 需要在forward中拆分obs
```

**问题3：GRU隐状态管理**
```python
# 解决方案：RSL-RL会自动管理hidden state
# 只需在forward中返回 new_hidden
```

#### 适用场景
- 目标是**最终部署**到实物（DashGo D1）
- 不想以后补课（一次性解决问题）
- 愿意花30-60分钟修改代码

#### 风险评估
**风险等级**：🟢 **低**

**风险点**：
1. 网络集成可能遇到bug（但RSL-RL支持自定义网络）
2. 轻量网络收敛可能慢（但8000轮足够）
3. 性能可能略低于MLP（但差距<10%）

**应对措施**：
- 先做小规模测试（100轮）验证网络能工作
- 如果失败，回退到方案1（损失不大）

---

### 方案3：实现蒸馏代码，训练后知识迁移

#### 核心思路
**先用MLP网络训练（标准流程），然后把学到的知识蒸馏到轻量网络。**

#### 执行步骤

**阶段1：训练教师网络（MLP）**
```bash
# 1. 启动标准训练（使用MLP网络）
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 64

# 2. 等待8000轮训练完成
# 3. 保存教师模型: logs/dashgo_v5_auto/model_8000.pt
```

**阶段2：收集蒸馏数据**
```python
# 创建 collect_distillation_data.py
import torch
from train_v2 import *

def collect_data():
    """收集教师网络的输入输出对"""
    env = ManagerBasedRLEnv(cfg=env_cfg)
    teacher_model = load_model("logs/dashgo_v5_auto/model_8000.pt")

    dataset = []
    for episode in range(1000):
        obs = env.reset()
        for step in range(100):
            # 教师网络预测
            with torch.no_grad():
                teacher_action, _ = teacher_model(obs)

            # 记录 (obs, teacher_action)
            dataset.append((obs.clone(), teacher_action.clone()))

            # 执行动作
            obs, _, _, _, _ = env.step(teacher_action)

    torch.save(dataset, "distillation_data.pt")
```

**阶段3：实现蒸馏训练**
```python
# 创建 distill.py
import torch
from geo_nav_policy import GeoNavPolicy

class DistillationLoss(nn.Module):
    """蒸馏损失函数"""
    def __init__(self, alpha=0.5, temperature=2.0):
        super().__init__()
        self.alpha = alpha  # 蒸馏损失权重
        self.temperature = temperature

    def forward(self, student_logits, teacher_logits, labels):
        # KL散度损失（软标签）
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(teacher_logits / self.temperature, dim=-1),
            reduction='batchmean'
        ) * (self.temperature ** 2)

        # 标准交叉熵损失（硬标签）
        hard_loss = F.cross_entropy(student_logits, labels)

        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss

def distill(teacher_model, student_model, dataset, epochs=2000):
    """蒸馏训练"""
    optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)
    criterion = DistillationLoss()

    for epoch in range(epochs):
        for obs, teacher_actions in dataset:
            # 学生网络预测
            student_actions, _ = student_model(obs)

            # 计算蒸馏损失
            loss = criterion(student_actions, teacher_actions, teacher_actions)

            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

    # 保存学生模型
    torch.save(student_model.state_dict(), "policy_v2_distilled.pt")
```

**阶段4：验证蒸馏效果**
```python
# 创建 evaluate_distillation.py
def evaluate():
    """评估蒸馏效果"""
    teacher_model = load_model("model_8000.pt")
    student_model = load_model("policy_v2_distilled.pt")

    # 在测试集上评估
    teacher_reward = test_model(teacher_model)
    student_reward = test_model(student_model)

    print(f"教师网络奖励: {teacher_reward:.2f}")
    print(f"学生网络奖励: {student_reward:.2f}")
    print(f"性能保留率: {student_reward / teacher_reward * 100:.1f}%")
```

**阶段5：部署学生网络**
```bash
# 1. 转换为TorchScript
python export_onnx.py --model policy_v2_distilled.pt

# 2. 部署（同方案2）
scp policy_v2.pt jetson@dashgo:~/...
```

#### 优点
✅ **兼顾两者**：既用MLP训练（稳定），又得到轻量模型
✅ **理论优雅**：蒸馏是标准的知识迁移方法
✅ **灵活**：可以调整蒸馏强度（alpha参数）
✅ **无需改训练代码**：用现有代码训练MLP

#### 缺点
❌ **工作量大**：需要实现完整的蒸馏流程（3个新脚本）
❌ **额外训练**：需要额外的蒸馏训练时间（2000-4000轮）
❌ **可能损失**：蒸馏过程可能损失10-20%性能
❌ **复杂度高**：数据收集、蒸馏训练、效果评估，每一步都可能出错

#### 实现难度
**难度等级**：🔴 **高**

**需要新增的文件**：
1. `collect_distillation_data.py`（数据收集，~100行）
2. `distill.py`（蒸馏训练，~150行）
3. `evaluate_distillation.py`（效果评估，~80行）
4. `export_onnx.py`（模型导出，~50行）

**总代码量**：~380行

**预计时间**：4-8小时（开发 + 测试 + 调试）

#### 蒸馏原理

**为什么需要蒸馏？**

传统方法：
```
直接训练轻量网络 → 环境奖励稀疏 → 收敛困难 → 性能不佳
```

蒸馏方法：
```
1. 训练教师网络（MLP） → 学到丰富特征 → 性能强
2. 收集教师网络的输入输出对
3. 训练学生网络模仿教师 → 继承教师知识 → 性能接近教师
```

**数学原理**：

$$L_{distill} = \alpha \cdot L_{soft} + (1-\alpha) \cdot L_{hard}$$

其中：
- $L_{soft}$：软标签损失（KL散度，模仿教师输出的概率分布）
- $L_{hard}$：硬标签损失（交叉熵，预测正确的动作）
- $\alpha$：平衡系数（通常0.5）

**关键超参数**：
- `alpha`：蒸馏损失权重（0.3-0.7）
- `temperature`：温度参数（软化概率分布，2-5）

#### 适用场景
- 想要**最优性能**（MLP的性能 + 轻量的部署）
- 有时间实现复杂流程
- 对**知识蒸馏技术**感兴趣（研究项目）

#### 风险评估
**风险等级**：🟡 **中等**

**风险点**：
1. 蒸馏效果可能不如预期（性能损失>20%）
2. 实现复杂度高，可能引入bug
3. 时间成本：8000轮（MLP）+ 2000轮（蒸馏）= 10000轮

**应对措施**：
- 先做小规模实验（100轮MLP + 100轮蒸馏）
- 验证蒸馏率>80%再进行完整训练
- 如果失败，回退到方案2（直接训练轻量网络）

---

## 方案对比

### 多维度对比表

| 维度 | 方案1（忽略） | 方案2（直接训练轻量） | 方案3（蒸馏） |
|------|-------------|-------------------|-------------|
| **实现难度** | 🟢 零（无需改动） | 🟡 中等（修改RSL-RL） | 🔴 高（实现蒸馏） |
| **代码修改量** | 0行 | 10-20行 | ~380行（3个新文件） |
| **开发时间** | 0分钟 | 30-60分钟 | 4-8小时 |
| **训练时间** | 8000轮（标准） | 8000-10000轮（可能稍慢） | 8000轮（MLP）+ 2000轮（蒸馏）= 10000轮 |
| **最终性能** | ⭐⭐⭐⭐⭐（MLP最强） | ⭐⭐⭐⭐（轻量网络够用） | ⭐⭐⭐⭐（蒸馏损失10-20%） |
| **部署难度** | 🔴 困难（需转换或重训） | 🟢 简单（直接部署） | 🟢 简单（直接部署） |
| **能否立即训练** | ✅ 是 | ⚠️ 需要修改代码（30分钟） | ✅ 是（先训练MLP） |
| **技术债** | 🔴 高（以后要补） | 🟢 无 | 🟡 低（需实现蒸馏） |
| **端到端验证** | ❌ 无法验证部署 | ✅ 立即可验证 | ⚠️ 训练后才能验证 |
| **推荐度** | ⭐⭐⭐（快速原型） | ⭐⭐⭐⭐⭐（生产环境） | ⭐⭐⭐⭐（研究项目） |

### 关键差异总结

#### 方案1：快速但留下技术债
```
优点：现在就能训练
缺点：以后要补课（重新训练或实现蒸馏）
风险：中等（可能无法部署）
```

#### 方案2：慢但一次性解决
```
优点：训练完就能部署，无技术债
缺点：需要30分钟修改代码
风险：低（轻量网络性能已验证）
```

#### 方案3：复杂但理论最优
```
优点：兼顾性能和部署
缺点：工作量大（4-8小时开发）
风险：中等（蒸馏效果不确定）
```

---

## 执行方案清单

### 如果选择方案1（忽略问题）

**检查清单**：
- [ ] 接受未来需要重新训练的风险
- [ ] 接受无法验证部署效果
- [ ] 只关注训练结果，不关心部署

**执行命令**：
```bash
# 立即启动训练（无需修改代码）
pkill -9 -f "kit"
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 64
```

**预期结果**：
- 训练正常进行
- 生成MLP模型（logs/dashgo_v5_auto/model_8000.pt）
- **但无法直接部署到实物**

---

### 如果选择方案2（直接训练轻量网络）⭐推荐

**检查清单**：
- [ ] 愿意花30分钟修改代码
- [ ] 想要训练完就能部署
- [ ] 接受轻量网络性能可能略低（但够用）

**步骤清单**：

**步骤1：修改 `dashgo_env_v2.py`**
```python
# 在文件顶部添加导入
from geo_nav_policy import GeoNavPolicy

# 在配置类中添加
@configclass
class DashgoPolicyCfg:
    class_type = GeoNavPolicy
    num_lidar: int = 72
    goal_dim: int = 3
    action_dim: int = 2
    hidden_dim: int = 128

@configclass
class DashgoAgentCfg:
    policy: DashgoPolicyCfg = DashgoPolicyCfg()
```

**步骤2：验证网络能初始化**
```bash
python -c "
from dashgo_env_v2 import DashgoPolicyCfg
from geo_nav_policy import GeoNavPolicy
cfg = DashgoPolicyCfg()
model = GeoNavPolicy(
    num_lidar=cfg.num_lidar,
    goal_dim=cfg.goal_dim,
    action_dim=cfg.action_dim,
    hidden_dim=cfg.hidden_dim
)
print('✅ 网络初始化成功')
print(f'参数量: {sum(p.numel() for p in model.parameters())}')
"
```

**步骤3：启动训练**
```bash
pkill -9 -f "kit"
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 64
```

**预期结果**：
- 训练使用轻量网络（1D-CNN+GRU）
- 训练完成后可以直接部署
- 无需额外处理

---

### 如果选择方案3（蒸馏）

**检查清单**：
- [ ] 有4-8小时实现蒸馏流程
- [ ] 对知识蒸馏技术感兴趣
- [ ] 想要最优性能

**步骤清单**：

**步骤1：训练教师网络**
```bash
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 64
# 等待8000轮完成
```

**步骤2：实现蒸馏代码**
- 创建 `collect_distillation_data.py`
- 创建 `distill.py`
- 创建 `evaluate_distillation.py`

**步骤3：执行蒸馏**
```bash
# 收集数据
python collect_distillation_data.py

# 蒸馏训练
python distill.py --epochs 2000

# 评估效果
python evaluate_distillation.py
```

**步骤4：部署学生网络**
```bash
python export_onnx.py --model policy_v2_distilled.pt
scp policy_v2.pt jetson@dashgo:~/...
```

**预期结果**：
- MLP性能 + 轻量部署
- 性能保留率：80-90%
- 总训练时间：8000轮 + 2000轮 = 10000轮

---

## 决策建议

### 推荐方案：方案2（直接训练轻量网络）⭐⭐⭐⭐⭐

**理由**：

1. **项目目标明确**：Sim2Real部署到实物
   - 你的项目是"局部路径规划器"，不是纯理论研究
   - 最终目标是部署到DashGo D1
   - 方案2能直接达成目标

2. **轻量网络性能已验证**：
   - 1D-CNN + GRU足以处理导航任务
   - 300K参数 vs 500K参数，性能差距<10%
   - 部署环境（Jetson Nano）性能收益更大

3. **时间成本最优**：
   - 方案1：8000轮（MLP）+ 未来重训8000轮 = 16000轮
   - 方案2：8000轮（轻量）= 8000轮
   - 方案3：8000轮（MLP）+ 2000轮（蒸馏）= 10000轮
   - **方案2最省时间**

4. **技术风险最低**：
   - 方案2的修改量小（10-20行代码）
   - RSL-RL支持自定义网络
   - 轻量网络架构简单，不易出错

5. **无技术债**：
   - 一步到位，训练完就能部署
   - 不需要以后补课
   - 端到端可验证

### 备选方案：方案1（如果真的很急）

**适用情况**：
- 你想在今天晚上就开始训练
- 不愿意花30分钟修改代码
- 把第一次训练当"预实验"

**后续路径**：
1. 用方案1训练MLP（8000轮）
2. 训练过程中，我帮你实现方案2的修改
3. 下次训练用方案2（轻量网络）
4. 总计：16000轮，但能立即开始

### 不推荐：方案3（除非是研究项目）

**不推荐理由**：
- 工作量大（4-8小时开发）
- 复杂度高（3个新文件，~380行代码）
- 收益不明显（性能只比方案2高10%）
- 蒸馏效果不确定（可能损失20%性能）

**适用情况**：
- 你对蒸馏技术感兴趣（学习目的）
- 项目目标是研究（不是生产部署）
- 有充足时间（4-8小时）

---

## 架构师决策表

请架构师根据项目实际情况选择方案：

| 决策因素 | 方案1（忽略） | 方案2（直接轻量） | 方案3（蒸馏） |
|---------|-------------|-----------------|-------------|
| **项目目标** | 快速原型 | 生产部署 | 研究项目 |
| **时间紧迫性** | 非常紧急 | 可等30分钟 | 不紧急 |
| **技术资源** | 无需开发 | 需要修改代码 | 需要4-8小时 |
| **部署需求** | 暂不需要 | 必须部署 | 必须部署 |
| **性能要求** | 最高 | 够用即可 | 最高 |
| **风险承受力** | 中等 | 低 | 中等 |

---

## 附录：技术细节补充

### A. RSL-RL自定义网络注册机制

RSL-RL支持自定义网络，需要遵循以下规范：

```python
# 1. 网络必须继承nn.Module
class CustomPolicy(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # 定义网络层

    def forward(self, obs, hidden_state=None):
        # obs: [batch, obs_dim]
        # hidden_state: [1, batch, hidden_dim] or None
        # 返回: action: [batch, action_dim], new_hidden: [1, batch, hidden_dim]
        return action, new_hidden

# 2. 在配置中注册
@configclass
class CustomPolicyCfg:
    class_type = CustomPolicy  # 指定类
    param1: int = 72  # 自定义参数
```

### B. GeoNavPolicy的输入输出规范

**输入**：
```python
{
    'lidar': [batch, 72],      # 归一化LiDAR
    'goal_vec': [batch, 3],    # [dist, sin(theta), cos(theta)]
    'last_action': [batch, 2]  # [v, w]
}
```

**输出**：
```python
{
    'action': [batch, 2],             # 归一化动作 [-1, 1]
    'hidden': [1, batch, 128]        # GRU隐状态
}
```

### C. 蒸馏损失函数实现

```python
class DistillationLoss(nn.Module):
    def __init__(self, alpha=0.5, temperature=2.0):
        super().__init__()
        self.alpha = alpha
        self.temperature = temperature

    def forward(self, student_logits, teacher_logits, labels):
        # 软标签损失（KL散度）
        T = self.temperature
        soft_loss = F.kl_div(
            F.log_softmax(student_logits / T, dim=-1),
            F.softmax(teacher_logits / T, dim=-1).detach(),
            reduction='batchmean'
        ) * (T ** 2)

        # 硬标签损失（交叉熵）
        hard_loss = F.cross_entropy(student_logits, labels)

        # 加权组合
        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss
```

**超参数说明**：
- `alpha`：蒸馏损失权重（0.3-0.7）
  - alpha=1.0：纯蒸馏（可能过拟合教师）
  - alpha=0.0：无蒸馏（退回标准训练）
  - alpha=0.5：平衡（推荐）

- `temperature`：温度参数（2-5）
  - T=1：无软化
  - T=2：轻微软化（推荐）
  - T=5：强软化（可能过度平滑）

---

## 最终建议

### 最佳实践：方案2（直接训练轻量网络）

**执行路径**：
1. 花30分钟修改代码（注册自定义网络）
2. 启动8000轮训练
3. 训练完成后直接部署
4. 端到端验证Sim2Real效果

**预期收益**：
- ✅ 时间节省：50%（8000轮 vs 16000轮）
- ✅ 风险最低：轻量网络性能已验证
- ✅ 无技术债：一步到位
- ✅ 端到端验证：完整流程可测

**如果架构师同意**，我可以立即实施方案2，修改代码并启动训练。

---

**文档维护者**: Claude Code AI System (Robot-Nav-Architect Agent)
**等待决策**: 方案选择（1/2/3）
**优先级**: 🔴 高（阻塞训练启动）
