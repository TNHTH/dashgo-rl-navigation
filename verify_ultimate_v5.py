"""
DashGo ç»ˆæéªŒè¯å·¥å…· v5.1 (Sensor Probes Edition)
ä¿®å¤æ¸…å•ï¼š
1. [Config] ä¿®å¤ YAML è¯»å–é€»è¾‘ (å…¼å®¹æ‰å¹³/åµŒå¥—ç»“æ„)
2. [Network] ä¿®å¤ LayerNorm ç»Ÿè®¡é€»è¾‘ (v3.1 åº”æœ‰ 8 ä¸ª)
3. [Curriculum] å¢åŠ  v6.0 è‡ªåŠ¨è¯¾ç¨‹æ³¨å…¥éªŒè¯
4. [Environment] ä¿ç•™ç‰©ç†/ä¼ æ„Ÿå™¨/å¥–åŠ±å…¨æ ˆéªŒè¯
5. [V5.1 æ–°å¢] ä¼ æ„Ÿå™¨æ¢é’ˆ - å®æ—¶ LiDAR æ•°æ®ä½“æ£€ + ç¢°æ’åŠ›éªŒè¯

æ¶æ„å¸ˆ: Isaac Sim Architect + Assistant Fusion
ç‰ˆæœ¬: v5.1 Sensor Probes Edition
æ—¥æœŸ: 2026-01-30
"""

import torch
import os
import sys
import numpy as np
import yaml

# Isaac Lab æ ¸å¿ƒ
from isaaclab.app import AppLauncher

# ==============================================================================
# 1. å¯åŠ¨ä»¿çœŸå™¨ (å¿…é¡»åœ¨å¯¼å…¥ç¯å¢ƒå‰)
# ==============================================================================
app_launcher = AppLauncher(headless=True, enable_cameras=True)
simulation_app = app_launcher.app

print("\n" + "=" * 80)
print("ğŸ¤– [Isaac Sim] å¼•æ“å¯åŠ¨æˆåŠŸ... æ­£åœ¨åŠ è½½æ¨¡å—")
print("=" * 80)

from isaaclab.envs import ManagerBasedRLEnv
from dashgo_env_v2 import DashgoNavEnvV2Cfg
from geo_nav_policy import GeoNavPolicy

def check_curriculum_logic(env_cfg):
    """[æ ¸å¿ƒå¢å¼º] éªŒè¯è¯¾ç¨‹å­¦ä¹ é€»è¾‘ä¸å‚æ•°"""
    print("\nğŸ“… [1. è¯¾ç¨‹å­¦ä¹ é€»è¾‘æ·±åº¦éªŒè¯]")

    # 1. éªŒè¯ v6.0 è‡ªåŠ¨æ³¨å…¥æ˜¯å¦ç”Ÿæ•ˆ
    # ç›´æ¥æ£€æŸ¥ env_cfg å¯¹è±¡ä¸­çš„å‚æ•°ï¼Œè¿™æ˜¯æœ€çœŸå®çš„
    try:
        if hasattr(env_cfg, 'curriculum') and hasattr(env_cfg.curriculum, 'target_expansion'):
            params = env_cfg.curriculum.target_expansion.params
            end_step = params.get('end_step', 0)

            print(f"  â€¢ è¿è¡Œæ—¶ end_step å‚æ•°: {end_step:,}")

            if end_step > 100_000_000: # 3äº¿
                print("  âŒ [è­¦å‘Š] è¯¾ç¨‹å‚æ•°æœªè‡ªåŠ¨æ ¡å‡† (ä»æ˜¯é»˜è®¤å€¼ 300M)")
                print("     è¯·æ£€æŸ¥ train_v2.py æ˜¯å¦æ­£ç¡®æ³¨å…¥äº†è‡ªåŠ¨è®¡ç®—é€»è¾‘")
            elif end_step == 0:
                print("  âŒ [è­¦å‘Š] è¯¾ç¨‹å‚æ•°ä¸º 0ï¼Œå¯èƒ½æœªæ­£ç¡®é…ç½®")
            else:
                print("  âœ… [v6.0] è‡ªåŠ¨è¯¾ç¨‹æ³¨å…¥å·²ç”Ÿæ•ˆ (å‚æ•°å·²æ ¡å‡†)")
        else:
            print("  âš ï¸ æœªæ‰¾åˆ° target_expansion è¯¾ç¨‹é…ç½®")
    except Exception as e:
        print(f"  âš ï¸ è¯¾ç¨‹éªŒè¯è·³è¿‡: {e}")

    # 2. æ¨¡æ‹Ÿè®¡ç®—é€»è¾‘ (Double Check)
    yaml_path = "train_cfg_v2.yaml"
    if not os.path.exists(yaml_path):
        yaml_path = os.path.join(os.getcwd(), "train_cfg_v2.yaml")

    try:
        with open(yaml_path, 'r') as f:
            cfg = yaml.safe_load(f)

        # [æ¶æ„å¸ˆä¿®å¤] æ›´ç¨³å¥çš„è¯»å–é€»è¾‘
        runner_cfg = cfg.get('runner', {})
        if not runner_cfg: # å°è¯•åµŒå¥—ç»“æ„
             runner_cfg = cfg.get('algorithm', {}).get('runner', {})

        max_iterations = runner_cfg.get('max_iterations', 8000)
        num_steps_per_env = runner_cfg.get('num_steps_per_env', 24)
        sim_num_envs = env_cfg.scene.num_envs # ä½¿ç”¨å®é™…ç¯å¢ƒæ•°

        ratio = 0.75
        total_steps = max_iterations * num_steps_per_env * sim_num_envs
        calc_end_step = total_steps * ratio

        print(f"  â€¢ ç†è®ºè®¡ç®—å€¼ (åŸºäºå½“å‰ num_envs={sim_num_envs}): {int(calc_end_step):,}")

    except Exception as e:
        print(f"  âš ï¸ YAML è¯»å–å¤±è´¥: {e}")

def count_layernorm(policy):
    """ç»Ÿè®¡ LayerNorm æ•°é‡"""
    count = 0
    for module in policy.modules():
        if isinstance(module, torch.nn.LayerNorm):
            count += 1
    return count

def main():
    print("\nğŸ­ [2. ç¯å¢ƒåˆå§‹åŒ–éªŒè¯]")
    env_cfg = DashgoNavEnvV2Cfg()
    env_cfg.scene.num_envs = 4

    # æ‰§è¡Œè¯¾ç¨‹æ£€æŸ¥
    check_curriculum_logic(env_cfg)

    try:
        env = ManagerBasedRLEnv(cfg=env_cfg)
    except Exception as e:
        print(f"  âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
        return

    obs, _ = env.reset()

    # æå– Tensor
    if hasattr(obs, "get"):
        policy_obs = obs["policy"] if "policy" in obs.keys() else list(obs.values())[0]
    else:
        policy_obs = obs

    print(f"  âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸã€‚è§‚æµ‹ç»´åº¦: {policy_obs.shape}")

    # ==========================================================================
    # 3. ç­–ç•¥ç½‘ç»œå¥åº·åº¦æ£€æŸ¥
    # ==========================================================================
    print("\nğŸ§  [3. ç­–ç•¥ç½‘ç»œ(Brain)å¥åº·åº¦æ£€æŸ¥]")

    try:
        num_actions = env.action_space.shape[1]
        policy = GeoNavPolicy(obs=obs, obs_groups=None, num_actions=num_actions).to(env.device)

        # [æ¶æ„å¸ˆä¿®å¤] ä¸¥æ ¼æ£€æŸ¥ LayerNorm æ•°é‡
        ln_count = count_layernorm(policy)
        print(f"  â€¢ æ£€æµ‹åˆ° {ln_count} ä¸ª LayerNorm å±‚")
        if ln_count >= 8:
            print("  âœ… LayerNorm é…ç½®å®Œæ•´ (ç¬¦åˆ v3.1 æ¶æ„)")
        else:
            print(f"  âš ï¸ LayerNorm æ•°é‡ä¸è¶³ (é¢„æœŸ >= 8ï¼Œå®é™… {ln_count})")

        # æ­£å¸¸ä¸æç«¯æµ‹è¯•
        print("  â€¢ å‰å‘ä¼ æ’­æµ‹è¯•...")
        with torch.no_grad():
            action = policy.act(obs)

            # æç«¯æµ‹è¯• (Inf)
            bad_obs = policy_obs.clone()
            bad_obs[:] = float('inf')
            fake_obs = {"policy": bad_obs} if hasattr(obs, "get") else bad_obs
            bad_action = policy.act(fake_obs)

        if torch.isnan(bad_action).any():
            print("  âŒ [è‡´å‘½é”™è¯¯] Input Clamp æœªç”Ÿæ•ˆï¼Inf è¾“å…¥å¯¼è‡´ NaN è¾“å‡ºã€‚")
        else:
            print(f"  âœ… å‹åŠ›æµ‹è¯•é€šè¿‡ (Clamp ç”Ÿæ•ˆ)ã€‚")

    except Exception as e:
        print(f"  âŒ ç½‘ç»œéªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # ==========================================================================
    # 4. ç‰©ç†ä¸å¥–åŠ±éªŒè¯ (å« V5.1 ä¼ æ„Ÿå™¨æ¢é’ˆ)
    # ==========================================================================
    print("\nğŸš€ [4. ç‰©ç†ä¸å¥–åŠ±å¾ªç¯éªŒè¯] (100æ­¥)")

    for i in range(100):
        with torch.no_grad():
            actions = policy.act(obs)

        # æ‰§è¡Œç‰©ç†æ­¥
        obs, rew, terminated, truncated, extras = env.step(actions)

        # ----------------------------------------------------------------------
        # [æ¶æ„å¸ˆæ¢é’ˆ] V5.1: å®æ—¶ä¼ æ„Ÿå™¨æ•°æ®ä½“æ£€
        # ----------------------------------------------------------------------
        if i % 20 == 0:
            # 1. æå– LiDAR æ•°æ® (å‡è®¾å‰216ä½æ˜¯LiDAR)
            # æ³¨æ„: éœ€è¦æ ¹æ®ä½ çš„è§‚æµ‹ç©ºé—´å®šä¹‰ç¡®è®¤åˆ‡ç‰‡èŒƒå›´ï¼Œè¿™é‡Œå‡è®¾æ˜¯ [:, 0:216]
            if hasattr(obs, "get"):
                current_obs = obs["policy"]
            else:
                current_obs = obs

            lidar_data = current_obs[:, 0:216]

            # 2. éªŒè¯ LiDAR æ˜¯å¦"æ´»ç€"
            l_min = lidar_data.min().item()
            l_max = lidar_data.max().item()
            l_mean = lidar_data.mean().item()

            # 3. éªŒè¯ç¢°æ’åŠ› (Contact Forces)
            # é€šè¿‡å¥–åŠ±å­—å…¸ä¾§é¢éªŒè¯ï¼Œæˆ–è€…ç›´æ¥è¯»å– contact_forces_base (å¦‚æœèƒ½è®¿é—®åˆ°env.scene)
            has_collision = False
            if "episode" in extras:
                col_rew = extras["episode"].get("reward_collision", 0.0)
                if isinstance(col_rew, torch.Tensor):
                    col_rew = col_rew.mean().item()
                if col_rew < 0:
                    has_collision = True

            # 4. æ‰“å°ç»¼åˆä½“æ£€æŠ¥å‘Š
            print(f"  Step {i:03d}:")

            # é€Ÿåº¦æ•°æ®
            robot = env.scene["robot"]
            v = robot.data.root_lin_vel_b[:, 0].mean().item()
            print(f"    ğŸš„ é€Ÿåº¦: {v:.3f} m/s")

            # LiDAR ä¼ æ„Ÿå™¨å¥åº·åº¦
            print(f"    ğŸ‘ï¸ LiDAR: Min={l_min:.2f}, Max={l_max:.2f}, Mean={l_mean:.2f} (æ•°æ®æµåŠ¨æ­£å¸¸)")

            if l_max == 0.0 and l_min == 0.0:
                print("    âš ï¸ [è­¦å‘Š] LiDAR æ•°æ®å…¨ä¸º 0ï¼ä¼ æ„Ÿå™¨å¯èƒ½æœªå·¥ä½œæˆ–è¢«å®Œå…¨é®æŒ¡ï¼")

            # ç¢°æ’åŠ›æ£€æµ‹
            if has_collision:
                print("    ğŸ’¥ [æ£€æµ‹] å‘ç”Ÿç¢°æ’ï¼ç‰©ç†å¼•æ“æ¥è§¦åŠ›åé¦ˆæ­£å¸¸ã€‚")

            # å¥–åŠ±æ±‡æ€»
            r_mean = rew.mean().item()
            print(f"    ğŸ’° å¥–åŠ±: {r_mean:.4f}")

            # [æ¶æ„å¸ˆä¿®å¤] å¥–åŠ±åˆ†é¡¹å¿«ç…§
            if "episode" in extras and i == 20:
                print(f"     ğŸ“Š [å¥–åŠ±åˆ†é¡¹å¿«ç…§]:")
                found = False
                for k, v_val in extras["episode"].items():
                    if ("Reward" in k or "Penalty" in k):
                        val = v_val.item() if torch.is_tensor(v_val) else v_val
                        if abs(val) > 1e-4:
                            print(f"       â€¢ {k}: {val:.4f}")
                            found = True
                if not found:
                    print("       âš ï¸ æ‰€æœ‰åˆ†é¡¹å¥–åŠ±å‡ä¸º 0.0000")

    print("\n" + "=" * 80)
    print("âœ… ç»ˆæéªŒè¯å®Œæˆã€‚å¦‚æœä»¥ä¸Šå…¨ç»¿ï¼Œä½ çš„ä»£ç å°±æ˜¯é˜²å¼¹çš„ã€‚")
    print("=" * 80)
    simulation_app.close()

if __name__ == "__main__":
    main()
