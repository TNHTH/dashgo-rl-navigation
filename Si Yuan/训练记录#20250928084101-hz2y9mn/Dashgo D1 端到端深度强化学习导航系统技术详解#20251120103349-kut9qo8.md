## 1. 系统概述 (System Overview)

本项目采用 **端到端 (End-to-End) 无地图导航** 方案，旨在解决移动机器人在未知、动态环境中的自主避障与目标导向问题。

与传统的导航栈（如 ROS Navigation Stack 分为全局规划+局部规划）不同，本系统**没有显式的路径规划算法**（如 A\* 或 Dijkstra）。机器人通过 **深度强化学习 (Deep Reinforcement Learning)**  训练出的神经网络策略（Policy），直接将传感器原始数据映射为电机控制指令。

* **核心优势**：无需预先建图 (Map-less)，对动态障碍物反应迅速，具备极强的抗干扰能力。
* **核心算法**：PPO (Proximal Policy Optimization)。
* **仿真平台**：NVIDIA Isaac Lab (基于 Isaac Sim)。

## 2. 算法架构 (Algorithm Architecture)

### 2.1 核心算法：PPO

采用了 **PPO (近端策略优化)**  算法。这是一种 On-Policy 的强化学习算法，以其在连续控制任务中的稳定性和高效性著称。

* **Actor Network (策略网络)** ：负责根据当前观测输出动作。
* **Critic Network (价值网络)** ：负责评估当前状态的好坏，指导 Actor 更新。

### 2.2 观测空间 (Observation Space - 输入)

神经网络的输入是一个 **88维** 的向量，融合了多源传感器数据：

1. **激光雷达数据 (Lidar Ranges) - [80维]**

   * **仿真实现**：使用分辨率为 `1x80` 的深度相机 (Depth Camera) 模拟。深度图被展平并截断为 10米量程。
   * **真机对应**：YDLIDAR 的 `/scan` 数据降采样至 80 个点。
   * **作用**：感知前方及侧前方障碍物的距离和形状。
2. **超声波数据 (Ultrasonic Ranges) - [4维]**

   * **仿真实现**：使用 4 个 `1x1` 像素的微型深度相机，分别朝向 **前、左、右、后**。
   * **真机对应**：Dashgo 底盘的 4 个超声波探头数据。
   * **作用**：弥补激光雷达盲区，检测玻璃/镜面物体，以及防止倒车撞墙。
3. **当前速度 (Velocity) - [2维]**

   * 机器人的线速度 (\$v\$) 和角速度 (\$\\omega\$)。
4. **目标相对位置 (Target Command) - [2维]**

   * 目标点相对于机器人当前坐标系的 \$(x, y)\$ 坐标。
   * **注意**：训练时引入了高斯噪声 (`std=0.05`)，以模拟真实里程计 (Odometry) 的累积误差，提高鲁棒性。

### 2.3 动作空间 (Action Space - 输出)

* **输出**：左右轮的目标角速度 (Left/Right Wheel Target Velocity)。
* **控制方式**：差分驱动 (Differential Drive)。神经网络直接输出归一化动作 \$[-1, 1]\$，经缩放后转换为电机指令。

## 3. 导航与避障策略 (Navigation Strategy)

由于没有显式的路径规划器，机器人的导航行为完全是通过 **奖励函数塑形 (Reward Shaping)**  涌现出来的。

目前的策略经过激进调整，旨在克服“局部最优（不动）”陷阱，形成  **“勇敢且安全”**  的驾驶风格。

### 3.1 核心驱动力 (The "Carrot")

* **靠近目标奖励 (**​**​`progress_to_goal`​**​ **)** ：权重 **15.0**。

  * 只要机器人向目标方向移动，就会获得巨额奖励。这是驱动机器人“动起来”的最强动力。
* **到达目标奖励 (**​**​`reach_goal`​**​ **)** ：权重 **50.0**。

  * 成功抵达目标点（误差 \< 0.5m）时给予的终极奖励。

### 3.2 约束与惩罚 (The "Stick")

* **碰撞惩罚 (**​**​`collision`​**​ **)** ：权重  **-5.0**。

  * 一旦检测到机体受力（通过 `ContactSensor`），即判定为碰撞。相比早期的 -50，现在的惩罚较轻，鼓励机器人在略微冒险中学习绕障，而不是因恐惧而瘫痪。
* **静止惩罚 (**​**​`stand_still`​**​ **)** ：权重  **-0.5/step**。

  * 如果线速度低于 0.05 m/s，每一步都会扣分。这强迫机器人必须时刻保持运动，防止“装死”。
* **动作平滑惩罚 (**​**​`action_rate`​**​ **)** ：权重  **-0.002**。

  * 轻微惩罚动作的剧烈变化，保护电机，防止震荡。

## 4. Sim-to-Real 迁移技术 (Sim-to-Real Transfer)

为了让仿真训练的模型能在真实世界工作，我们采用了以下关键技术跨越“虚实鸿沟”：

### 4.1 传感器仿真技巧 (Sensor Trick)

由于 Isaac Lab 对 `RayCaster` 的网格数量限制，我们创新性地使用了 **Pinhole Camera (针孔深度相机)**  来模拟雷达和超声波。

* **原理**：深度图的一行像素在数学上等价于激光雷达的距离数组。
* **优势**：可以无视环境中的 Object 数量限制，完美支持动态障碍物训练。

### 4.2 域随机化 (Domain Randomization)

在训练过程中，环境参数不断随机变化，迫使网络学习通用的特征：

* **物理属性**：随机化车身质量 (0.8x-1.2x)、地面摩擦力。
* **感知噪声**：给雷达、超声波和目标位置读数添加高斯噪声。
* **环境布局**：每次 Reset，机器人、目标点、圆柱障碍物、立方体障碍物的位置都会随机重置。

### 4.3 经验归一化 (Empirical Normalization)

* 在训练和推理时，使用运行时的均值和方差对观测数据进行标准化处理。这对于处理量纲差异巨大的输入（如 10米的雷达数据 vs 0.5的速度数据）至关重要。

## 5. 部署流程 (Deployment)

1. **导出**：将 PyTorch (`.pt`) 模型及归一化参数导出为 ONNX 格式。
2. **推理**：在机器人上位机（如 Jetson）上运行 `run_robot.py`。
3. **数据对齐**：

   * 订阅 ROS `/scan` -\> 降采样至 80 点 -\> Clip 到 10米。
   * 订阅 ROS `/sonar` -\> 格式化为 4 点数组。
   * 订阅 `/odom` -\> 计算目标相对坐标。
4. **控制**：模型输出 -\> 逆运动学解算 -\> 发布 `/cmd_vel`。

## 总结

这套系统是一个**高度鲁棒的局部导航方案**。它不需要地图，不依赖 GPS，仅靠本体感知（雷达+超声波+里程计）就能在复杂的动态环境中实现**无碰撞的点对点移动**。
