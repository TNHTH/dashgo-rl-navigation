################################################################################
                      Learning iteration 516/3250

                       Computation: 75 steps/s (collection: 20.283s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 112.1293
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 2.6222
                       Mean reward: -35.96
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1903
          Episode_Reward/collision: -2.1137
        Episode_Reward/action_rate: -0.0752
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 5.0247
Metrics/target_pose/orientation_error: 1.9457
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 410112
                    Iteration time: 20.33s
                      Time elapsed: 01:26:12
                               ETA: 14:42:47

################################################################################
                      Learning iteration 517/3250

                       Computation: 74 steps/s (collection: 20.465s, learning 0.047s)
             Mean action noise std: 0.90
          Mean value_function loss: 100.7509
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 2.6223
                       Mean reward: -40.37
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0032
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2042
          Episode_Reward/collision: -1.2713
        Episode_Reward/action_rate: -0.0728
        Episode_Reward/stand_still: -0.0976
Metrics/target_pose/position_error: 4.1863
Metrics/target_pose/orientation_error: 1.4117
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 411648
                    Iteration time: 20.51s
                      Time elapsed: 01:26:33
                               ETA: 14:42:40

################################################################################
                      Learning iteration 518/3250

                       Computation: 75 steps/s (collection: 20.375s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 52.1597
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 2.6222
                       Mean reward: -44.53
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0008
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2227
          Episode_Reward/collision: -1.3125
        Episode_Reward/action_rate: -0.0191
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 5.1756
Metrics/target_pose/orientation_error: 1.9727
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 413184
                    Iteration time: 20.43s
                      Time elapsed: 01:26:53
                               ETA: 14:42:31

################################################################################
                      Learning iteration 519/3250

                       Computation: 75 steps/s (collection: 20.383s, learning 0.051s)
             Mean action noise std: 0.90
          Mean value_function loss: 124.2014
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 2.6219
                       Mean reward: -35.36
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0006
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2001
          Episode_Reward/collision: -0.1354
        Episode_Reward/action_rate: -0.0417
        Episode_Reward/stand_still: -0.0991
Metrics/target_pose/position_error: 4.1074
Metrics/target_pose/orientation_error: 1.2650
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 414720
                    Iteration time: 20.43s
                      Time elapsed: 01:27:14
                               ETA: 14:42:22

################################################################################
                      Learning iteration 520/3250

                       Computation: 75 steps/s (collection: 20.302s, learning 0.051s)
             Mean action noise std: 0.90
          Mean value_function loss: 138.8258
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 2.6216
                       Mean reward: -40.24
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0012
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1845
          Episode_Reward/collision: -2.0113
        Episode_Reward/action_rate: -0.3036
        Episode_Reward/stand_still: -0.0988
Metrics/target_pose/position_error: 3.4344
Metrics/target_pose/orientation_error: 1.0444
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 416256
                    Iteration time: 20.35s
                      Time elapsed: 01:27:34
                               ETA: 14:42:12

################################################################################
                      Learning iteration 521/3250

                       Computation: 74 steps/s (collection: 20.456s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 376.5824
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 2.6212
                       Mean reward: -44.12
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0008
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2020
          Episode_Reward/collision: -1.5694
        Episode_Reward/action_rate: -0.1044
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 3.6381
Metrics/target_pose/orientation_error: 0.6114
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 417792
                    Iteration time: 20.51s
                      Time elapsed: 01:27:55
                               ETA: 14:42:04

################################################################################
                      Learning iteration 522/3250

                       Computation: 75 steps/s (collection: 20.357s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 198.8216
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 2.6207
                       Mean reward: -52.01
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0006
         Episode_Reward/reach_goal: 0.0521
          Episode_Reward/face_goal: 0.1668
          Episode_Reward/collision: -0.5655
        Episode_Reward/action_rate: -0.0495
        Episode_Reward/stand_still: -0.0803
Metrics/target_pose/position_error: 4.1000
Metrics/target_pose/orientation_error: 1.9058
      Episode_Termination/time_out: 0.9759
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0241

---

                   Total timesteps: 419328
                    Iteration time: 20.41s
                      Time elapsed: 01:28:15
                               ETA: 14:41:55

################################################################################
                      Learning iteration 523/3250

                       Computation: 75 steps/s (collection: 20.382s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 36.5575
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 2.6206
                       Mean reward: -55.05
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2091
          Episode_Reward/collision: -1.2925
        Episode_Reward/action_rate: -0.8760
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 4.8860
Metrics/target_pose/orientation_error: 1.3313
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 420864
                    Iteration time: 20.43s
                      Time elapsed: 01:28:35
                               ETA: 14:41:46

################################################################################
                      Learning iteration 524/3250

                       Computation: 75 steps/s (collection: 20.319s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 35.7206
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 2.6207
                       Mean reward: -51.61
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2008
          Episode_Reward/collision: -0.1311
        Episode_Reward/action_rate: -0.6677
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 4.5842
Metrics/target_pose/orientation_error: 0.9185
      Episode_Termination/time_out: 0.9694
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0306

---

                   Total timesteps: 422400
                    Iteration time: 20.37s
                      Time elapsed: 01:28:56
                               ETA: 14:41:36

################################################################################
                      Learning iteration 525/3250

                       Computation: 75 steps/s (collection: 20.292s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 8.4299
               Mean surrogate loss: 0.0854
                 Mean entropy loss: 2.6213
                       Mean reward: -49.13
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0010
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2024
          Episode_Reward/collision: -0.1085
        Episode_Reward/action_rate: -0.0770
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 3.7069
Metrics/target_pose/orientation_error: 0.8339
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 423936
                    Iteration time: 20.34s
                      Time elapsed: 01:29:16
                               ETA: 14:41:26

################################################################################
                      Learning iteration 526/3250

                       Computation: 75 steps/s (collection: 20.294s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 9.4622
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 2.6226
                       Mean reward: -48.83
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2008
          Episode_Reward/collision: -0.1089
        Episode_Reward/action_rate: -0.0690
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 4.0218
Metrics/target_pose/orientation_error: 1.1558
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 425472
                    Iteration time: 20.35s
                      Time elapsed: 01:29:36
                               ETA: 14:41:16

################################################################################
                      Learning iteration 527/3250

                       Computation: 74 steps/s (collection: 20.441s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 5.4049
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 2.6227
                       Mean reward: -43.16
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2009
          Episode_Reward/collision: -0.0278
        Episode_Reward/action_rate: -0.0058
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 3.9190
Metrics/target_pose/orientation_error: 1.2130
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 427008
                    Iteration time: 20.50s
                      Time elapsed: 01:29:57
                               ETA: 14:41:07

################################################################################
                      Learning iteration 528/3250

                       Computation: 75 steps/s (collection: 20.408s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 15.9650
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 2.6231
                       Mean reward: -40.59
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2021
          Episode_Reward/collision: -0.0087
        Episode_Reward/action_rate: -0.0278
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 3.3826
Metrics/target_pose/orientation_error: 1.7114
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 428544
                    Iteration time: 20.46s
                      Time elapsed: 01:30:17
                               ETA: 14:40:58

################################################################################
                      Learning iteration 529/3250

                       Computation: 75 steps/s (collection: 20.391s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 5.1023
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 2.6232
                       Mean reward: -36.48
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0008
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1994
          Episode_Reward/collision: -0.1319
        Episode_Reward/action_rate: -0.0085
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 4.2898
Metrics/target_pose/orientation_error: 2.4854
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 430080
                    Iteration time: 20.45s
                      Time elapsed: 01:30:38
                               ETA: 14:40:48

################################################################################
                      Learning iteration 530/3250

                       Computation: 74 steps/s (collection: 20.439s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 4.1031
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 2.6233
                       Mean reward: -32.18
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2007
          Episode_Reward/collision: -0.1389
        Episode_Reward/action_rate: -0.1198
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 4.2459
Metrics/target_pose/orientation_error: 0.9905
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 431616
                    Iteration time: 20.49s
                      Time elapsed: 01:30:58
                               ETA: 14:40:39

################################################################################
                      Learning iteration 531/3250

                       Computation: 74 steps/s (collection: 20.439s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 2.8430
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 2.6235
                       Mean reward: -31.33
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0011
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2023
          Episode_Reward/collision: -0.0625
        Episode_Reward/action_rate: -0.0028
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 4.8127
Metrics/target_pose/orientation_error: 1.8013
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 433152
                    Iteration time: 20.49s
                      Time elapsed: 01:31:19
                               ETA: 14:40:30

################################################################################
                      Learning iteration 532/3250

                       Computation: 74 steps/s (collection: 20.489s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 43.4958
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 2.6239
                       Mean reward: -23.56
               Mean episode length: 177.94
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0145
          Episode_Reward/face_goal: 0.2056
          Episode_Reward/collision: -0.7014
        Episode_Reward/action_rate: -0.0470
        Episode_Reward/stand_still: -0.0992
Metrics/target_pose/position_error: 4.6148
Metrics/target_pose/orientation_error: 1.3380
      Episode_Termination/time_out: 0.9909
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0091

---

                   Total timesteps: 434688
                    Iteration time: 20.55s
                      Time elapsed: 01:31:39
                               ETA: 14:40:21

################################################################################
                      Learning iteration 533/3250

                       Computation: 75 steps/s (collection: 20.416s, learning 0.047s)
             Mean action noise std: 0.90
          Mean value_function loss: 84.7454
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 2.6244
                       Mean reward: -25.63
               Mean episode length: 177.94
   Episode_Reward/progress_to_goal: 0.0013
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2166
          Episode_Reward/collision: -1.1806
        Episode_Reward/action_rate: -0.0100
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 4.2816
Metrics/target_pose/orientation_error: 1.4637
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 436224
                    Iteration time: 20.46s
                      Time elapsed: 01:32:00
                               ETA: 14:40:12

################################################################################
                      Learning iteration 534/3250

                       Computation: 74 steps/s (collection: 20.448s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 85.1972
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 2.6248
                       Mean reward: -7.97
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: -0.0000
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2085
          Episode_Reward/collision: -0.4809
        Episode_Reward/action_rate: -0.1618
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 5.6966
Metrics/target_pose/orientation_error: 1.6521
      Episode_Termination/time_out: 0.9544
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0456

---

                   Total timesteps: 437760
                    Iteration time: 20.50s
                      Time elapsed: 01:32:20
                               ETA: 14:40:03

################################################################################
                      Learning iteration 535/3250

                       Computation: 74 steps/s (collection: 20.484s, learning 0.047s)
             Mean action noise std: 0.90
          Mean value_function loss: 8.5089
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 2.6250
                       Mean reward: -9.65
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0021
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2050
          Episode_Reward/collision: -0.2122
        Episode_Reward/action_rate: -2.3061
        Episode_Reward/stand_still: -0.0981
Metrics/target_pose/position_error: 5.0007
Metrics/target_pose/orientation_error: 1.7861
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 439296
                    Iteration time: 20.53s
                      Time elapsed: 01:32:41
                               ETA: 14:39:53

################################################################################
                      Learning iteration 536/3250

                       Computation: 74 steps/s (collection: 20.450s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 18.0446
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 2.6247
                       Mean reward: -9.64
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: -0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1983
          Episode_Reward/collision: -0.6979
        Episode_Reward/action_rate: -0.0153
        Episode_Reward/stand_still: -0.0999
Metrics/target_pose/position_error: 3.4950
Metrics/target_pose/orientation_error: 1.8541
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 440832
                    Iteration time: 20.51s
                      Time elapsed: 01:33:01
                               ETA: 14:39:44

################################################################################
                      Learning iteration 537/3250

                       Computation: 74 steps/s (collection: 20.437s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 4.0114
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 2.6245
                       Mean reward: -9.48
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2010
          Episode_Reward/collision: -0.0469
        Episode_Reward/action_rate: -0.0024
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.5751
Metrics/target_pose/orientation_error: 1.4295
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 442368
                    Iteration time: 20.49s
                      Time elapsed: 01:33:22
                               ETA: 14:39:34

################################################################################
                      Learning iteration 538/3250

                       Computation: 74 steps/s (collection: 20.462s, learning 0.048s)
             Mean action noise std: 0.90
          Mean value_function loss: 69.8276
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 2.6247
                       Mean reward: -9.65
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2022
          Episode_Reward/collision: -0.0608
        Episode_Reward/action_rate: -0.0208
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 3.9877
Metrics/target_pose/orientation_error: 1.0725
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 443904
                    Iteration time: 20.51s
                      Time elapsed: 01:33:42
                               ETA: 14:39:25

################################################################################
                      Learning iteration 539/3250

                       Computation: 75 steps/s (collection: 20.383s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 57.1323
               Mean surrogate loss: 0.0191
                 Mean entropy loss: 2.6243
                       Mean reward: -13.58
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1984
          Episode_Reward/collision: -0.4553
        Episode_Reward/action_rate: -0.0367
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 2.9331
Metrics/target_pose/orientation_error: 2.3044
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 445440
                    Iteration time: 20.43s
                      Time elapsed: 01:34:03
                               ETA: 14:39:14

################################################################################
                      Learning iteration 540/3250

                       Computation: 74 steps/s (collection: 20.488s, learning 0.053s)
             Mean action noise std: 0.90
          Mean value_function loss: 3.2130
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 2.6241
                       Mean reward: -12.59
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2015
          Episode_Reward/collision: -0.0217
        Episode_Reward/action_rate: -0.0025
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 3.4717
Metrics/target_pose/orientation_error: 1.8446
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 446976
                    Iteration time: 20.54s
                      Time elapsed: 01:34:23
                               ETA: 14:39:05

################################################################################
                      Learning iteration 541/3250

                       Computation: 74 steps/s (collection: 20.473s, learning 0.051s)
             Mean action noise std: 0.90
          Mean value_function loss: 1.1843
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 2.6239
                       Mean reward: -12.41
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2020
          Episode_Reward/collision: -0.0681
        Episode_Reward/action_rate: -0.0220
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.1677
Metrics/target_pose/orientation_error: 1.9405
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 448512
                    Iteration time: 20.52s
                      Time elapsed: 01:34:44
                               ETA: 14:38:55

################################################################################
                      Learning iteration 542/3250

                       Computation: 75 steps/s (collection: 20.415s, learning 0.048s)
             Mean action noise std: 0.90
          Mean value_function loss: 23.8670
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 2.6237
                       Mean reward: -12.97
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2010
          Episode_Reward/collision: -0.1016
        Episode_Reward/action_rate: -0.6301
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 2.7835
Metrics/target_pose/orientation_error: 2.0360
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 450048
                    Iteration time: 20.46s
                      Time elapsed: 01:35:04
                               ETA: 14:38:45

################################################################################
                      Learning iteration 543/3250

                       Computation: 74 steps/s (collection: 20.512s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 1472.9826
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 2.6239
                       Mean reward: -13.94
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2002
          Episode_Reward/collision: -0.1020
        Episode_Reward/action_rate: -0.3256
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.0273
Metrics/target_pose/orientation_error: 1.8242
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 451584
                    Iteration time: 20.57s
                      Time elapsed: 01:35:25
                               ETA: 14:38:36

################################################################################
                      Learning iteration 544/3250

                       Computation: 74 steps/s (collection: 20.532s, learning 0.048s)
             Mean action noise std: 0.90
          Mean value_function loss: 97.6260
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 2.6245
                       Mean reward: -13.22
               Mean episode length: 179.73
   Episode_Reward/progress_to_goal: 0.0009
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2030
          Episode_Reward/collision: -0.0703
        Episode_Reward/action_rate: -0.0761
        Episode_Reward/stand_still: -0.0999
Metrics/target_pose/position_error: 2.9578
Metrics/target_pose/orientation_error: 2.1767
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 453120
                    Iteration time: 20.58s
                      Time elapsed: 01:35:45
                               ETA: 14:38:27

################################################################################
                      Learning iteration 545/3250

                       Computation: 74 steps/s (collection: 20.596s, learning 0.050s)
             Mean action noise std: 0.90

            Mean action noise std: 0.90
          Mean value_function loss: 82.0242
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 2.6249
                       Mean reward: -26.53
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2040
          Episode_Reward/collision: -0.2760
        Episode_Reward/action_rate: -2.6333
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 4.7920
Metrics/target_pose/orientation_error: 1.2354
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 454656
                    Iteration time: 20.65s
                      Time elapsed: 01:36:06
                               ETA: 14:38:18

################################################################################
                      Learning iteration 546/3250

                       Computation: 74 steps/s (collection: 20.626s, learning 0.058s)
             Mean action noise std: 0.90
          Mean value_function loss: 20.8269
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 2.6249
                       Mean reward: -29.98
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0000
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2175
          Episode_Reward/collision: -7.5781
        Episode_Reward/action_rate: -0.0549
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 4.4290
Metrics/target_pose/orientation_error: 1.9488
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 456192
                    Iteration time: 20.68s
                      Time elapsed: 01:36:27
                               ETA: 14:38:09

################################################################################
                      Learning iteration 547/3250

                       Computation: 74 steps/s (collection: 20.605s, learning 0.058s)
             Mean action noise std: 0.90
          Mean value_function loss: 146.3069
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 2.6251
                       Mean reward: -25.47
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0000
         Episode_Reward/reach_goal: 0.0014
          Episode_Reward/face_goal: 0.2011
          Episode_Reward/collision: -0.0681
        Episode_Reward/action_rate: -0.0784
        Episode_Reward/stand_still: -0.0991
Metrics/target_pose/position_error: 3.7981
Metrics/target_pose/orientation_error: 1.5688
      Episode_Termination/time_out: 0.9889
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0111

---

                   Total timesteps: 457728
                    Iteration time: 20.66s
                      Time elapsed: 01:36:47
                               ETA: 14:38:00

################################################################################
                      Learning iteration 548/3250

                       Computation: 74 steps/s (collection: 20.586s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 236.9600
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 2.6252
                       Mean reward: -30.22
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1986
          Episode_Reward/collision: -0.4028
        Episode_Reward/action_rate: -0.6895
        Episode_Reward/stand_still: -0.0992
Metrics/target_pose/position_error: 3.2798
Metrics/target_pose/orientation_error: 2.2547
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 459264
                    Iteration time: 20.64s
                      Time elapsed: 01:37:08
                               ETA: 14:37:51

################################################################################
                      Learning iteration 549/3250

                       Computation: 74 steps/s (collection: 20.628s, learning 0.053s)
             Mean action noise std: 0.90
          Mean value_function loss: 9.8070
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 2.6254
                       Mean reward: -36.47
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0003
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1998
          Episode_Reward/collision: -0.3520
        Episode_Reward/action_rate: -0.1900
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.3810
Metrics/target_pose/orientation_error: 1.8946
      Episode_Termination/time_out: 0.9564
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0436

---

                   Total timesteps: 460800
                    Iteration time: 20.68s
                      Time elapsed: 01:37:29
                               ETA: 14:37:42

################################################################################
                      Learning iteration 550/3250

                       Computation: 74 steps/s (collection: 20.577s, learning 0.053s)
             Mean action noise std: 0.90
          Mean value_function loss: 2.7591
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 2.6261
                       Mean reward: -36.85
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0009
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2017
          Episode_Reward/collision: -0.1597
        Episode_Reward/action_rate: -0.1565
        Episode_Reward/stand_still: -0.0989
Metrics/target_pose/position_error: 3.5335
Metrics/target_pose/orientation_error: 1.2154
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 462336
                    Iteration time: 20.63s
                      Time elapsed: 01:37:49
                               ETA: 14:37:33

################################################################################
                      Learning iteration 551/3250

                       Computation: 74 steps/s (collection: 20.609s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 46.3041
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 2.6275
                       Mean reward: -33.77
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2001
          Episode_Reward/collision: -0.0369
        Episode_Reward/action_rate: -0.1352
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 5.0979
Metrics/target_pose/orientation_error: 1.5780
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 463872
                    Iteration time: 20.66s
                      Time elapsed: 01:38:10
                               ETA: 14:37:24

################################################################################
                      Learning iteration 552/3250

                       Computation: 74 steps/s (collection: 20.589s, learning 0.054s)
             Mean action noise std: 0.90
          Mean value_function loss: 89.6612
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 2.6277
                       Mean reward: -35.63
               Mean episode length: 177.10
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0637
          Episode_Reward/face_goal: 0.1861
          Episode_Reward/collision: -1.3954
        Episode_Reward/action_rate: -0.0362
        Episode_Reward/stand_still: -0.0855
Metrics/target_pose/position_error: 4.4789
Metrics/target_pose/orientation_error: 0.9033
      Episode_Termination/time_out: 0.9557
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0443

---

                   Total timesteps: 465408
                    Iteration time: 20.64s
                      Time elapsed: 01:38:31
                               ETA: 14:37:15

################################################################################
                      Learning iteration 553/3250

                       Computation: 74 steps/s (collection: 20.592s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 41.4285
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 2.6278
                       Mean reward: -40.31
               Mean episode length: 177.10
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1734
          Episode_Reward/collision: -2.7431
        Episode_Reward/action_rate: -0.1659
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.6699
Metrics/target_pose/orientation_error: 1.0430
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 466944
                    Iteration time: 20.65s
                      Time elapsed: 01:38:51
                               ETA: 14:37:05

################################################################################
                      Learning iteration 554/3250

                       Computation: 74 steps/s (collection: 20.540s, learning 0.058s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.9107
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 2.6278
                       Mean reward: -40.86
               Mean episode length: 177.10
   Episode_Reward/progress_to_goal: -0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1992
          Episode_Reward/collision: -0.1493
        Episode_Reward/action_rate: -0.1467
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 5.7281
Metrics/target_pose/orientation_error: 1.6632
      Episode_Termination/time_out: 0.9896
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0104

---

                   Total timesteps: 468480
                    Iteration time: 20.60s
                      Time elapsed: 01:39:12
                               ETA: 14:36:55

################################################################################
                      Learning iteration 555/3250

                       Computation: 74 steps/s (collection: 20.586s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 76.1535
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 2.6274
                       Mean reward: -40.11
               Mean episode length: 177.10
   Episode_Reward/progress_to_goal: 0.0000
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2008
          Episode_Reward/collision: -0.0226
        Episode_Reward/action_rate: -0.2212
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 5.3541
Metrics/target_pose/orientation_error: 2.0352
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 470016
                    Iteration time: 20.64s
                      Time elapsed: 01:39:33
                               ETA: 14:36:46

################################################################################
                      Learning iteration 556/3250

                       Computation: 74 steps/s (collection: 20.675s, learning 0.060s)
             Mean action noise std: 0.90
          Mean value_function loss: 37.4935
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 2.6266
                       Mean reward: -28.48
               Mean episode length: 177.10
   Episode_Reward/progress_to_goal: 0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1997
          Episode_Reward/collision: -0.3529
        Episode_Reward/action_rate: -0.0237
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 5.0460
Metrics/target_pose/orientation_error: 1.9845
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 471552
                    Iteration time: 20.73s
                      Time elapsed: 01:39:53
                               ETA: 14:36:37

################################################################################
                      Learning iteration 557/3250

                       Computation: 74 steps/s (collection: 20.632s, learning 0.053s)
             Mean action noise std: 0.90
          Mean value_function loss: 4072.7209
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 2.6267
                       Mean reward: -37.16
               Mean episode length: 177.10
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2013
          Episode_Reward/collision: -0.0868
        Episode_Reward/action_rate: -2.5428
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 4.2012
Metrics/target_pose/orientation_error: 1.5255
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 473088
                    Iteration time: 20.69s
                      Time elapsed: 01:40:14
                               ETA: 14:36:27

################################################################################
                      Learning iteration 558/3250

                       Computation: 74 steps/s (collection: 20.638s, learning 0.051s)
             Mean action noise std: 0.90
          Mean value_function loss: 2.4143
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 2.6270
                       Mean reward: -32.85
               Mean episode length: 177.10
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2006
          Episode_Reward/collision: -0.1228
        Episode_Reward/action_rate: -4.4113
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 4.9988
Metrics/target_pose/orientation_error: 1.3209
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 474624
                    Iteration time: 20.69s
                      Time elapsed: 01:40:35
                               ETA: 14:36:18

################################################################################
                      Learning iteration 559/3250

                       Computation: 74 steps/s (collection: 20.660s, learning 0.054s)
             Mean action noise std: 0.90
          Mean value_function loss: 1.5195
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 2.6269
                       Mean reward: -32.63
               Mean episode length: 178.89
   Episode_Reward/progress_to_goal: 0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2021
          Episode_Reward/collision: -0.0486
        Episode_Reward/action_rate: -0.0022
        Episode_Reward/stand_still: -0.0999
Metrics/target_pose/position_error: 5.9531
Metrics/target_pose/orientation_error: 1.0162
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 476160
                    Iteration time: 20.71s
                      Time elapsed: 01:40:55
                               ETA: 14:36:09

################################################################################
                      Learning iteration 560/3250

                       Computation: 74 steps/s (collection: 20.612s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 92.4221
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 2.6267
                       Mean reward: -24.18
               Mean episode length: 178.89
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2015
          Episode_Reward/collision: -0.0981
        Episode_Reward/action_rate: -0.0507
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 4.8133
Metrics/target_pose/orientation_error: 1.7941
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 477696
                    Iteration time: 20.67s
                      Time elapsed: 01:41:16
                               ETA: 14:35:59

################################################################################
                      Learning iteration 561/3250

                       Computation: 74 steps/s (collection: 20.602s, learning 0.051s)
             Mean action noise std: 0.90
          Mean value_function loss: 3.7144
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 2.6267
                       Mean reward: -24.08
               Mean episode length: 178.89
   Episode_Reward/progress_to_goal: -0.0003
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2010
          Episode_Reward/collision: -0.1189
        Episode_Reward/action_rate: -0.6498
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 3.7147
Metrics/target_pose/orientation_error: 1.4573
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 479232
                    Iteration time: 20.65s
                      Time elapsed: 01:41:37
                               ETA: 14:35:49

################################################################################
                      Learning iteration 562/3250

                       Computation: 74 steps/s (collection: 20.656s, learning 0.053s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.5351
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 2.6270
                       Mean reward: -25.90
               Mean episode length: 178.89
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2007
          Episode_Reward/collision: -0.0959
        Episode_Reward/action_rate: -0.4344
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 4.7235
Metrics/target_pose/orientation_error: 1.6934
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 480768
                    Iteration time: 20.71s
                      Time elapsed: 01:41:57
                               ETA: 14:35:40

################################################################################
                      Learning iteration 563/3250

                       Computation: 74 steps/s (collection: 20.637s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.5415
               Mean surrogate loss: 0.0182
                 Mean entropy loss: 2.6280
                       Mean reward: -25.06
               Mean episode length: 178.89
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1995
          Episode_Reward/collision: -0.0538
        Episode_Reward/action_rate: -0.0132
        Episode_Reward/stand_still: -0.1000
Metrics/target_pose/position_error: 4.5511
Metrics/target_pose/orientation_error: 1.0714
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 482304
                    Iteration time: 20.69s
                      Time elapsed: 01:42:18
                               ETA: 14:35:30

################################################################################
                      Learning iteration 564/3250

                       Computation: 74 steps/s (collection: 20.665s, learning 0.054s)
             Mean action noise std: 0.90
          Mean value_function loss: 89.6437
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 2.6284
                       Mean reward: -18.90
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2021
          Episode_Reward/collision: -0.0204
        Episode_Reward/action_rate: -0.0284
        Episode_Reward/stand_still: -0.0999
Metrics/target_pose/position_error: 5.4527
Metrics/target_pose/orientation_error: 2.1876
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 483840
                    Iteration time: 20.72s
                      Time elapsed: 01:42:39
                               ETA: 14:35:20

################################################################################
                      Learning iteration 565/3250

                       Computation: 74 steps/s (collection: 20.627s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 96.4130
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 2.6281
                       Mean reward: -17.96
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0000
         Episode_Reward/reach_goal: 0.0116
          Episode_Reward/face_goal: 0.1921
          Episode_Reward/collision: -0.1220
        Episode_Reward/action_rate: -0.0326
        Episode_Reward/stand_still: -0.0953
Metrics/target_pose/position_error: 5.9332
Metrics/target_pose/orientation_error: 1.4602
      Episode_Termination/time_out: 0.9961
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0039

---

                   Total timesteps: 485376
                    Iteration time: 20.68s
                      Time elapsed: 01:43:00
                               ETA: 14:35:10

################################################################################
                      Learning iteration 566/3250

                       Computation: 74 steps/s (collection: 20.666s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 367.8538
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 2.6270
                       Mean reward: -25.54
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1653
          Episode_Reward/collision: -4.1302
        Episode_Reward/action_rate: -2.4874
        Episode_Reward/stand_still: -0.1000
Metrics/target_pose/position_error: 4.3159
Metrics/target_pose/orientation_error: 1.5039
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 486912
                    Iteration time: 20.72s
                      Time elapsed: 01:43:20
                               ETA: 14:35:01

################################################################################
                      Learning iteration 567/3250

                       Computation: 73 steps/s (collection: 20.750s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 72.4316
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.6271
                       Mean reward: -32.99
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1935
          Episode_Reward/collision: -0.8741
        Episode_Reward/action_rate: -0.4857
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.5709
Metrics/target_pose/orientation_error: 1.5435
      Episode_Termination/time_out: 0.9492
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0508

---

                   Total timesteps: 488448
                    Iteration time: 20.81s
                      Time elapsed: 01:43:41
                               ETA: 14:34:51

################################################################################
                      Learning iteration 568/3250

                       Computation: 74 steps/s (collection: 20.684s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 64.4103
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 2.6280
                       Mean reward: -30.40
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2012
          Episode_Reward/collision: -0.0538
        Episode_Reward/action_rate: -0.0742
        Episode_Reward/stand_still: -0.0999
Metrics/target_pose/position_error: 4.0046
Metrics/target_pose/orientation_error: 1.5805
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 489984
                    Iteration time: 20.73s
                      Time elapsed: 01:44:02
                               ETA: 14:34:42

################################################################################
                      Learning iteration 569/3250

                       Computation: 73 steps/s (collection: 20.757s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 85.6155
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 2.6284
                       Mean reward: -22.33
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2027
          Episode_Reward/collision: -0.2804
        Episode_Reward/action_rate: -0.2949
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 4.9962
Metrics/target_pose/orientation_error: 1.7107
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 491520
                    Iteration time: 20.81s
                      Time elapsed: 01:44:23
                               ETA: 14:34:33

################################################################################
                      Learning iteration 570/3250

                       Computation: 73 steps/s (collection: 20.772s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 8.0311
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 2.6288
                       Mean reward: -27.55
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2553
          Episode_Reward/collision: -3.2812
        Episode_Reward/action_rate: -0.8742
        Episode_Reward/stand_still: -0.1000
Metrics/target_pose/position_error: 4.5237
Metrics/target_pose/orientation_error: 1.3791
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 493056
                    Iteration time: 20.82s
                      Time elapsed: 01:44:43
                               ETA: 14:34:23

################################################################################
                      Learning iteration 571/3250

                       Computation: 73 steps/s (collection: 20.746s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 9.5354
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 2.6295
                       Mean reward: -28.29
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2015
          Episode_Reward/collision: -0.1259
        Episode_Reward/action_rate: -0.4549
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 4.3211
Metrics/target_pose/orientation_error: 2.0885
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 494592
                    Iteration time: 20.80s
                      Time elapsed: 01:45:04
                               ETA: 14:34:14

################################################################################
                      Learning iteration 572/3250

                       Computation: 74 steps/s (collection: 20.695s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 527.3379
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 2.6301
                       Mean reward: -28.58
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0019
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2014
          Episode_Reward/collision: -0.1276
        Episode_Reward/action_rate: -0.1188
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 5.6639
Metrics/target_pose/orientation_error: 2.0216
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 496128
                    Iteration time: 20.74s
                      Time elapsed: 01:45:25
                               ETA: 14:34:04

################################################################################
                      Learning iteration 573/3250

                       Computation: 74 steps/s (collection: 20.703s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 39.1608
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 2.6302
                       Mean reward: -35.86
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0003
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1821
          Episode_Reward/collision: -2.0495
        Episode_Reward/action_rate: -3.0769
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 4.9758
Metrics/target_pose/orientation_error: 1.5220
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 497664
                    Iteration time: 20.75s
                      Time elapsed: 01:45:46
                               ETA: 14:33:54

################################################################################
                      Learning iteration 574/3250

                       Computation: 73 steps/s (collection: 20.756s, learning 0.054s)
             Mean action noise std: 0.90
          Mean value_function loss: 45.6545
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 2.6303
                       Mean reward: -39.36
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0006
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2021
          Episode_Reward/collision: -0.1120
        Episode_Reward/action_rate: -2.3220
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 4.3536
Metrics/target_pose/orientation_error: 1.6107
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 499200
                    Iteration time: 20.81s
                      Time elapsed: 01:46:07
                               ETA: 14:33:45

################################################################################
                      Learning iteration 575/3250

                       Computation: 73 steps/s (collection: 20.809s, learning 0.048s)
             Mean action noise std: 0.90
          Mean value_function loss: 16.7203
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 2.6305
                       Mean reward: -43.02
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: -0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2007
          Episode_Reward/collision: -0.1328
        Episode_Reward/action_rate: -1.1435
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 4.2319
Metrics/target_pose/orientation_error: 1.7696
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 500736
                    Iteration time: 20.86s
                      Time elapsed: 01:46:27
                               ETA: 14:33:36

################################################################################
                      Learning iteration 576/3250

                       Computation: 73 steps/s (collection: 20.795s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 80.7234
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 2.6294
                       Mean reward: -43.90
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0009
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2017
          Episode_Reward/collision: -0.1589
        Episode_Reward/action_rate: -0.4347
        Episode_Reward/stand_still: -0.0992
Metrics/target_pose/position_error: 4.5831
Metrics/target_pose/orientation_error: 0.8834
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 502272
                    Iteration time: 20.84s
                      Time elapsed: 01:46:48
                               ETA: 14:33:26

################################################################################
                      Learning iteration 577/3250

                       Computation: 73 steps/s (collection: 20.811s, learning 0.057s)
             Mean action noise std: 0.90
          Mean value_function loss: 39.0536
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 2.6288
                       Mean reward: -48.21
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2060
          Episode_Reward/collision: -1.1146
        Episode_Reward/action_rate: -0.0248
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.2669
Metrics/target_pose/orientation_error: 0.8996
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 503808
                    Iteration time: 20.87s
                      Time elapsed: 01:47:09
                               ETA: 14:33:17

################################################################################
                      Learning iteration 578/3250

                       Computation: 73 steps/s (collection: 20.760s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 2.1803
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 2.6281
                       Mean reward: -40.25
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0010
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2016
          Episode_Reward/collision: -0.1424
        Episode_Reward/action_rate: -0.0049
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 4.1362
Metrics/target_pose/orientation_error: 1.9675
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 505344
                    Iteration time: 20.81s
                      Time elapsed: 01:47:30
                               ETA: 14:33:07

################################################################################
                      Learning iteration 579/3250

                       Computation: 73 steps/s (collection: 20.853s, learning 0.051s)
             Mean action noise std: 0.90
          Mean value_function loss: 1.8862
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 2.6290
                       Mean reward: -33.32
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0003
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2012
          Episode_Reward/collision: -0.0773
        Episode_Reward/action_rate: -0.0916
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 5.0969
Metrics/target_pose/orientation_error: 2.3635
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 506880
                    Iteration time: 20.90s
                      Time elapsed: 01:47:51
                               ETA: 14:32:58

################################################################################
                      Learning iteration 580/3250

                       Computation: 73 steps/s (collection: 20.851s, learning 0.053s)
             Mean action noise std: 0.90
          Mean value_function loss: 5.5764
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 2.6291
                       Mean reward: -32.24
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0010
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2006
          Episode_Reward/collision: -0.2161
        Episode_Reward/action_rate: -0.0782
        Episode_Reward/stand_still: -0.0991
Metrics/target_pose/position_error: 5.4275
Metrics/target_pose/orientation_error: 1.9731
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 508416
                    Iteration time: 20.90s
                      Time elapsed: 01:48:12
                               ETA: 14:32:49

################################################################################
                      Learning iteration 581/3250

                       Computation: 73 steps/s (collection: 20.821s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 140.4648
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 2.6290
                       Mean reward: -27.25
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2015
          Episode_Reward/collision: -0.0616
        Episode_Reward/action_rate: -0.0369
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 5.4202
Metrics/target_pose/orientation_error: 1.5662
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 509952
                    Iteration time: 20.87s
                      Time elapsed: 01:48:33
                               ETA: 14:32:39

################################################################################
                      Learning iteration 582/3250

                       Computation: 73 steps/s (collection: 20.786s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 141.8441
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 2.6290
                       Mean reward: -35.74
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0000
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2100
          Episode_Reward/collision: -0.9262
        Episode_Reward/action_rate: -0.2143
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 4.5292
Metrics/target_pose/orientation_error: 1.0894
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 511488
                    Iteration time: 20.84s
                      Time elapsed: 01:48:53
                               ETA: 14:32:29

################################################################################
                      Learning iteration 583/3250

                       Computation: 73 steps/s (collection: 20.758s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 2.2840
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 2.6290
                       Mean reward: -36.89
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2025
          Episode_Reward/collision: -0.0451
        Episode_Reward/action_rate: -0.4594
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 4.7048
Metrics/target_pose/orientation_error: 1.7070
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 513024
                    Iteration time: 20.81s
                      Time elapsed: 01:49:14
                               ETA: 14:32:19

################################################################################
                      Learning iteration 584/3250

                       Computation: 73 steps/s (collection: 20.774s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 800.8326
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 2.6288
                       Mean reward: -34.69
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0014
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2025
          Episode_Reward/collision: -0.1641
        Episode_Reward/action_rate: -0.4715
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 3.6301
Metrics/target_pose/orientation_error: 1.9096
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 514560
                    Iteration time: 20.82s
                      Time elapsed: 01:49:35
                               ETA: 14:32:09

################################################################################
                      Learning iteration 585/3250

                       Computation: 73 steps/s (collection: 20.805s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 134.6815
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 2.6288
                       Mean reward: -25.02
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2005
          Episode_Reward/collision: -0.0278
        Episode_Reward/action_rate: -0.0120
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 4.6390
Metrics/target_pose/orientation_error: 0.8086
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 516096
                    Iteration time: 20.86s
                      Time elapsed: 01:49:56
                               ETA: 14:31:59

################################################################################
                      Learning iteration 586/3250

                       Computation: 73 steps/s (collection: 20.852s, learning 0.048s)
             Mean action noise std: 0.90
          Mean value_function loss: 90.0898
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 2.6291
                       Mean reward: -35.54
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2067
          Episode_Reward/collision: -0.8212
        Episode_Reward/action_rate: -1.0277
        Episode_Reward/stand_still: -0.0991
Metrics/target_pose/position_error: 4.2301
Metrics/target_pose/orientation_error: 1.6400
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 517632
                    Iteration time: 20.90s
                      Time elapsed: 01:50:17
                               ETA: 14:31:50

################################################################################
                      Learning iteration 587/3250

                       Computation: 73 steps/s (collection: 20.798s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 44.9965
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 2.6303
                       Mean reward: -34.13
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2023
          Episode_Reward/collision: -0.0755
        Episode_Reward/action_rate: -0.2357
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 3.7818
Metrics/target_pose/orientation_error: 1.3302
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 519168
                    Iteration time: 20.85s
                      Time elapsed: 01:50:38
                               ETA: 14:31:40

################################################################################
                      Learning iteration 588/3250

                       Computation: 73 steps/s (collection: 20.832s, learning 0.053s)
             Mean action noise std: 0.90
          Mean value_function loss: 6.4533
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 2.6311
                       Mean reward: -33.20
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2003
          Episode_Reward/collision: -0.0686
        Episode_Reward/action_rate: -0.3960
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 4.4956
Metrics/target_pose/orientation_error: 1.2476
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 520704
                    Iteration time: 20.88s
                      Time elapsed: 01:50:59
                               ETA: 14:31:30

################################################################################
                      Learning iteration 589/3250

                       Computation: 73 steps/s (collection: 20.815s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 2.5073
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 2.6329
                       Mean reward: -29.01
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2002
          Episode_Reward/collision: -0.1267
        Episode_Reward/action_rate: -0.0999
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 4.8828
Metrics/target_pose/orientation_error: 1.1422
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 522240
                    Iteration time: 20.86s
                      Time elapsed: 01:51:19
                               ETA: 14:31:20

################################################################################
                      Learning iteration 590/3250

                       Computation: 73 steps/s (collection: 20.920s, learning 0.058s)
             Mean action noise std: 0.90
          Mean value_function loss: 2.6805
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 2.6342
                       Mean reward: -29.99
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2004
          Episode_Reward/collision: -0.0981
        Episode_Reward/action_rate: -0.8050
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 5.6234
Metrics/target_pose/orientation_error: 1.7690
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 523776
                    Iteration time: 20.98s
                      Time elapsed: 01:51:40
                               ETA: 14:31:10

################################################################################
                      Learning iteration 591/3250

                       Computation: 73 steps/s (collection: 20.934s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 68.7606
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 2.6343
                       Mean reward: -28.91
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1995
          Episode_Reward/collision: -0.0425
        Episode_Reward/action_rate: -0.0048
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 4.2511
Metrics/target_pose/orientation_error: 1.4251
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 525312
                    Iteration time: 20.98s
                      Time elapsed: 01:52:01
                               ETA: 14:31:01

################################################################################
                      Learning iteration 592/3250

                       Computation: 73 steps/s (collection: 20.980s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 64.5970
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 2.6342
                       Mean reward: -33.40
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2055
          Episode_Reward/collision: -0.3255
        Episode_Reward/action_rate: -0.0468
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 5.7859
Metrics/target_pose/orientation_error: 1.7455
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 526848
                    Iteration time: 21.03s
                      Time elapsed: 01:52:22
                               ETA: 14:30:52

################################################################################
                      Learning iteration 593/3250

                       Computation: 73 steps/s (collection: 20.893s, learning 0.047s)
             Mean action noise std: 0.90
          Mean value_function loss: 235.5897
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 2.6332
                       Mean reward: -33.30
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2005
          Episode_Reward/collision: -0.0833
        Episode_Reward/action_rate: -0.0058
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 2.8854
Metrics/target_pose/orientation_error: 1.1773
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 528384
                    Iteration time: 20.94s
                      Time elapsed: 01:52:43
                               ETA: 14:30:42

################################################################################
                      Learning iteration 594/3250

                       Computation: 73 steps/s (collection: 20.966s, learning 0.051s)
             Mean action noise std: 0.90
          Mean value_function loss: 69.3103
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 2.6331
                       Mean reward: -23.11
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0003
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2007
          Episode_Reward/collision: -0.0521
        Episode_Reward/action_rate: -0.0061
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 5.1556
Metrics/target_pose/orientation_error: 1.3087
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 529920
                    Iteration time: 21.02s
                      Time elapsed: 01:53:04
                               ETA: 14:30:33

################################################################################
                      Learning iteration 595/3250

                       Computation: 72 steps/s (collection: 20.994s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 402.9529
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 2.6333
                       Mean reward: -26.43
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0006
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2004
          Episode_Reward/collision: -0.0851
        Episode_Reward/action_rate: -2.8283
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 5.5740
Metrics/target_pose/orientation_error: 1.3893
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 531456
                    Iteration time: 21.05s
                      Time elapsed: 01:53:25
                               ETA: 14:30:24

################################################################################
                      Learning iteration 596/3250

                       Computation: 73 steps/s (collection: 20.868s, learning 0.054s)
             Mean action noise std: 0.90
          Mean value_function loss: 938.5703
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 2.6333
                       Mean reward: -29.24
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2006
          Episode_Reward/collision: -0.0425
        Episode_Reward/action_rate: -0.2294
        Episode_Reward/stand_still: -0.0998
Metrics/target_pose/position_error: 3.6464
Metrics/target_pose/orientation_error: 1.6957
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 532992
                    Iteration time: 20.92s
                      Time elapsed: 01:53:46
                               ETA: 14:30:14

################################################################################
                      Learning iteration 597/3250

                       Computation: 73 steps/s (collection: 20.892s, learning 0.048s)
             Mean action noise std: 0.90
          Mean value_function loss: 361.9212
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 2.6335
                       Mean reward: -42.13
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0003
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1955
          Episode_Reward/collision: -0.8359
        Episode_Reward/action_rate: -0.2096
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 4.5147
Metrics/target_pose/orientation_error: 1.2705
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 534528
                    Iteration time: 20.94s
                      Time elapsed: 01:54:07
                               ETA: 14:30:04

################################################################################
                      Learning iteration 598/3250

                       Computation: 73 steps/s (collection: 20.842s, learning 0.049s)
             Mean action noise std: 0.90
          Mean value_function loss: 103.7287
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 2.6334
                       Mean reward: -45.32
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2039
          Episode_Reward/collision: -0.8420
        Episode_Reward/action_rate: -0.8697
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 4.4340
Metrics/target_pose/orientation_error: 1.7067
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 536064
                    Iteration time: 20.89s
                      Time elapsed: 01:54:28
                               ETA: 14:29:54

################################################################################
                      Learning iteration 599/3250

                       Computation: 73 steps/s (collection: 20.863s, learning 0.056s)
             Mean action noise std: 0.90
          Mean value_function loss: 42.1028
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 2.6331
                       Mean reward: -45.51
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0012
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1998
          Episode_Reward/collision: -0.1207
        Episode_Reward/action_rate: -1.1655
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 3.7598
Metrics/target_pose/orientation_error: 1.5688
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 537600
                    Iteration time: 20.92s
                      Time elapsed: 01:54:49
                               ETA: 14:29:43

################################################################################
                      Learning iteration 600/3250

                       Computation: 73 steps/s (collection: 20.953s, learning 0.054s)
             Mean action noise std: 0.90
          Mean value_function loss: 97.2804
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 2.6332
                       Mean reward: -47.59
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2015
          Episode_Reward/collision: -0.1345
        Episode_Reward/action_rate: -0.0349
        Episode_Reward/stand_still: -0.0996
Metrics/target_pose/position_error: 4.4522
Metrics/target_pose/orientation_error: 1.5788
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 539136
                    Iteration time: 21.01s
                      Time elapsed: 01:55:10
                               ETA: 14:29:34

################################################################################
                      Learning iteration 601/3250

                       Computation: 73 steps/s (collection: 20.918s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 475.5898
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 2.6339
                       Mean reward: -56.49
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1990
          Episode_Reward/collision: -1.1780
        Episode_Reward/action_rate: -2.5670
        Episode_Reward/stand_still: -0.0991
Metrics/target_pose/position_error: 4.5504
Metrics/target_pose/orientation_error: 1.9301
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 540672
                    Iteration time: 20.97s
                      Time elapsed: 01:55:31
                               ETA: 14:29:23

################################################################################
                      Learning iteration 602/3250

                       Computation: 73 steps/s (collection: 20.886s, learning 0.047s)
             Mean action noise std: 0.90
          Mean value_function loss: 8.0748
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 2.6341
                       Mean reward: -57.75
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0017
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2011
          Episode_Reward/collision: -0.1771
        Episode_Reward/action_rate: -0.4102
        Episode_Reward/stand_still: -0.0990
Metrics/target_pose/position_error: 3.7174
Metrics/target_pose/orientation_error: 1.6922
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 542208
                    Iteration time: 20.93s
                      Time elapsed: 01:55:52
                               ETA: 14:29:13

################################################################################
                      Learning iteration 603/3250

                       Computation: 73 steps/s (collection: 20.953s, learning 0.050s)
             Mean action noise std: 0.90
          Mean value_function loss: 1.0799
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 2.6345
                       Mean reward: -57.65
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2022
          Episode_Reward/collision: -0.1467
        Episode_Reward/action_rate: -0.0275
        Episode_Reward/stand_still: -0.0995
Metrics/target_pose/position_error: 5.1838
Metrics/target_pose/orientation_error: 1.1608
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 543744
                    Iteration time: 21.00s
                      Time elapsed: 01:56:13
                               ETA: 14:29:03

################################################################################
                      Learning iteration 604/3250

                       Computation: 73 steps/s (collection: 20.971s, learning 0.055s)
             Mean action noise std: 0.90
          Mean value_function loss: 483.0010
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 2.6360
                       Mean reward: -54.17
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0039
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2026
          Episode_Reward/collision: -0.1875
        Episode_Reward/action_rate: -0.3762
        Episode_Reward/stand_still: -0.0984
Metrics/target_pose/position_error: 5.2540
Metrics/target_pose/orientation_error: 1.3709
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 545280
                    Iteration time: 21.03s
                      Time elapsed: 01:56:34
                               ETA: 14:28:53

################################################################################
                      Learning iteration 605/3250

                       Computation: 72 steps/s (collection: 21.060s, learning 0.057s)
             Mean action noise std: 0.90
          Mean value_function loss: 174.1155
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 2.6364
                       Mean reward: -57.75
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2053
          Episode_Reward/collision: -0.4418
        Episode_Reward/action_rate: -0.1553
        Episode_Reward/stand_still: -0.0997
Metrics/target_pose/position_error: 5.3375
Metrics/target_pose/orientation_error: 1.5135
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 546816
                    Iteration time: 21.12s
                      Time elapsed: 01:56:55
                               ETA: 14:28:44

################################################################################
                      Learning iteration 606/3250

                       Computation: 72 steps/s (collection: 20.993s, learning 0.056s)
             Mean action noise std: 0.91
          Mean value_function loss: 162.4913
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 2.6371
                       Mean reward: -65.85
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2092
          Episode_Reward/collision: -3.0000
        Episode_Reward/action_rate: -3.2466
        Episode_Reward/stand_still: -0.0993
Metrics/target_pose/position_error: 4.6459
Metrics/target_pose/orientation_error: 2.2208
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 548352
                    Iteration time: 21.05s
                      Time elapsed: 01:57:16
                               ETA: 14:28:34

################################################################################
                      Learning iteration 607/3250

                       Computation: 72 steps/s (collection: 20.996s, learning 0.051s)
             Mean action noise std: 0.91
          Mean value_function loss: 189.0454
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 2.6407
                       Mean reward: -71.55
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2601
          Episode_Reward/collision: -3.5095
        Episode_Reward/action_rate: -0.2464
        Episode_Reward/stand_still: -0.0994
Metrics/target_pose/position_error: 3.6631
Metrics/target_pose/orientation_error: 1.9166
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 549888
                    Iteration time: 21.05s
                      Time elapsed: 01:57:37
                               ETA: 14:28:25
