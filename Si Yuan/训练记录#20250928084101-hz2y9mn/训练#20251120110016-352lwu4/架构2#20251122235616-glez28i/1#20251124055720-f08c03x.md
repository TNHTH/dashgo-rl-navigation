                      Computation: 11 steps/s (collection: 130.180s, learning 0.153s)
             Mean action noise std: 1.01
          Mean value\_function loss: 1133.1848
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.8475
                       Mean reward: -111.85
               Mean episode length: 46.22
   Episode\_Reward/progress\_to\_goal: 0.0006
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.0495
          Episode\_Reward/collision: -3.7170
        Episode\_Reward/action\_rate: -0.0005
        Episode\_Reward/stand\_still: -0.0240
Metrics/target\_pose/position\_error: 4.4440
Metrics/target\_pose/orientation\_error: 1.6641
      Episode\_Termination/time\_out: 0.2975
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 1536
                    Iteration time: 130.33s
                      Time elapsed: 00:02:10
                               ETA: 12:36:37
Could not find git repository in /home/gwh/.conda/envs/env\_isaaclab/lib/python3.10/site-packages/rsl\_rl/\_\_init\_\_.py. Skipping.
################################################################################
                       Learning iteration 1/3000
                       Computation: 11 steps/s (collection: 129.116s, learning 0.053s)
             Mean action noise std: 1.01
          Mean value\_function loss: 1122.5393
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 2.8596
                       Mean reward: -240.04
               Mean episode length: 97.82
   Episode\_Reward/progress\_to\_goal: 0.0035
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1051
          Episode\_Reward/collision: -10.8655
        Episode\_Reward/action\_rate: -0.0015
        Episode\_Reward/stand\_still: -0.0683
Metrics/target\_pose/position\_error: 3.8415
Metrics/target\_pose/orientation\_error: 1.2985
      Episode\_Termination/time\_out: 0.7474
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 3072
                    Iteration time: 129.17s
                      Time elapsed: 00:04:19
                               ETA: 12:05:23
################################################################################
                       Learning iteration 2/3000
                       Computation: 11 steps/s (collection: 133.222s, learning 0.055s)
             Mean action noise std: 1.02
          Mean value\_function loss: 1094.8388
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 2.8664
                       Mean reward: -304.71
               Mean episode length: 124.12
   Episode\_Reward/progress\_to\_goal: 0.0019
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2915
          Episode\_Reward/collision: -14.9340
        Episode\_Reward/action\_rate: -0.0021
        Episode\_Reward/stand\_still: -0.0980
Metrics/target\_pose/position\_error: 4.6588
Metrics/target\_pose/orientation\_error: 1.2776
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 4608
                    Iteration time: 133.28s
                      Time elapsed: 00:06:32
                               ETA: 13:01:56
################################################################################
                       Learning iteration 3/3000
                       Computation: 11 steps/s (collection: 136.914s, learning 0.049s)
             Mean action noise std: 1.02
          Mean value\_function loss: 1020.5720
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 2.8769
                       Mean reward: -345.55
               Mean episode length: 141.19
   Episode\_Reward/progress\_to\_goal: 0.0065
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2935
          Episode\_Reward/collision: -14.6016
        Episode\_Reward/action\_rate: -0.0020
        Episode\_Reward/stand\_still: -0.0948
Metrics/target\_pose/position\_error: 5.1177
Metrics/target\_pose/orientation\_error: 1.0355
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 6144
                    Iteration time: 136.96s
                      Time elapsed: 00:08:49
                               ETA: 14:15:08
################################################################################
                       Learning iteration 4/3000
                       Computation: 10 steps/s (collection: 139.693s, learning 0.057s)
             Mean action noise std: 1.02
          Mean value\_function loss: 1096.8426
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 2.8803
                       Mean reward: -356.88
               Mean episode length: 145.93
   Episode\_Reward/progress\_to\_goal: -0.0067
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1998
          Episode\_Reward/collision: -14.6441
        Episode\_Reward/action\_rate: -0.0020
        Episode\_Reward/stand\_still: -0.0933
Metrics/target\_pose/position\_error: 5.2725
Metrics/target\_pose/orientation\_error: 1.7386
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 7680
                    Iteration time: 139.75s
                      Time elapsed: 00:11:09
                               ETA: 15:25:58
################################################################################
                       Learning iteration 5/3000
                       Computation: 10 steps/s (collection: 141.512s, learning 0.058s)
             Mean action noise std: 1.02
          Mean value\_function loss: 919.3829
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 2.8816
                       Mean reward: -373.85
               Mean episode length: 153.13
   Episode\_Reward/progress\_to\_goal: -0.0058
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2395
          Episode\_Reward/collision: -14.5564
        Episode\_Reward/action\_rate: -0.0022
        Episode\_Reward/stand\_still: -0.0924
Metrics/target\_pose/position\_error: 5.0370
Metrics/target\_pose/orientation\_error: 1.2651
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 9216
                    Iteration time: 141.57s
                      Time elapsed: 00:13:31
                               ETA: 16:27:34
################################################################################
                       Learning iteration 6/3000
                       Computation: 10 steps/s (collection: 142.443s, learning 0.048s)
             Mean action noise std: 1.02
          Mean value\_function loss: 1034.2238
               Mean surrogate loss: 0.0145
                 Mean entropy loss: 2.8782
                       Mean reward: -378.60
               Mean episode length: 155.49
   Episode\_Reward/progress\_to\_goal: -0.0069
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.3386
          Episode\_Reward/collision: -14.6059
        Episode\_Reward/action\_rate: -0.0022
        Episode\_Reward/stand\_still: -0.0915
Metrics/target\_pose/position\_error: 5.6958
Metrics/target\_pose/orientation\_error: 1.2209
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 10752
                    Iteration time: 142.49s
                      Time elapsed: 00:15:53
                               ETA: 17:17:26
################################################################################
                       Learning iteration 7/3000
                       Computation: 10 steps/s (collection: 143.426s, learning 0.051s)
             Mean action noise std: 1.02
          Mean value\_function loss: 925.5759
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 2.8784
                       Mean reward: -388.21
               Mean episode length: 159.75
   Episode\_Reward/progress\_to\_goal: -0.0010
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2322
          Episode\_Reward/collision: -14.5191
        Episode\_Reward/action\_rate: -0.0042
        Episode\_Reward/stand\_still: -0.0909
Metrics/target\_pose/position\_error: 5.2687
Metrics/target\_pose/orientation\_error: 1.7020
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 12288
                    Iteration time: 143.48s
                      Time elapsed: 00:18:17
                               ETA: 18:00:24
################################################################################
                       Learning iteration 8/3000
                       Computation: 10 steps/s (collection: 143.476s, learning 0.051s)
             Mean action noise std: 1.02
          Mean value\_function loss: 967.1556
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 2.8789
                       Mean reward: -393.20
               Mean episode length: 161.62
   Episode\_Reward/progress\_to\_goal: 0.0064
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1295
          Episode\_Reward/collision: -14.7318
        Episode\_Reward/action\_rate: -0.0295
        Episode\_Reward/stand\_still: -0.0950
Metrics/target\_pose/position\_error: 5.6714
Metrics/target\_pose/orientation\_error: 0.4065
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 13824
                    Iteration time: 143.53s
                      Time elapsed: 00:20:40
                               ETA: 18:33:35
################################################################################
                       Learning iteration 9/3000
                       Computation: 10 steps/s (collection: 144.153s, learning 0.051s)
             Mean action noise std: 1.02
          Mean value\_function loss: 864.8390
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 2.8810
                       Mean reward: -397.82
               Mean episode length: 163.76
   Episode\_Reward/progress\_to\_goal: 0.0060
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2376
          Episode\_Reward/collision: -14.7691
        Episode\_Reward/action\_rate: -0.0076
        Episode\_Reward/stand\_still: -0.0968
Metrics/target\_pose/position\_error: 5.1608
Metrics/target\_pose/orientation\_error: 1.3289
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 15360
                    Iteration time: 144.20s
                      Time elapsed: 00:23:04
                               ETA: 19:03:00
################################################################################
                       Learning iteration 10/3000
                       Computation: 10 steps/s (collection: 144.540s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value\_function loss: 907.5392
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 2.8818
                       Mean reward: -399.98
               Mean episode length: 164.82
   Episode\_Reward/progress\_to\_goal: 0.0126
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2643
          Episode\_Reward/collision: -14.5156
        Episode\_Reward/action\_rate: -0.0028
        Episode\_Reward/stand\_still: -0.0914
Metrics/target\_pose/position\_error: 3.3409
Metrics/target\_pose/orientation\_error: 1.5470
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 16896
                    Iteration time: 144.60s
                      Time elapsed: 00:25:29
                               ETA: 19:28:26
################################################################################
                       Learning iteration 11/3000
                       Computation: 10 steps/s (collection: 144.752s, learning 0.049s)
             Mean action noise std: 1.02
          Mean value\_function loss: 863.4206
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 2.8823
                       Mean reward: -410.66
               Mean episode length: 169.36
   Episode\_Reward/progress\_to\_goal: 0.0047
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1770
          Episode\_Reward/collision: -14.5017
        Episode\_Reward/action\_rate: -0.0048
        Episode\_Reward/stand\_still: -0.0918
Metrics/target\_pose/position\_error: 4.3010
Metrics/target\_pose/orientation\_error: 2.0107
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 18432
                    Iteration time: 144.80s
                      Time elapsed: 00:27:54
                               ETA: 19:50:03
################################################################################
                       Learning iteration 12/3000
                       Computation: 10 steps/s (collection: 145.144s, learning 0.049s)
             Mean action noise std: 1.02
          Mean value\_function loss: 900.3960
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 2.8825
                       Mean reward: -431.60
               Mean episode length: 178.20
   Episode\_Reward/progress\_to\_goal: 0.0061
         Episode\_Reward/reach\_goal: 0.0260
          Episode\_Reward/face\_goal: 0.1509
          Episode\_Reward/collision: -13.5295
        Episode\_Reward/action\_rate: -0.0041
        Episode\_Reward/stand\_still: -0.0843
Metrics/target\_pose/position\_error: 4.8653
Metrics/target\_pose/orientation\_error: 1.4127
      Episode\_Termination/time\_out: 0.9382
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0618
                   Total timesteps: 19968
                    Iteration time: 145.19s
                      Time elapsed: 00:30:19
                               ETA: 20:09:29
################################################################################
                       Learning iteration 13/3000
                       Computation: 10 steps/s (collection: 145.109s, learning 0.050s)
             Mean action noise std: 1.02
          Mean value\_function loss: 876.9671
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 2.8825
                       Mean reward: -432.74
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0072
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1735
          Episode\_Reward/collision: -14.6693
        Episode\_Reward/action\_rate: -0.0346
        Episode\_Reward/stand\_still: -0.0948
Metrics/target\_pose/position\_error: 4.7367
Metrics/target\_pose/orientation\_error: 1.4102
      Episode\_Termination/time\_out: 0.9447
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0553
                   Total timesteps: 21504
                    Iteration time: 145.16s
                      Time elapsed: 00:32:44
                               ETA: 20:25:41
################################################################################
                       Learning iteration 14/3000
                       Computation: 10 steps/s (collection: 145.585s, learning 0.050s)
             Mean action noise std: 1.02
          Mean value\_function loss: 823.7138
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 2.8826
                       Mean reward: -432.56
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0207
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2280
          Episode\_Reward/collision: -13.8168
        Episode\_Reward/action\_rate: -0.0079
        Episode\_Reward/stand\_still: -0.0798
Metrics/target\_pose/position\_error: 3.7140
Metrics/target\_pose/orientation\_error: 0.6846
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 23040
                    Iteration time: 145.63s
                      Time elapsed: 00:35:10
                               ETA: 20:40:58
################################################################################
                       Learning iteration 15/3000
                       Computation: 10 steps/s (collection: 146.022s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value\_function loss: 828.8483
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 2.8847
                       Mean reward: -431.15
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0077
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2925
          Episode\_Reward/collision: -14.5417
        Episode\_Reward/action\_rate: -0.0047
        Episode\_Reward/stand\_still: -0.0933
Metrics/target\_pose/position\_error: 4.5217
Metrics/target\_pose/orientation\_error: 1.8271
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 24576
                    Iteration time: 146.08s
                      Time elapsed: 00:37:36
                               ETA: 20:55:25
################################################################################
                       Learning iteration 16/3000
                       Computation: 10 steps/s (collection: 146.184s, learning 0.050s)
             Mean action noise std: 1.02
          Mean value\_function loss: 737.7673
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 2.8833
                       Mean reward: -429.88
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0201
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2919
          Episode\_Reward/collision: -13.8333
        Episode\_Reward/action\_rate: -0.0044
        Episode\_Reward/stand\_still: -0.0833
Metrics/target\_pose/position\_error: 4.6119
Metrics/target\_pose/orientation\_error: 1.9698
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 26112
                    Iteration time: 146.23s
                      Time elapsed: 00:40:02
                               ETA: 21:08:20
################################################################################
                       Learning iteration 17/3000
                       Computation: 10 steps/s (collection: 146.373s, learning 0.055s)
             Mean action noise std: 1.02
          Mean value\_function loss: 779.6383
               Mean surrogate loss: 0.0241
                 Mean entropy loss: 2.8768
                       Mean reward: -428.84
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0157
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2438
          Episode\_Reward/collision: -14.1111
        Episode\_Reward/action\_rate: -0.0025
        Episode\_Reward/stand\_still: -0.0877
Metrics/target\_pose/position\_error: 3.5856
Metrics/target\_pose/orientation\_error: 2.1685
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 27648
                    Iteration time: 146.43s
                      Time elapsed: 00:42:28
                               ETA: 21:20:05
################################################################################
                       Learning iteration 18/3000
                       Computation: 10 steps/s (collection: 146.762s, learning 0.052s)
             Mean action noise std: 1.02
          Mean value\_function loss: 773.7967
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 2.8764
                       Mean reward: -429.22
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0193
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1866
          Episode\_Reward/collision: -13.8889
        Episode\_Reward/action\_rate: -0.0048
        Episode\_Reward/stand\_still: -0.0850
Metrics/target\_pose/position\_error: 3.3204
Metrics/target\_pose/orientation\_error: 2.1676
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 29184
                    Iteration time: 146.81s
                      Time elapsed: 00:44:55
                               ETA: 21:31:21
################################################################################
                       Learning iteration 19/3000
                       Computation: 10 steps/s (collection: 146.725s, learning 0.058s)
             Mean action noise std: 1.02
          Mean value\_function loss: 768.8155
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 2.8768
                       Mean reward: -428.38
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0096
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2157
          Episode\_Reward/collision: -14.5781
        Episode\_Reward/action\_rate: -0.0069
        Episode\_Reward/stand\_still: -0.0920
Metrics/target\_pose/position\_error: 5.0403
Metrics/target\_pose/orientation\_error: 1.6380
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 30720
                    Iteration time: 146.78s
                      Time elapsed: 00:47:22
                               ETA: 21:41:11
################################################################################
                       Learning iteration 20/3000
                       Computation: 10 steps/s (collection: 147.311s, learning 0.052s)
             Mean action noise std: 1.02
          Mean value\_function loss: 727.6858
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 2.8757
                       Mean reward: -428.18
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0077
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2249
          Episode\_Reward/collision: -14.5295
        Episode\_Reward/action\_rate: -0.0137
        Episode\_Reward/stand\_still: -0.0908
Metrics/target\_pose/position\_error: 5.0574
Metrics/target\_pose/orientation\_error: 1.8882
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 32256
                    Iteration time: 147.36s
                      Time elapsed: 00:49:49
                               ETA: 21:51:12
################################################################################
                       Learning iteration 21/3000
                       Computation: 10 steps/s (collection: 147.359s, learning 0.057s)
             Mean action noise std: 1.02
          Mean value\_function loss: 723.0786
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 2.8781
                       Mean reward: -427.62
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0087
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2257
          Episode\_Reward/collision: -14.2552
        Episode\_Reward/action\_rate: -0.0052
        Episode\_Reward/stand\_still: -0.0835
Metrics/target\_pose/position\_error: 4.7872
Metrics/target\_pose/orientation\_error: 1.9607
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 33792
                    Iteration time: 147.42s
                      Time elapsed: 00:52:17
                               ETA: 22:00:12
################################################################################
                       Learning iteration 22/3000
                       Computation: 10 steps/s (collection: 148.102s, learning 0.053s)
             Mean action noise std: 1.02
          Mean value\_function loss: 680.2210
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 2.8748
                       Mean reward: -427.56
               Mean episode length: 178.83
   Episode\_Reward/progress\_to\_goal: 0.0022
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2661
          Episode\_Reward/collision: -14.3872
        Episode\_Reward/action\_rate: -0.0349
        Episode\_Reward/stand\_still: -0.0894
Metrics/target\_pose/position\_error: 4.8421
Metrics/target\_pose/orientation\_error: 2.2709
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 35328
                    Iteration time: 148.16s
                      Time elapsed: 00:54:45
                               ETA: 22:09:49
################################################################################
                       Learning iteration 23/3000
                       Computation: 10 steps/s (collection: 147.994s, learning 0.057s)
             Mean action noise std: 1.02
          Mean value\_function loss: 691.9523
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 2.8770
                       Mean reward: -428.69
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0112
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1660
          Episode\_Reward/collision: -13.7717
        Episode\_Reward/action\_rate: -0.0033
        Episode\_Reward/stand\_still: -0.0825
Metrics/target\_pose/position\_error: 5.1929
Metrics/target\_pose/orientation\_error: 2.7423
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 36864
                    Iteration time: 148.05s
                      Time elapsed: 00:57:13
                               ETA: 22:18:12
################################################################################
                       Learning iteration 24/3000
                       Computation: 10 steps/s (collection: 148.085s, learning 0.051s)
             Mean action noise std: 1.04
          Mean value\_function loss: 683.8382
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 2.8890
                       Mean reward: -428.26
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0064
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2381
          Episode\_Reward/collision: -14.3568
        Episode\_Reward/action\_rate: -0.0035
        Episode\_Reward/stand\_still: -0.0904
Metrics/target\_pose/position\_error: 5.3085
Metrics/target\_pose/orientation\_error: 0.8170
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 38400
                    Iteration time: 148.14s
                      Time elapsed: 00:59:41
                               ETA: 22:25:53
################################################################################
                       Learning iteration 25/3000
                       Computation: 10 steps/s (collection: 148.257s, learning 0.049s)
             Mean action noise std: 1.05
          Mean value\_function loss: 668.9804
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 2.9226
                       Mean reward: -427.74
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0039
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2702
          Episode\_Reward/collision: -14.6623
        Episode\_Reward/action\_rate: -0.0020
        Episode\_Reward/stand\_still: -0.0942
Metrics/target\_pose/position\_error: 4.8001
Metrics/target\_pose/orientation\_error: 1.4178
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 39936
                    Iteration time: 148.31s
                      Time elapsed: 01:02:09
                               ETA: 22:33:06
################################################################################
                       Learning iteration 26/3000
                       Computation: 10 steps/s (collection: 148.409s, learning 0.055s)
             Mean action noise std: 1.05
          Mean value\_function loss: 682.5305
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 2.9299
                       Mean reward: -427.21
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0028
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2390
          Episode\_Reward/collision: -14.1632
        Episode\_Reward/action\_rate: -0.0022
        Episode\_Reward/stand\_still: -0.0900
Metrics/target\_pose/position\_error: 5.1312
Metrics/target\_pose/orientation\_error: 2.0415
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 41472
                    Iteration time: 148.46s
                      Time elapsed: 01:04:38
                               ETA: 22:39:55
################################################################################
                       Learning iteration 27/3000
                       Computation: 10 steps/s (collection: 148.774s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 681.5113
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 2.9311
                       Mean reward: -427.97
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0216
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1326
          Episode\_Reward/collision: -14.1076
        Episode\_Reward/action\_rate: -0.0025
        Episode\_Reward/stand\_still: -0.0844
Metrics/target\_pose/position\_error: 4.5191
Metrics/target\_pose/orientation\_error: 1.7027
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 43008
                    Iteration time: 148.82s
                      Time elapsed: 01:07:07
                               ETA: 22:46:41
################################################################################
                       Learning iteration 28/3000
                       Computation: 10 steps/s (collection: 148.787s, learning 0.053s)
             Mean action noise std: 1.05
          Mean value\_function loss: 623.6489
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.9326
                       Mean reward: -428.68
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0057
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2043
          Episode\_Reward/collision: -14.6753
        Episode\_Reward/action\_rate: -0.0033
        Episode\_Reward/stand\_still: -0.0934
Metrics/target\_pose/position\_error: 5.6844
Metrics/target\_pose/orientation\_error: 1.7362
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 44544
                    Iteration time: 148.84s
                      Time elapsed: 01:09:36
                               ETA: 22:52:51
################################################################################
                       Learning iteration 29/3000
                       Computation: 10 steps/s (collection: 148.864s, learning 0.048s)
             Mean action noise std: 1.05
          Mean value\_function loss: 624.8702
               Mean surrogate loss: 0.0224
                 Mean entropy loss: 2.9393
                       Mean reward: -428.56
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0037
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2204
          Episode\_Reward/collision: -13.6128
        Episode\_Reward/action\_rate: -0.0027
        Episode\_Reward/stand\_still: -0.0787
Metrics/target\_pose/position\_error: 6.2473
Metrics/target\_pose/orientation\_error: 1.2259
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 46080
                    Iteration time: 148.91s
                      Time elapsed: 01:12:04
                               ETA: 22:58:33
################################################################################
                       Learning iteration 30/3000
                       Computation: 10 steps/s (collection: 148.911s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 627.3233
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 2.9411
                       Mean reward: -428.09
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0121
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2288
          Episode\_Reward/collision: -14.2040
        Episode\_Reward/action\_rate: -0.0026
        Episode\_Reward/stand\_still: -0.0862
Metrics/target\_pose/position\_error: 4.1795
Metrics/target\_pose/orientation\_error: 1.9305
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 47616
                    Iteration time: 148.96s
                      Time elapsed: 01:14:33
                               ETA: 23:03:49
################################################################################
                       Learning iteration 31/3000
                       Computation: 10 steps/s (collection: 149.102s, learning 0.056s)
             Mean action noise std: 1.05
          Mean value\_function loss: 698.2553
               Mean surrogate loss: 0.0313
                 Mean entropy loss: 2.9425
                       Mean reward: -428.21
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0239
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1456
          Episode\_Reward/collision: -14.2300
        Episode\_Reward/action\_rate: -0.0027
        Episode\_Reward/stand\_still: -0.0850
Metrics/target\_pose/position\_error: 4.5832
Metrics/target\_pose/orientation\_error: 2.0959
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 49152
                    Iteration time: 149.16s
                      Time elapsed: 01:17:03
                               ETA: 23:08:53
################################################################################
                       Learning iteration 32/3000
                       Computation: 10 steps/s (collection: 149.380s, learning 0.050s)
             Mean action noise std: 1.05
          Mean value\_function loss: 594.2666
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 2.9426
                       Mean reward: -427.73
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0113
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2436
          Episode\_Reward/collision: -14.4800
        Episode\_Reward/action\_rate: -0.0041
        Episode\_Reward/stand\_still: -0.0915
Metrics/target\_pose/position\_error: 4.4613
Metrics/target\_pose/orientation\_error: 2.1668
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 50688
                    Iteration time: 149.43s
                      Time elapsed: 01:19:32
                               ETA: 23:13:55
################################################################################
                       Learning iteration 33/3000
                       Computation: 10 steps/s (collection: 149.110s, learning 0.052s)
             Mean action noise std: 1.05
          Mean value\_function loss: 573.9823
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 2.9428
                       Mean reward: -428.25
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0173
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2306
          Episode\_Reward/collision: -14.4635
        Episode\_Reward/action\_rate: -0.0026
        Episode\_Reward/stand\_still: -0.0898
Metrics/target\_pose/position\_error: 4.3617
Metrics/target\_pose/orientation\_error: 1.7055
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 52224
                    Iteration time: 149.16s
                      Time elapsed: 01:22:01
                               ETA: 23:18:06
################################################################################
                       Learning iteration 34/3000
                       Computation: 10 steps/s (collection: 149.229s, learning 0.055s)
             Mean action noise std: 1.05
          Mean value\_function loss: 536.8149
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 2.9433
                       Mean reward: -428.42
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0000
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1276
          Episode\_Reward/collision: -14.5243
        Episode\_Reward/action\_rate: -0.0028
        Episode\_Reward/stand\_still: -0.0925
Metrics/target\_pose/position\_error: 4.2409
Metrics/target\_pose/orientation\_error: 1.7561
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 53760
                    Iteration time: 149.28s
                      Time elapsed: 01:24:30
                               ETA: 23:22:05
################################################################################
                       Learning iteration 35/3000
                       Computation: 10 steps/s (collection: 149.169s, learning 0.048s)
             Mean action noise std: 1.05
          Mean value\_function loss: 508.3076
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 2.9429
                       Mean reward: -427.98
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0137
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1679
          Episode\_Reward/collision: -13.8819
        Episode\_Reward/action\_rate: -0.0033
        Episode\_Reward/stand\_still: -0.0850
Metrics/target\_pose/position\_error: 5.0554
Metrics/target\_pose/orientation\_error: 1.7242
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 55296
                    Iteration time: 149.22s
                      Time elapsed: 01:27:00
                               ETA: 23:25:37
################################################################################
                       Learning iteration 36/3000
                       Computation: 10 steps/s (collection: 149.236s, learning 0.056s)
             Mean action noise std: 1.05
          Mean value\_function loss: 534.6744
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 2.9433
                       Mean reward: -428.28
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0123
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2879
          Episode\_Reward/collision: -14.5286
        Episode\_Reward/action\_rate: -0.0023
        Episode\_Reward/stand\_still: -0.0934
Metrics/target\_pose/position\_error: 3.5997
Metrics/target\_pose/orientation\_error: 1.5575
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 56832
                    Iteration time: 149.29s
                      Time elapsed: 01:29:29
                               ETA: 23:28:56
################################################################################
                       Learning iteration 37/3000
                       Computation: 10 steps/s (collection: 149.471s, learning 0.059s)
             Mean action noise std: 1.05
          Mean value\_function loss: 466.3020
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 2.9436
                       Mean reward: -427.18
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0083
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2008
          Episode\_Reward/collision: -13.7431
        Episode\_Reward/action\_rate: -0.0052
        Episode\_Reward/stand\_still: -0.0812
Metrics/target\_pose/position\_error: 4.8236
Metrics/target\_pose/orientation\_error: 2.0564
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 58368
                    Iteration time: 149.53s
                      Time elapsed: 01:31:58
                               ETA: 23:32:15
################################################################################
                       Learning iteration 38/3000
                       Computation: 10 steps/s (collection: 149.443s, learning 0.057s)
             Mean action noise std: 1.06
          Mean value\_function loss: 508.1756
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 2.9448
                       Mean reward: -427.39
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0259
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2657
          Episode\_Reward/collision: -14.2196
        Episode\_Reward/action\_rate: -0.0024
        Episode\_Reward/stand\_still: -0.0865
Metrics/target\_pose/position\_error: 6.0493
Metrics/target\_pose/orientation\_error: 1.6951
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 59904
                    Iteration time: 149.50s
                      Time elapsed: 01:34:28
                               ETA: 23:35:13
################################################################################
                       Learning iteration 39/3000
                       Computation: 10 steps/s (collection: 149.315s, learning 0.047s)
             Mean action noise std: 1.06
          Mean value\_function loss: 479.6236
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 2.9472
                       Mean reward: -425.91
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0184
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2143
          Episode\_Reward/collision: -14.0512
        Episode\_Reward/action\_rate: -0.0023
        Episode\_Reward/stand\_still: -0.0835
Metrics/target\_pose/position\_error: 3.7011
Metrics/target\_pose/orientation\_error: 1.6908
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 61440
                    Iteration time: 149.36s
                      Time elapsed: 01:36:57
                               ETA: 23:37:45
################################################################################
                       Learning iteration 40/3000
                       Computation: 10 steps/s (collection: 149.639s, learning 0.061s)
             Mean action noise std: 1.06
          Mean value\_function loss: 435.8373
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 2.9503
                       Mean reward: -426.28
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0164
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.3093
          Episode\_Reward/collision: -14.7760
        Episode\_Reward/action\_rate: -0.0025
        Episode\_Reward/stand\_still: -0.0953
Metrics/target\_pose/position\_error: 4.2489
Metrics/target\_pose/orientation\_error: 1.1731
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 62976
                    Iteration time: 149.70s
                      Time elapsed: 01:39:27
                               ETA: 23:40:27
################################################################################
                       Learning iteration 41/3000
                       Computation: 10 steps/s (collection: 149.575s, learning 0.053s)
             Mean action noise std: 1.06
          Mean value\_function loss: 403.3598
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 2.9512
                       Mean reward: -425.91
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0161
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2697
          Episode\_Reward/collision: -14.1016
        Episode\_Reward/action\_rate: -0.0026
        Episode\_Reward/stand\_still: -0.0864
Metrics/target\_pose/position\_error: 5.3610
Metrics/target\_pose/orientation\_error: 1.9963
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 64512
                    Iteration time: 149.63s
                      Time elapsed: 01:41:57
                               ETA: 23:42:49
################################################################################
                       Learning iteration 42/3000
                       Computation: 10 steps/s (collection: 149.796s, learning 0.058s)
             Mean action noise std: 1.06
          Mean value\_function loss: 411.7082
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 2.9522
                       Mean reward: -424.23
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0050
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2293
          Episode\_Reward/collision: -13.4592
        Episode\_Reward/action\_rate: -0.0025
        Episode\_Reward/stand\_still: -0.0830
Metrics/target\_pose/position\_error: 6.4083
Metrics/target\_pose/orientation\_error: 1.0899
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 66048
                    Iteration time: 149.85s
                      Time elapsed: 01:44:27
                               ETA: 23:45:12
################################################################################
                       Learning iteration 43/3000
                       Computation: 10 steps/s (collection: 150.048s, learning 0.054s)
             Mean action noise std: 1.06
          Mean value\_function loss: 400.1485
               Mean surrogate loss: 0.0220
                 Mean entropy loss: 2.9513
                       Mean reward: -420.61
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0039
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2516
          Episode\_Reward/collision: -13.3012
        Episode\_Reward/action\_rate: -0.0039
        Episode\_Reward/stand\_still: -0.0790
Metrics/target\_pose/position\_error: 5.2374
Metrics/target\_pose/orientation\_error: 1.3550
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 67584
                    Iteration time: 150.10s
                      Time elapsed: 01:46:57
                               ETA: 23:47:39
################################################################################
                       Learning iteration 44/3000
                       Computation: 10 steps/s (collection: 150.169s, learning 0.058s)
             Mean action noise std: 1.06
          Mean value\_function loss: 345.2754
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 2.9509
                       Mean reward: -418.95
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0047
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2281
          Episode\_Reward/collision: -13.3030
        Episode\_Reward/action\_rate: -0.0027
        Episode\_Reward/stand\_still: -0.0768
Metrics/target\_pose/position\_error: 4.2067
Metrics/target\_pose/orientation\_error: 1.4318
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 69120
                    Iteration time: 150.23s
                      Time elapsed: 01:49:27
                               ETA: 23:50:02
################################################################################
                       Learning iteration 45/3000
                       Computation: 10 steps/s (collection: 150.295s, learning 0.058s)
             Mean action noise std: 1.06
          Mean value\_function loss: 333.3367
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 2.9502
                       Mean reward: -416.45
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0000
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2263
          Episode\_Reward/collision: -13.3047
        Episode\_Reward/action\_rate: -0.0057
        Episode\_Reward/stand\_still: -0.0782
Metrics/target\_pose/position\_error: 3.2359
Metrics/target\_pose/orientation\_error: 1.6517
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 70656
                    Iteration time: 150.35s
                      Time elapsed: 01:51:57
                               ETA: 23:52:19
################################################################################
                       Learning iteration 46/3000
                       Computation: 10 steps/s (collection: 150.919s, learning 0.052s)
             Mean action noise std: 1.06
          Mean value\_function loss: 255.3518
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 2.9487
                       Mean reward: -414.19
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0483
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2263
          Episode\_Reward/collision: -12.8698
        Episode\_Reward/action\_rate: -0.0026
        Episode\_Reward/stand\_still: -0.0706
Metrics/target\_pose/position\_error: 3.0029
Metrics/target\_pose/orientation\_error: 1.3512
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 72192
                    Iteration time: 150.97s
                      Time elapsed: 01:54:28
                               ETA: 23:55:03
################################################################################
                       Learning iteration 47/3000
                       Computation: 10 steps/s (collection: 151.033s, learning 0.055s)
             Mean action noise std: 1.05
          Mean value\_function loss: 352.7791
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 2.9457
                       Mean reward: -409.36
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0140
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2146
          Episode\_Reward/collision: -11.4123
        Episode\_Reward/action\_rate: -0.0089
        Episode\_Reward/stand\_still: -0.0517
Metrics/target\_pose/position\_error: 5.2840
Metrics/target\_pose/orientation\_error: 1.9835
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 73728
                    Iteration time: 151.09s
                      Time elapsed: 01:56:59
                               ETA: 23:57:41
################################################################################
                       Learning iteration 48/3000
                       Computation: 10 steps/s (collection: 151.388s, learning 0.055s)
             Mean action noise std: 1.05
          Mean value\_function loss: 277.5294
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 2.9399
                       Mean reward: -406.71
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0037
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2065
          Episode\_Reward/collision: -12.8142
        Episode\_Reward/action\_rate: -0.0098
        Episode\_Reward/stand\_still: -0.0738
Metrics/target\_pose/position\_error: 5.2710
Metrics/target\_pose/orientation\_error: 1.0598
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 75264
                    Iteration time: 151.44s
                      Time elapsed: 01:59:31
                               ETA: 00:00:28
################################################################################
                       Learning iteration 49/3000
                       Computation: 10 steps/s (collection: 151.381s, learning 0.056s)
             Mean action noise std: 1.05
          Mean value\_function loss: 307.8216
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 2.9393
                       Mean reward: -403.87
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0264
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2189
          Episode\_Reward/collision: -11.7231
        Episode\_Reward/action\_rate: -0.0026
        Episode\_Reward/stand\_still: -0.0581
Metrics/target\_pose/position\_error: 3.9005
Metrics/target\_pose/orientation\_error: 1.4946
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 76800
                    Iteration time: 151.44s
                      Time elapsed: 02:02:02
                               ETA: 00:03:02
################################################################################
                       Learning iteration 50/3000
                       Computation: 10 steps/s (collection: 151.718s, learning 0.058s)
             Mean action noise std: 1.05
          Mean value\_function loss: 259.7478
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 2.9400
                       Mean reward: -401.87
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0096
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2310
          Episode\_Reward/collision: -12.7613
        Episode\_Reward/action\_rate: -0.0290
        Episode\_Reward/stand\_still: -0.0735
Metrics/target\_pose/position\_error: 4.4237
Metrics/target\_pose/orientation\_error: 1.3872
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 78336
                    Iteration time: 151.78s
                      Time elapsed: 02:04:34
                               ETA: 00:05:43
################################################################################
                       Learning iteration 51/3000
                       Computation: 10 steps/s (collection: 151.568s, learning 0.061s)
             Mean action noise std: 1.05
          Mean value\_function loss: 301.1330
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 2.9394
                       Mean reward: -401.10
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0271
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1963
          Episode\_Reward/collision: -13.0208
        Episode\_Reward/action\_rate: -0.0031
        Episode\_Reward/stand\_still: -0.0753
Metrics/target\_pose/position\_error: 4.3854
Metrics/target\_pose/orientation\_error: 1.0920
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 79872
                    Iteration time: 151.63s
                      Time elapsed: 02:07:06
                               ETA: 00:08:05
################################################################################
                       Learning iteration 52/3000
                       Computation: 10 steps/s (collection: 152.192s, learning 0.053s)
             Mean action noise std: 1.05
          Mean value\_function loss: 354.9083
               Mean surrogate loss: 0.0322
                 Mean entropy loss: 2.9381
                       Mean reward: -398.35
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0165
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2545
          Episode\_Reward/collision: -13.5312
        Episode\_Reward/action\_rate: -0.0134
        Episode\_Reward/stand\_still: -0.0815
Metrics/target\_pose/position\_error: 5.2563
Metrics/target\_pose/orientation\_error: 2.1277
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 81408
                    Iteration time: 152.25s
                      Time elapsed: 02:09:38
                               ETA: 00:10:49
################################################################################
                       Learning iteration 53/3000
                       Computation: 10 steps/s (collection: 152.104s, learning 0.062s)
             Mean action noise std: 1.05
          Mean value\_function loss: 258.3718
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 2.9381
                       Mean reward: -396.92
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0109
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2011
          Episode\_Reward/collision: -12.6988
        Episode\_Reward/action\_rate: -0.0203
        Episode\_Reward/stand\_still: -0.0759
Metrics/target\_pose/position\_error: 4.3087
Metrics/target\_pose/orientation\_error: 0.9044
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 82944
                    Iteration time: 152.17s
                      Time elapsed: 02:12:10
                               ETA: 00:13:17
################################################################################
                       Learning iteration 54/3000
                       Computation: 10 steps/s (collection: 152.486s, learning 0.055s)
             Mean action noise std: 1.05
          Mean value\_function loss: 196.1247
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 2.9381
                       Mean reward: -395.63
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0046
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2491
          Episode\_Reward/collision: -13.1823
        Episode\_Reward/action\_rate: -0.0036
        Episode\_Reward/stand\_still: -0.0791
Metrics/target\_pose/position\_error: 4.5411
Metrics/target\_pose/orientation\_error: 1.4962
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 84480
                    Iteration time: 152.54s
                      Time elapsed: 02:14:43
                               ETA: 00:15:54
################################################################################
                       Learning iteration 55/3000
                       Computation: 10 steps/s (collection: 152.702s, learning 0.055s)
             Mean action noise std: 1.05
          Mean value\_function loss: 217.3367
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 2.9381
                       Mean reward: -395.24
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0199
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1783
          Episode\_Reward/collision: -12.8212
        Episode\_Reward/action\_rate: -0.5062
        Episode\_Reward/stand\_still: -0.0744
Metrics/target\_pose/position\_error: 4.6182
Metrics/target\_pose/orientation\_error: 1.6679
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 86016
                    Iteration time: 152.76s
                      Time elapsed: 02:17:15
                               ETA: 00:18:32
################################################################################
                       Learning iteration 56/3000
                       Computation: 10 steps/s (collection: 153.030s, learning 0.052s)
             Mean action noise std: 1.05
          Mean value\_function loss: 182.2373
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 2.9381
                       Mean reward: -391.63
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0076
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1802
          Episode\_Reward/collision: -11.5816
        Episode\_Reward/action\_rate: -0.0327
        Episode\_Reward/stand\_still: -0.0633
Metrics/target\_pose/position\_error: 4.7355
Metrics/target\_pose/orientation\_error: 1.7556
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 87552
                    Iteration time: 153.08s
                      Time elapsed: 02:19:48
                               ETA: 00:21:16
################################################################################
                       Learning iteration 57/3000
                       Computation: 10 steps/s (collection: 153.445s, learning 0.057s)
             Mean action noise std: 1.05
          Mean value\_function loss: 212.4310
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 2.9382
                       Mean reward: -388.13
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0182
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2446
          Episode\_Reward/collision: -12.9757
        Episode\_Reward/action\_rate: -0.0115
        Episode\_Reward/stand\_still: -0.0750
Metrics/target\_pose/position\_error: 5.2648
Metrics/target\_pose/orientation\_error: 1.3202
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 89088
                    Iteration time: 153.50s
                      Time elapsed: 02:22:22
                               ETA: 00:24:10
################################################################################
                       Learning iteration 58/3000
                       Computation: 9 steps/s (collection: 153.701s, learning 0.054s)
             Mean action noise std: 1.05
          Mean value\_function loss: 204.8176
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 2.9388
                       Mean reward: -387.82
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0051
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2180
          Episode\_Reward/collision: -11.5434
        Episode\_Reward/action\_rate: -0.0187
        Episode\_Reward/stand\_still: -0.0573
Metrics/target\_pose/position\_error: 4.4319
Metrics/target\_pose/orientation\_error: 1.8635
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 90624
                    Iteration time: 153.75s
                      Time elapsed: 02:24:56
                               ETA: 00:27:05
################################################################################
                       Learning iteration 59/3000
                       Computation: 9 steps/s (collection: 153.822s, learning 0.055s)
             Mean action noise std: 1.05
          Mean value\_function loss: 244.5350
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 2.9391
                       Mean reward: -385.13
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0237
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2569
          Episode\_Reward/collision: -11.0252
        Episode\_Reward/action\_rate: -0.0038
        Episode\_Reward/stand\_still: -0.0528
Metrics/target\_pose/position\_error: 4.6920
Metrics/target\_pose/orientation\_error: 0.9701
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 92160
                    Iteration time: 153.88s
                      Time elapsed: 02:27:29
                               ETA: 00:29:56
################################################################################
                       Learning iteration 60/3000
                       Computation: 9 steps/s (collection: 154.494s, learning 0.057s)
             Mean action noise std: 1.05
          Mean value\_function loss: 174.9723
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 2.9395
                       Mean reward: -382.34
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0166
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2256
          Episode\_Reward/collision: -11.5634
        Episode\_Reward/action\_rate: -0.1555
        Episode\_Reward/stand\_still: -0.0567
Metrics/target\_pose/position\_error: 5.1241
Metrics/target\_pose/orientation\_error: 1.7175
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 93696
                    Iteration time: 154.55s
                      Time elapsed: 02:30:04
                               ETA: 00:33:08
################################################################################
                       Learning iteration 61/3000
                       Computation: 9 steps/s (collection: 154.294s, learning 0.049s)
             Mean action noise std: 1.05
          Mean value\_function loss: 181.7862
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 2.9392
                       Mean reward: -377.97
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0297
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2203
          Episode\_Reward/collision: -9.7370
        Episode\_Reward/action\_rate: -0.3199
        Episode\_Reward/stand\_still: -0.0389
Metrics/target\_pose/position\_error: 3.7805
Metrics/target\_pose/orientation\_error: 1.6748
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 95232
                    Iteration time: 154.34s
                      Time elapsed: 02:32:38
                               ETA: 00:35:59
################################################################################
                       Learning iteration 62/3000
                       Computation: 9 steps/s (collection: 155.119s, learning 0.050s)
             Mean action noise std: 1.05
          Mean value\_function loss: 151.8638
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 2.9383
                       Mean reward: -375.23
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0044
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2430
          Episode\_Reward/collision: -12.0122
        Episode\_Reward/action\_rate: -0.0161
        Episode\_Reward/stand\_still: -0.0610
Metrics/target\_pose/position\_error: 4.9410
Metrics/target\_pose/orientation\_error: 2.1328
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 96768
                    Iteration time: 155.17s
                      Time elapsed: 02:35:14
                               ETA: 00:39:19
################################################################################
                       Learning iteration 63/3000
                       Computation: 9 steps/s (collection: 155.629s, learning 0.052s)
             Mean action noise std: 1.05
          Mean value\_function loss: 180.7289
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 2.9384
                       Mean reward: -365.78
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0060
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2072
          Episode\_Reward/collision: -10.4557
        Episode\_Reward/action\_rate: -0.0052
        Episode\_Reward/stand\_still: -0.0507
Metrics/target\_pose/position\_error: 3.7417
Metrics/target\_pose/orientation\_error: 1.9715
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 98304
                    Iteration time: 155.68s
                      Time elapsed: 02:37:49
                               ETA: 00:42:51
################################################################################
                       Learning iteration 64/3000
                       Computation: 9 steps/s (collection: 155.897s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 147.8885
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 2.9374
                       Mean reward: -360.05
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0271
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1720
          Episode\_Reward/collision: -9.8464
        Episode\_Reward/action\_rate: -0.0025
        Episode\_Reward/stand\_still: -0.0385
Metrics/target\_pose/position\_error: 5.0134
Metrics/target\_pose/orientation\_error: 2.2869
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 99840
                    Iteration time: 155.95s
                      Time elapsed: 02:40:25
                               ETA: 00:46:23
################################################################################
                       Learning iteration 65/3000
                       Computation: 9 steps/s (collection: 156.805s, learning 0.054s)
             Mean action noise std: 1.05
          Mean value\_function loss: 225.5266
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 2.9370
                       Mean reward: -353.85
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0114
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1541
          Episode\_Reward/collision: -9.8984
        Episode\_Reward/action\_rate: -0.0069
        Episode\_Reward/stand\_still: -0.0405
Metrics/target\_pose/position\_error: 5.0741
Metrics/target\_pose/orientation\_error: 1.5483
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 101376
                    Iteration time: 156.86s
                      Time elapsed: 02:43:02
                               ETA: 00:50:25
################################################################################
                       Learning iteration 66/3000
                       Computation: 9 steps/s (collection: 156.955s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 157.6997
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 2.9370
                       Mean reward: -349.52
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0733
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1816
          Episode\_Reward/collision: -11.0668
        Episode\_Reward/action\_rate: -0.0266
        Episode\_Reward/stand\_still: -0.0571
Metrics/target\_pose/position\_error: 6.4065
Metrics/target\_pose/orientation\_error: 0.4981
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 102912
                    Iteration time: 157.01s
                      Time elapsed: 02:45:39
                               ETA: 00:54:22
################################################################################
                       Learning iteration 67/3000
                       Computation: 9 steps/s (collection: 157.617s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 112.0003
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 2.9369
                       Mean reward: -343.55
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0160
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2178
          Episode\_Reward/collision: -10.2856
        Episode\_Reward/action\_rate: -0.1171
        Episode\_Reward/stand\_still: -0.0473
Metrics/target\_pose/position\_error: 5.9470
Metrics/target\_pose/orientation\_error: 1.2113
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 104448
                    Iteration time: 157.67s
                      Time elapsed: 02:48:17
                               ETA: 00:58:35
################################################################################
                       Learning iteration 68/3000
                       Computation: 9 steps/s (collection: 158.000s, learning 0.057s)
             Mean action noise std: 1.05
          Mean value\_function loss: 142.1166
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 2.9368
                       Mean reward: -339.59
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0413
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2064
          Episode\_Reward/collision: -9.3498
        Episode\_Reward/action\_rate: -0.0041
        Episode\_Reward/stand\_still: -0.0289
Metrics/target\_pose/position\_error: 5.0806
Metrics/target\_pose/orientation\_error: 1.9282
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 105984
                    Iteration time: 158.06s
                      Time elapsed: 02:50:55
                               ETA: 01:02:54
################################################################################
                       Learning iteration 69/3000
                       Computation: 9 steps/s (collection: 158.459s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 182.1825
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 2.9365
                       Mean reward: -336.10
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0187
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2328
          Episode\_Reward/collision: -11.1797
        Episode\_Reward/action\_rate: -0.2612
        Episode\_Reward/stand\_still: -0.0536
Metrics/target\_pose/position\_error: 4.7186
Metrics/target\_pose/orientation\_error: 1.7389
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 107520
                    Iteration time: 158.51s
                      Time elapsed: 02:53:33
                               ETA: 01:07:19
################################################################################
                       Learning iteration 70/3000
                       Computation: 9 steps/s (collection: 158.994s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 190.1168
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 2.9358
                       Mean reward: -335.71
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0013
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1394
          Episode\_Reward/collision: -12.2231
        Episode\_Reward/action\_rate: -0.0136
        Episode\_Reward/stand\_still: -0.0671
Metrics/target\_pose/position\_error: 3.9068
Metrics/target\_pose/orientation\_error: 1.4674
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 109056
                    Iteration time: 159.05s
                      Time elapsed: 02:56:12
                               ETA: 01:11:54
################################################################################
                       Learning iteration 71/3000
                       Computation: 9 steps/s (collection: 159.355s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 88.9346
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 2.9354
                       Mean reward: -331.84
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0839
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2294
          Episode\_Reward/collision: -9.4392
        Episode\_Reward/action\_rate: -0.0145
        Episode\_Reward/stand\_still: -0.0322
Metrics/target\_pose/position\_error: 4.7534
Metrics/target\_pose/orientation\_error: 2.0462
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 110592
                    Iteration time: 159.41s
                      Time elapsed: 02:58:52
                               ETA: 01:16:32
################################################################################
                       Learning iteration 72/3000
                       Computation: 9 steps/s (collection: 160.431s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 154.6620
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 2.9348
                       Mean reward: -319.47
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: 0.0611
         Episode\_Reward/reach\_goal: 0.0376
          Episode\_Reward/face\_goal: 0.1960
          Episode\_Reward/collision: -8.9158
        Episode\_Reward/action\_rate: -0.0142
        Episode\_Reward/stand\_still: -0.0300
Metrics/target\_pose/position\_error: 5.7710
Metrics/target\_pose/orientation\_error: 1.2716
      Episode\_Termination/time\_out: 0.9837
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0163
                   Total timesteps: 112128
                    Iteration time: 160.48s
                      Time elapsed: 03:01:32
                               ETA: 01:21:41
################################################################################
                       Learning iteration 73/3000
                       Computation: 9 steps/s (collection: 160.760s, learning 0.050s)
             Mean action noise std: 1.05
          Mean value\_function loss: 151.2468
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.9346
                       Mean reward: -316.89
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: -0.0013
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2688
          Episode\_Reward/collision: -9.1727
        Episode\_Reward/action\_rate: -0.0057
        Episode\_Reward/stand\_still: -0.0372
Metrics/target\_pose/position\_error: 4.0215
Metrics/target\_pose/orientation\_error: 1.3895
      Episode\_Termination/time\_out: 0.9375
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0625
                   Total timesteps: 113664
                    Iteration time: 160.81s
                      Time elapsed: 03:04:13
                               ETA: 01:26:51
################################################################################
                       Learning iteration 74/3000
                       Computation: 9 steps/s (collection: 161.242s, learning 0.049s)
             Mean action noise std: 1.05
          Mean value\_function loss: 137.9113
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 2.9343
                       Mean reward: -315.49
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: 0.0123
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1927
          Episode\_Reward/collision: -8.8498
        Episode\_Reward/action\_rate: -0.0295
        Episode\_Reward/stand\_still: -0.0265
Metrics/target\_pose/position\_error: 5.3756
Metrics/target\_pose/orientation\_error: 1.9143
      Episode\_Termination/time\_out: 0.9616
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0384
                   Total timesteps: 115200
                    Iteration time: 161.29s
                      Time elapsed: 03:06:54
                               ETA: 01:32:06
################################################################################
                       Learning iteration 75/3000
                       Computation: 9 steps/s (collection: 161.542s, learning 0.054s)
             Mean action noise std: 1.05
          Mean value\_function loss: 132.1492
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 2.9341
                       Mean reward: -314.59
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: -0.0218
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1966
          Episode\_Reward/collision: -8.4306
        Episode\_Reward/action\_rate: -0.0585
        Episode\_Reward/stand\_still: -0.0252
Metrics/target\_pose/position\_error: 5.0995
Metrics/target\_pose/orientation\_error: 2.1514
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 116736
                    Iteration time: 161.60s
                      Time elapsed: 03:09:36
                               ETA: 01:37:21
################################################################################
                       Learning iteration 76/3000
                       Computation: 9 steps/s (collection: 162.105s, learning 0.062s)
             Mean action noise std: 1.05
          Mean value\_function loss: 104.6754
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 2.9333
                       Mean reward: -311.65
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: -0.0174
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1932
          Episode\_Reward/collision: -7.9531
        Episode\_Reward/action\_rate: -0.0886
        Episode\_Reward/stand\_still: -0.0237
Metrics/target\_pose/position\_error: 5.0484
Metrics/target\_pose/orientation\_error: 1.2650
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 118272
                    Iteration time: 162.17s
                      Time elapsed: 03:12:18
                               ETA: 01:42:45
################################################################################
                       Learning iteration 77/3000
                       Computation: 9 steps/s (collection: 162.741s, learning 0.053s)
             Mean action noise std: 1.05
          Mean value\_function loss: 105.5253
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 2.9325
                       Mean reward: -304.90
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: 0.0339
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1814
          Episode\_Reward/collision: -8.6979
        Episode\_Reward/action\_rate: -0.0341
        Episode\_Reward/stand\_still: -0.0295
Metrics/target\_pose/position\_error: 4.4810
Metrics/target\_pose/orientation\_error: 1.5022
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 119808
                    Iteration time: 162.79s
                      Time elapsed: 03:15:01
                               ETA: 01:48:20
################################################################################
                       Learning iteration 78/3000
                       Computation: 9 steps/s (collection: 163.337s, learning 0.049s)
             Mean action noise std: 1.05
          Mean value\_function loss: 91.5039
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 2.9323
                       Mean reward: -295.07
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: -0.0189
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2376
          Episode\_Reward/collision: -8.6615
        Episode\_Reward/action\_rate: -0.0377
        Episode\_Reward/stand\_still: -0.0305
Metrics/target\_pose/position\_error: 4.8216
Metrics/target\_pose/orientation\_error: 1.5474
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 121344
                    Iteration time: 163.39s
                      Time elapsed: 03:17:44
                               ETA: 01:54:05
################################################################################
                       Learning iteration 79/3000
                       Computation: 9 steps/s (collection: 163.671s, learning 0.050s)
             Mean action noise std: 1.05
          Mean value\_function loss: 124.2443
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 2.9316
                       Mean reward: -292.94
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: 0.0210
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1921
          Episode\_Reward/collision: -7.7161
        Episode\_Reward/action\_rate: -0.0384
        Episode\_Reward/stand\_still: -0.0217
Metrics/target\_pose/position\_error: 3.2888
Metrics/target\_pose/orientation\_error: 1.2429
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 122880
                    Iteration time: 163.72s
                      Time elapsed: 03:20:28
                               ETA: 01:59:49
################################################################################
                       Learning iteration 80/3000
                       Computation: 9 steps/s (collection: 164.301s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 91.0360
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 2.9313
                       Mean reward: -293.89
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: -0.0258
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2516
          Episode\_Reward/collision: -10.0234
        Episode\_Reward/action\_rate: -0.0048
        Episode\_Reward/stand\_still: -0.0454
Metrics/target\_pose/position\_error: 4.2191
Metrics/target\_pose/orientation\_error: 1.5781
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 124416
                    Iteration time: 164.35s
                      Time elapsed: 03:23:12
                               ETA: 02:05:43
################################################################################
                       Learning iteration 81/3000
                       Computation: 9 steps/s (collection: 164.783s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 71.8051
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 2.9306
                       Mean reward: -289.93
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: 0.0042
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2054
          Episode\_Reward/collision: -10.3420
        Episode\_Reward/action\_rate: -0.0044
        Episode\_Reward/stand\_still: -0.0473
Metrics/target\_pose/position\_error: 4.3881
Metrics/target\_pose/orientation\_error: 1.7838
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 125952
                    Iteration time: 164.83s
                      Time elapsed: 03:25:57
                               ETA: 02:11:42
################################################################################
                       Learning iteration 82/3000
                       Computation: 9 steps/s (collection: 165.431s, learning 0.054s)
             Mean action noise std: 1.05
          Mean value\_function loss: 117.0373
               Mean surrogate loss: 0.0185
                 Mean entropy loss: 2.9294
                       Mean reward: -280.38
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: 0.0332
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1854
          Episode\_Reward/collision: -9.2535
        Episode\_Reward/action\_rate: -0.1140
        Episode\_Reward/stand\_still: -0.0356
Metrics/target\_pose/position\_error: 6.2246
Metrics/target\_pose/orientation\_error: 1.1306
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 127488
                    Iteration time: 165.49s
                      Time elapsed: 03:28:43
                               ETA: 02:17:51
################################################################################
                       Learning iteration 83/3000
                       Computation: 9 steps/s (collection: 165.728s, learning 0.056s)
             Mean action noise std: 1.05
          Mean value\_function loss: 142.5638
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 2.9292
                       Mean reward: -282.27
               Mean episode length: 179.27
   Episode\_Reward/progress\_to\_goal: -0.0647
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2084
          Episode\_Reward/collision: -7.8550
        Episode\_Reward/action\_rate: -0.0032
        Episode\_Reward/stand\_still: -0.0187
Metrics/target\_pose/position\_error: 5.5149
Metrics/target\_pose/orientation\_error: 2.3470
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 129024
                    Iteration time: 165.78s
                      Time elapsed: 03:31:28
                               ETA: 02:23:57
################################################################################
                       Learning iteration 84/3000
                       Computation: 9 steps/s (collection: 166.072s, learning 0.048s)
             Mean action noise std: 1.05
          Mean value\_function loss: 114.2461
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 2.9284
                       Mean reward: -290.33
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0490
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1760
          Episode\_Reward/collision: -10.0981
        Episode\_Reward/action\_rate: -0.0027
        Episode\_Reward/stand\_still: -0.0444
Metrics/target\_pose/position\_error: 5.4849
Metrics/target\_pose/orientation\_error: 1.2995
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 130560
                    Iteration time: 166.12s
                      Time elapsed: 03:34:15
                               ETA: 02:30:03
################################################################################
                       Learning iteration 85/3000
                       Computation: 9 steps/s (collection: 166.678s, learning 0.051s)
             Mean action noise std: 1.05
          Mean value\_function loss: 125.5975
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 2.9270
                       Mean reward: -290.15
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0069
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2387
          Episode\_Reward/collision: -12.9002
        Episode\_Reward/action\_rate: -0.0375
        Episode\_Reward/stand\_still: -0.0724
Metrics/target\_pose/position\_error: 4.5870
Metrics/target\_pose/orientation\_error: 1.8273
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 132096
                    Iteration time: 166.73s
                      Time elapsed: 03:37:01
                               ETA: 02:36:17
################################################################################
                       Learning iteration 86/3000
                       Computation: 9 steps/s (collection: 166.897s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value\_function loss: 96.4187
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 2.9244
                       Mean reward: -291.17
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0359
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2077
          Episode\_Reward/collision: -9.3307
        Episode\_Reward/action\_rate: -0.0293
        Episode\_Reward/stand\_still: -0.0338
Metrics/target\_pose/position\_error: 2.6222
Metrics/target\_pose/orientation\_error: 1.6742
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 133632
                    Iteration time: 166.95s
                      Time elapsed: 03:39:48
                               ETA: 02:42:26
################################################################################
                       Learning iteration 87/3000
                       Computation: 9 steps/s (collection: 167.634s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value\_function loss: 78.8271
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 2.9238
                       Mean reward: -287.49
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0162
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2062
          Episode\_Reward/collision: -8.9288
        Episode\_Reward/action\_rate: -0.0054
        Episode\_Reward/stand\_still: -0.0241
Metrics/target\_pose/position\_error: 4.8412
Metrics/target\_pose/orientation\_error: 1.5377
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 135168
                    Iteration time: 167.69s
                      Time elapsed: 03:42:36
                               ETA: 02:48:47
################################################################################
                       Learning iteration 88/3000
                       Computation: 9 steps/s (collection: 168.018s, learning 0.053s)
             Mean action noise std: 1.04
          Mean value\_function loss: 91.6437
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 2.9234
                       Mean reward: -288.83
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0518
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2140
          Episode\_Reward/collision: -8.6667
        Episode\_Reward/action\_rate: -0.0050
        Episode\_Reward/stand\_still: -0.0259
Metrics/target\_pose/position\_error: 4.7544
Metrics/target\_pose/orientation\_error: 1.7010
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 136704
                    Iteration time: 168.07s
                      Time elapsed: 03:45:24
                               ETA: 02:55:08
################################################################################
                       Learning iteration 89/3000
                       Computation: 9 steps/s (collection: 168.349s, learning 0.048s)
             Mean action noise std: 1.04
          Mean value\_function loss: 114.4506
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 2.9225
                       Mean reward: -292.82
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0041
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2666
          Episode\_Reward/collision: -11.0243
        Episode\_Reward/action\_rate: -0.0300
        Episode\_Reward/stand\_still: -0.0536
Metrics/target\_pose/position\_error: 4.1911
Metrics/target\_pose/orientation\_error: 1.7130
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 138240
                    Iteration time: 168.40s
                      Time elapsed: 03:48:12
                               ETA: 03:01:28
################################################################################
                       Learning iteration 90/3000
                       Computation: 9 steps/s (collection: 168.779s, learning 0.050s)
             Mean action noise std: 1.04
          Mean value\_function loss: 103.2341
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 2.9224
                       Mean reward: -297.88
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0416
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2094
          Episode\_Reward/collision: -9.8984
        Episode\_Reward/action\_rate: -0.0150
        Episode\_Reward/stand\_still: -0.0413
Metrics/target\_pose/position\_error: 6.1139
Metrics/target\_pose/orientation\_error: 1.6977
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 139776
                    Iteration time: 168.83s
                      Time elapsed: 03:51:01
                               ETA: 03:07:49
################################################################################
                       Learning iteration 91/3000
                       Computation: 9 steps/s (collection: 169.639s, learning 0.049s)
             Mean action noise std: 1.04
          Mean value\_function loss: 91.1471
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 2.9225
                       Mean reward: -296.65
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0113
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1993
          Episode\_Reward/collision: -8.1424
        Episode\_Reward/action\_rate: -0.0379
        Episode\_Reward/stand\_still: -0.0156
Metrics/target\_pose/position\_error: 5.1710
Metrics/target\_pose/orientation\_error: 1.1932
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 141312
                    Iteration time: 169.69s
                      Time elapsed: 03:53:51
                               ETA: 03:14:26
################################################################################
                       Learning iteration 92/3000
                       Computation: 9 steps/s (collection: 169.511s, learning 0.050s)
             Mean action noise std: 1.04
          Mean value\_function loss: 96.2324
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 2.9224
                       Mean reward: -295.56
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0031
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2037
          Episode\_Reward/collision: -8.0208
        Episode\_Reward/action\_rate: -0.4917
        Episode\_Reward/stand\_still: -0.0175
Metrics/target\_pose/position\_error: 3.8124
Metrics/target\_pose/orientation\_error: 2.7960
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 142848
                    Iteration time: 169.56s
                      Time elapsed: 03:56:40
                               ETA: 03:20:47
################################################################################
                       Learning iteration 93/3000
                       Computation: 9 steps/s (collection: 170.455s, learning 0.049s)
             Mean action noise std: 1.04
          Mean value\_function loss: 93.1684
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 2.9222
                       Mean reward: -298.24
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0977
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1950
          Episode\_Reward/collision: -9.0095
        Episode\_Reward/action\_rate: -0.4372
        Episode\_Reward/stand\_still: -0.0275
Metrics/target\_pose/position\_error: 3.5763
Metrics/target\_pose/orientation\_error: 2.1955
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 144384
                    Iteration time: 170.50s
                      Time elapsed: 03:59:31
                               ETA: 03:27:25
################################################################################
                       Learning iteration 94/3000
                       Computation: 9 steps/s (collection: 170.394s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value\_function loss: 94.3308
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 2.9221
                       Mean reward: -302.67
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0305
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2307
          Episode\_Reward/collision: -11.1615
        Episode\_Reward/action\_rate: -0.3740
        Episode\_Reward/stand\_still: -0.0527
Metrics/target\_pose/position\_error: 4.1879
Metrics/target\_pose/orientation\_error: 1.9337
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 145920
                    Iteration time: 170.45s
                      Time elapsed: 04:02:21
                               ETA: 03:33:49
################################################################################
                       Learning iteration 95/3000
                       Computation: 8 steps/s (collection: 171.254s, learning 0.051s)
             Mean action noise std: 1.04
          Mean value\_function loss: 69.8222
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 2.9218
                       Mean reward: -296.50
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0394
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1896
          Episode\_Reward/collision: -8.4748
        Episode\_Reward/action\_rate: -0.1218
        Episode\_Reward/stand\_still: -0.0244
Metrics/target\_pose/position\_error: 5.8189
Metrics/target\_pose/orientation\_error: 0.9485
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 147456
                    Iteration time: 171.31s
                      Time elapsed: 04:05:13
                               ETA: 03:40:28
################################################################################
                       Learning iteration 96/3000
                       Computation: 8 steps/s (collection: 171.492s, learning 0.050s)
             Mean action noise std: 1.04
          Mean value\_function loss: 88.8556
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 2.9216
                       Mean reward: -289.68
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0571
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1908
          Episode\_Reward/collision: -7.2622
        Episode\_Reward/action\_rate: -0.0189
        Episode\_Reward/stand\_still: -0.0129
Metrics/target\_pose/position\_error: 3.5595
Metrics/target\_pose/orientation\_error: 2.0640
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 148992
                    Iteration time: 171.54s
                      Time elapsed: 04:08:04
                               ETA: 03:47:02
################################################################################
                       Learning iteration 97/3000
                       Computation: 8 steps/s (collection: 172.300s, learning 0.050s)
             Mean action noise std: 1.04
          Mean value\_function loss: 78.1397
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 2.9192
                       Mean reward: -283.86
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0190
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1814
          Episode\_Reward/collision: -8.5417
        Episode\_Reward/action\_rate: -0.0024
        Episode\_Reward/stand\_still: -0.0264
Metrics/target\_pose/position\_error: 4.9580
Metrics/target\_pose/orientation\_error: 1.2869
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 150528
                    Iteration time: 172.35s
                      Time elapsed: 04:10:57
                               ETA: 03:53:48
################################################################################
                       Learning iteration 98/3000
                       Computation: 8 steps/s (collection: 172.715s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value\_function loss: 84.9318
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 2.9175
                       Mean reward: -280.12
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0017
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2222
          Episode\_Reward/collision: -8.3073
        Episode\_Reward/action\_rate: -0.0351
        Episode\_Reward/stand\_still: -0.0223
Metrics/target\_pose/position\_error: 5.3817
Metrics/target\_pose/orientation\_error: 1.9464
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 152064
                    Iteration time: 172.77s
                      Time elapsed: 04:13:49
                               ETA: 04:00:35
################################################################################
                       Learning iteration 99/3000
                       Computation: 8 steps/s (collection: 173.192s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value\_function loss: 139.3274
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.9143
                       Mean reward: -281.14
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0222
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2081
          Episode\_Reward/collision: -7.1753
        Episode\_Reward/action\_rate: -0.0024
        Episode\_Reward/stand\_still: -0.0170
Metrics/target\_pose/position\_error: 5.0432
Metrics/target\_pose/orientation\_error: 1.4576
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 153600
                    Iteration time: 173.25s
                      Time elapsed: 04:16:43
                               ETA: 04:07:24
################################################################################
                      Learning iteration 100/3000
                       Computation: 8 steps/s (collection: 173.722s, learning 0.059s)
             Mean action noise std: 1.04
          Mean value\_function loss: 55.8465
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 2.9138
                       Mean reward: -281.35
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0365
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2128
          Episode\_Reward/collision: -12.4297
        Episode\_Reward/action\_rate: -0.0160
        Episode\_Reward/stand\_still: -0.0715
Metrics/target\_pose/position\_error: 4.0868
Metrics/target\_pose/orientation\_error: 2.1083
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 155136
                    Iteration time: 173.78s
                      Time elapsed: 04:19:36
                               ETA: 04:14:17
################################################################################
                      Learning iteration 101/3000
                       Computation: 8 steps/s (collection: 174.454s, learning 0.053s)
             Mean action noise std: 1.04
          Mean value\_function loss: 81.3176
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 2.9136
                       Mean reward: -277.82
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0123
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2065
          Episode\_Reward/collision: -7.2474
        Episode\_Reward/action\_rate: -0.2041
        Episode\_Reward/stand\_still: -0.0146
Metrics/target\_pose/position\_error: 6.4632
Metrics/target\_pose/orientation\_error: 1.7453
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 156672
                    Iteration time: 174.51s
                      Time elapsed: 04:22:31
                               ETA: 04:21:19
################################################################################
                      Learning iteration 102/3000
                       Computation: 8 steps/s (collection: 174.771s, learning 0.054s)
             Mean action noise std: 1.04
          Mean value\_function loss: 113.5017
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 2.9127
                       Mean reward: -267.29
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0206
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1977
          Episode\_Reward/collision: -7.8490
        Episode\_Reward/action\_rate: -0.0046
        Episode\_Reward/stand\_still: -0.0192
Metrics/target\_pose/position\_error: 5.5426
Metrics/target\_pose/orientation\_error: 1.2059
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 158208
                    Iteration time: 174.82s
                      Time elapsed: 04:25:26
                               ETA: 04:28:19
################################################################################
                      Learning iteration 103/3000
                       Computation: 8 steps/s (collection: 175.054s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value\_function loss: 105.5769
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 2.9113
                       Mean reward: -269.15
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0019
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2249
          Episode\_Reward/collision: -8.0877
        Episode\_Reward/action\_rate: -0.0045
        Episode\_Reward/stand\_still: -0.0355
Metrics/target\_pose/position\_error: 4.9072
Metrics/target\_pose/orientation\_error: 1.8153
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 159744
                    Iteration time: 175.11s
                      Time elapsed: 04:28:21
                               ETA: 04:35:15
################################################################################
                      Learning iteration 104/3000
                       Computation: 8 steps/s (collection: 175.535s, learning 0.062s)
             Mean action noise std: 1.04
          Mean value\_function loss: 92.7538
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 2.9102
                       Mean reward: -261.17
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0440
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2058
          Episode\_Reward/collision: -7.9800
        Episode\_Reward/action\_rate: -0.0046
        Episode\_Reward/stand\_still: -0.0243
Metrics/target\_pose/position\_error: 4.9095
Metrics/target\_pose/orientation\_error: 2.0953
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 161280
                    Iteration time: 175.60s
                      Time elapsed: 04:31:16
                               ETA: 04:42:13
################################################################################
                      Learning iteration 105/3000
                       Computation: 8 steps/s (collection: 175.953s, learning 0.058s)
             Mean action noise std: 1.04
          Mean value\_function loss: 164.3892
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 2.9093
                       Mean reward: -262.85
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0362
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2102
          Episode\_Reward/collision: -7.2405
        Episode\_Reward/action\_rate: -0.0251
        Episode\_Reward/stand\_still: -0.0201
Metrics/target\_pose/position\_error: 4.8189
Metrics/target\_pose/orientation\_error: 1.3979
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 162816
                    Iteration time: 176.01s
                      Time elapsed: 04:34:12
                               ETA: 04:49:11
################################################################################
                      Learning iteration 106/3000
                       Computation: 8 steps/s (collection: 176.599s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value\_function loss: 206.5848
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 2.9075
                       Mean reward: -267.21
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0195
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2571
          Episode\_Reward/collision: -10.8125
        Episode\_Reward/action\_rate: -0.0282
        Episode\_Reward/stand\_still: -0.0565
Metrics/target\_pose/position\_error: 4.5524
Metrics/target\_pose/orientation\_error: 1.2945
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 164352
                    Iteration time: 176.65s
                      Time elapsed: 04:37:09
                               ETA: 04:56:16
################################################################################
                      Learning iteration 107/3000
                       Computation: 8 steps/s (collection: 176.882s, learning 0.054s)
             Mean action noise std: 1.03
          Mean value\_function loss: 209.6069
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 2.9063
                       Mean reward: -265.80
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0465
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2061
          Episode\_Reward/collision: -7.5269
        Episode\_Reward/action\_rate: -0.0048
        Episode\_Reward/stand\_still: -0.0260
Metrics/target\_pose/position\_error: 4.4299
Metrics/target\_pose/orientation\_error: 2.3064
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 165888
                    Iteration time: 176.94s
                      Time elapsed: 04:40:06
                               ETA: 05:03:17
################################################################################
                      Learning iteration 108/3000
                       Computation: 8 steps/s (collection: 177.529s, learning 0.059s)
             Mean action noise std: 1.03
          Mean value\_function loss: 159.4734
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 2.9051
                       Mean reward: -264.52
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0140
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2111
          Episode\_Reward/collision: -6.8290
        Episode\_Reward/action\_rate: -0.0050
        Episode\_Reward/stand\_still: -0.0213
Metrics/target\_pose/position\_error: 5.2331
Metrics/target\_pose/orientation\_error: 1.6852
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 167424
                    Iteration time: 177.59s
                      Time elapsed: 04:43:04
                               ETA: 05:10:24
################################################################################
                      Learning iteration 109/3000
                       Computation: 8 steps/s (collection: 177.884s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value\_function loss: 125.8506
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 2.9047
                       Mean reward: -263.62
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0307
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2323
          Episode\_Reward/collision: -7.2769
        Episode\_Reward/action\_rate: -0.0041
        Episode\_Reward/stand\_still: -0.0306
Metrics/target\_pose/position\_error: 3.9986
Metrics/target\_pose/orientation\_error: 1.8092
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 168960
                    Iteration time: 177.94s
                      Time elapsed: 04:46:02
                               ETA: 05:17:30
################################################################################
                      Learning iteration 110/3000
                       Computation: 8 steps/s (collection: 178.706s, learning 0.060s)
             Mean action noise std: 1.03
          Mean value\_function loss: 149.5729
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 2.9041
                       Mean reward: -258.50
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0470
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2449
          Episode\_Reward/collision: -7.6458
        Episode\_Reward/action\_rate: -0.0042
        Episode\_Reward/stand\_still: -0.0325
Metrics/target\_pose/position\_error: 4.1791
Metrics/target\_pose/orientation\_error: 1.3194
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 170496
                    Iteration time: 178.77s
                      Time elapsed: 04:49:00
                               ETA: 05:24:46
################################################################################
                      Learning iteration 111/3000
                       Computation: 8 steps/s (collection: 178.516s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value\_function loss: 160.8088
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 2.9036
                       Mean reward: -259.35
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0047
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2331
          Episode\_Reward/collision: -10.2109
        Episode\_Reward/action\_rate: -0.0034
        Episode\_Reward/stand\_still: -0.0546
Metrics/target\_pose/position\_error: 4.1123
Metrics/target\_pose/orientation\_error: 1.2721
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 172032
                    Iteration time: 178.57s
                      Time elapsed: 04:51:59
                               ETA: 05:31:46
################################################################################
                      Learning iteration 112/3000
                       Computation: 8 steps/s (collection: 179.350s, learning 0.054s)
             Mean action noise std: 1.03
          Mean value\_function loss: 144.2649
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 2.9036
                       Mean reward: -252.36
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0033
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2102
          Episode\_Reward/collision: -7.9592
        Episode\_Reward/action\_rate: -0.0040
        Episode\_Reward/stand\_still: -0.0366
Metrics/target\_pose/position\_error: 4.3254
Metrics/target\_pose/orientation\_error: 1.2242
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 173568
                    Iteration time: 179.40s
                      Time elapsed: 04:54:58
                               ETA: 05:38:57
################################################################################
                      Learning iteration 113/3000
                       Computation: 8 steps/s (collection: 179.501s, learning 0.055s)
             Mean action noise std: 1.03
          Mean value\_function loss: 145.8055
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 2.9036
                       Mean reward: -255.88
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0349
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1893
          Episode\_Reward/collision: -7.2005
        Episode\_Reward/action\_rate: -0.0071
        Episode\_Reward/stand\_still: -0.0289
Metrics/target\_pose/position\_error: 6.0422
Metrics/target\_pose/orientation\_error: 1.5847
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 175104
                    Iteration time: 179.56s
                      Time elapsed: 04:57:58
                               ETA: 05:46:01
################################################################################
                      Learning iteration 114/3000
                       Computation: 8 steps/s (collection: 180.340s, learning 0.055s)
             Mean action noise std: 1.03
          Mean value\_function loss: 495.9351
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 2.9036
                       Mean reward: -254.45
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0352
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2135
          Episode\_Reward/collision: -6.9835
        Episode\_Reward/action\_rate: -0.0254
        Episode\_Reward/stand\_still: -0.0405
Metrics/target\_pose/position\_error: 3.7763
Metrics/target\_pose/orientation\_error: 2.1595
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 176640
                    Iteration time: 180.40s
                      Time elapsed: 05:00:58
                               ETA: 05:53:16
################################################################################
                      Learning iteration 115/3000
                       Computation: 8 steps/s (collection: 180.181s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value\_function loss: 140.6031
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 2.9036
                       Mean reward: -255.26
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0212
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1660
          Episode\_Reward/collision: -11.1701
        Episode\_Reward/action\_rate: -0.0030
        Episode\_Reward/stand\_still: -0.0643
Metrics/target\_pose/position\_error: 3.4747
Metrics/target\_pose/orientation\_error: 2.2198
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 178176
                    Iteration time: 180.23s
                      Time elapsed: 05:03:58
                               ETA: 06:00:15
################################################################################
                      Learning iteration 116/3000
                       Computation: 8 steps/s (collection: 181.151s, learning 0.061s)
             Mean action noise std: 1.03
          Mean value\_function loss: 346.2786
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 2.9033
                       Mean reward: -252.00
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0289
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1940
          Episode\_Reward/collision: -6.0234
        Episode\_Reward/action\_rate: -0.0243
        Episode\_Reward/stand\_still: -0.0297
Metrics/target\_pose/position\_error: 3.8537
Metrics/target\_pose/orientation\_error: 1.4428
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 179712
                    Iteration time: 181.21s
                      Time elapsed: 05:07:00
                               ETA: 06:07:29
################################################################################
                      Learning iteration 117/3000
                       Computation: 8 steps/s (collection: 181.146s, learning 0.056s)
             Mean action noise std: 1.03
          Mean value\_function loss: 173.1699
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 2.9029
                       Mean reward: -246.71
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: -0.0021
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2154
          Episode\_Reward/collision: -6.4696
        Episode\_Reward/action\_rate: -0.0033
        Episode\_Reward/stand\_still: -0.0280
Metrics/target\_pose/position\_error: 3.8646
Metrics/target\_pose/orientation\_error: 1.8762
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 181248
                    Iteration time: 181.20s
                      Time elapsed: 05:10:01
                               ETA: 06:14:33
################################################################################
                      Learning iteration 118/3000
                       Computation: 8 steps/s (collection: 181.812s, learning 0.050s)
             Mean action noise std: 1.03
          Mean value\_function loss: 189.2812
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 2.9025
                       Mean reward: -238.33
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0285
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2029
          Episode\_Reward/collision: -5.2240
        Episode\_Reward/action\_rate: -0.2902
        Episode\_Reward/stand\_still: -0.0316
Metrics/target\_pose/position\_error: 3.7778
Metrics/target\_pose/orientation\_error: 1.6137
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 182784
                    Iteration time: 181.86s
                      Time elapsed: 05:13:03
                               ETA: 06:21:42
################################################################################
                      Learning iteration 119/3000
                       Computation: 8 steps/s (collection: 182.360s, learning 0.050s)
             Mean action noise std: 1.03
          Mean value\_function loss: 137.5210
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 2.9023
                       Mean reward: -238.66
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0171
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1907
          Episode\_Reward/collision: -7.6250
        Episode\_Reward/action\_rate: -0.0063
        Episode\_Reward/stand\_still: -0.0396
Metrics/target\_pose/position\_error: 5.8391
Metrics/target\_pose/orientation\_error: 1.7301
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 184320
                    Iteration time: 182.41s
                      Time elapsed: 05:16:05
                               ETA: 06:28:54
################################################################################
                      Learning iteration 120/3000
                       Computation: 8 steps/s (collection: 182.379s, learning 0.050s)
             Mean action noise std: 1.03
          Mean value\_function loss: 120.7248
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 2.9017
                       Mean reward: -233.91
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0337
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2043
          Episode\_Reward/collision: -5.2778
        Episode\_Reward/action\_rate: -0.0318
        Episode\_Reward/stand\_still: -0.0271
Metrics/target\_pose/position\_error: 5.2765
Metrics/target\_pose/orientation\_error: 2.0121
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 185856
                    Iteration time: 182.43s
                      Time elapsed: 05:19:08
                               ETA: 06:35:56
################################################################################
                      Learning iteration 121/3000
                       Computation: 8 steps/s (collection: 183.224s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value\_function loss: 130.2336
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 2.9013
                       Mean reward: -228.76
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0284
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2019
          Episode\_Reward/collision: -5.4922
        Episode\_Reward/action\_rate: -0.0282
        Episode\_Reward/stand\_still: -0.0263
Metrics/target\_pose/position\_error: 5.6024
Metrics/target\_pose/orientation\_error: 1.9246
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 187392
                    Iteration time: 183.27s
                      Time elapsed: 05:22:11
                               ETA: 06:43:08
################################################################################
                      Learning iteration 122/3000
                       Computation: 8 steps/s (collection: 183.463s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value\_function loss: 172.6099
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 2.9012
                       Mean reward: -227.32
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0339
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2023
          Episode\_Reward/collision: -5.2821
        Episode\_Reward/action\_rate: -0.0334
        Episode\_Reward/stand\_still: -0.0280
Metrics/target\_pose/position\_error: 6.1167
Metrics/target\_pose/orientation\_error: 2.0396
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 188928
                    Iteration time: 183.51s
                      Time elapsed: 05:25:14
                               ETA: 06:50:16
################################################################################
                      Learning iteration 123/3000
                       Computation: 8 steps/s (collection: 184.143s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value\_function loss: 126.0393
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 2.9010
                       Mean reward: -220.71
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0440
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2042
          Episode\_Reward/collision: -5.3776
        Episode\_Reward/action\_rate: -0.1252
        Episode\_Reward/stand\_still: -0.0290
Metrics/target\_pose/position\_error: 4.9261
Metrics/target\_pose/orientation\_error: 1.5862
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 190464
                    Iteration time: 184.19s
                      Time elapsed: 05:28:19
                               ETA: 06:57:30
################################################################################
                      Learning iteration 124/3000
                       Computation: 8 steps/s (collection: 184.080s, learning 0.055s)
             Mean action noise std: 1.03
          Mean value\_function loss: 142.6942
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 2.9008
                       Mean reward: -217.52
               Mean episode length: 180.00
   Episode\_Reward/progress\_to\_goal: 0.0416
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1991
          Episode\_Reward/collision: -4.7075
        Episode\_Reward/action\_rate: -0.0112
        Episode\_Reward/stand\_still: -0.0330
Metrics/target\_pose/position\_error: 5.7336
Metrics/target\_pose/orientation\_error: 1.6608
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 192000
                    Iteration time: 184.14s
                      Time elapsed: 05:31:23
                               ETA: 07:04:32
################################################################################
                      Learning iteration 125/3000
                       Computation: 8 steps/s (collection: 185.025s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value\_function loss: 311.5695
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 2.9003
                       Mean reward: -211.84
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0241
         Episode\_Reward/reach\_goal: 0.0029
          Episode\_Reward/face\_goal: 0.2013
          Episode\_Reward/collision: -4.6823
        Episode\_Reward/action\_rate: -0.0103
        Episode\_Reward/stand\_still: -0.0290
Metrics/target\_pose/position\_error: 3.8978
Metrics/target\_pose/orientation\_error: 2.1280
      Episode\_Termination/time\_out: 0.9909
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0091
                   Total timesteps: 193536
                    Iteration time: 185.08s
                      Time elapsed: 05:34:28
                               ETA: 07:11:47
################################################################################
                      Learning iteration 126/3000
                       Computation: 8 steps/s (collection: 185.102s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value\_function loss: 159.2069
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 2.8993
                       Mean reward: -205.40
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0837
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2082
          Episode\_Reward/collision: -4.9323
        Episode\_Reward/action\_rate: -0.0398
        Episode\_Reward/stand\_still: -0.0318
Metrics/target\_pose/position\_error: 4.9962
Metrics/target\_pose/orientation\_error: 1.2536
      Episode\_Termination/time\_out: 0.9375
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0625
                   Total timesteps: 195072
                    Iteration time: 185.15s
                      Time elapsed: 05:37:33
                               ETA: 07:18:53
################################################################################
                      Learning iteration 127/3000
                       Computation: 8 steps/s (collection: 185.808s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value\_function loss: 255.7963
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 2.8971
                       Mean reward: -196.84
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0312
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1963
          Episode\_Reward/collision: -5.7170
        Episode\_Reward/action\_rate: -0.0053
        Episode\_Reward/stand\_still: -0.0411
Metrics/target\_pose/position\_error: 5.5986
Metrics/target\_pose/orientation\_error: 2.4989
      Episode\_Termination/time\_out: 0.9544
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0456
                   Total timesteps: 196608
                    Iteration time: 185.86s
                      Time elapsed: 05:40:39
                               ETA: 07:26:06
################################################################################
                      Learning iteration 128/3000
                       Computation: 8 steps/s (collection: 185.640s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value\_function loss: 345.8673
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 2.8964
                       Mean reward: -196.39
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0900
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2148
          Episode\_Reward/collision: -6.0139
        Episode\_Reward/action\_rate: -0.0128
        Episode\_Reward/stand\_still: -0.0335
Metrics/target\_pose/position\_error: 2.9591
Metrics/target\_pose/orientation\_error: 2.4471
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 198144
                    Iteration time: 185.69s
                      Time elapsed: 05:43:44
                               ETA: 07:33:05
################################################################################
                      Learning iteration 129/3000
                       Computation: 8 steps/s (collection: 186.621s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value\_function loss: 253.9988
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 2.8969
                       Mean reward: -189.94
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0748
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2338
          Episode\_Reward/collision: -5.4818
        Episode\_Reward/action\_rate: -0.0102
        Episode\_Reward/stand\_still: -0.0329
Metrics/target\_pose/position\_error: 2.9112
Metrics/target\_pose/orientation\_error: 1.8630
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 199680
                    Iteration time: 186.67s
                      Time elapsed: 05:46:51
                               ETA: 07:40:17
################################################################################
                      Learning iteration 130/3000
                       Computation: 8 steps/s (collection: 186.955s, learning 0.047s)
             Mean action noise std: 1.03
          Mean value\_function loss: 149.5880
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 2.8966
                       Mean reward: -187.87
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0886
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2349
          Episode\_Reward/collision: -6.8542
        Episode\_Reward/action\_rate: -0.0280
        Episode\_Reward/stand\_still: -0.0390
Metrics/target\_pose/position\_error: 6.2030
Metrics/target\_pose/orientation\_error: 1.7820
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 201216
                    Iteration time: 187.00s
                      Time elapsed: 05:49:58
                               ETA: 07:47:27
################################################################################
                      Learning iteration 131/3000
                       Computation: 8 steps/s (collection: 187.216s, learning 0.059s)
             Mean action noise std: 1.03
          Mean value\_function loss: 118.1950
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 2.8966
                       Mean reward: -183.75
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0756
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2443
          Episode\_Reward/collision: -6.2891
        Episode\_Reward/action\_rate: -0.0229
        Episode\_Reward/stand\_still: -0.0366
Metrics/target\_pose/position\_error: 4.2747
Metrics/target\_pose/orientation\_error: 2.2410
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 202752
                    Iteration time: 187.27s
                      Time elapsed: 05:53:05
                               ETA: 07:54:33
################################################################################
                      Learning iteration 132/3000
                       Computation: 8 steps/s (collection: 187.805s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value\_function loss: 136.5094
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 2.8961
                       Mean reward: -182.74
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0923
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2220
          Episode\_Reward/collision: -5.9384
        Episode\_Reward/action\_rate: -0.0156
        Episode\_Reward/stand\_still: -0.0261
Metrics/target\_pose/position\_error: 3.5130
Metrics/target\_pose/orientation\_error: 2.1130
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 204288
                    Iteration time: 187.85s
                      Time elapsed: 05:56:13
                               ETA: 08:01:42
################################################################################
                      Learning iteration 133/3000
                       Computation: 8 steps/s (collection: 187.994s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value\_function loss: 137.9959
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 2.8956
                       Mean reward: -183.66
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.1192
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2154
          Episode\_Reward/collision: -4.7804
        Episode\_Reward/action\_rate: -0.0149
        Episode\_Reward/stand\_still: -0.0268
Metrics/target\_pose/position\_error: 5.0569
Metrics/target\_pose/orientation\_error: 0.9248
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 205824
                    Iteration time: 188.05s
                      Time elapsed: 05:59:21
                               ETA: 08:08:46
################################################################################
                      Learning iteration 134/3000
                       Computation: 8 steps/s (collection: 188.565s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value\_function loss: 230.3041
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 2.8942
                       Mean reward: -183.03
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0623
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.2143
          Episode\_Reward/collision: -6.2847
        Episode\_Reward/action\_rate: -0.0032
        Episode\_Reward/stand\_still: -0.0299
Metrics/target\_pose/position\_error: 3.7789
Metrics/target\_pose/orientation\_error: 1.1447
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 207360
                    Iteration time: 188.61s
                      Time elapsed: 06:02:30
                               ETA: 08:15:54
################################################################################
                      Learning iteration 135/3000
                       Computation: 8 steps/s (collection: 190.631s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value\_function loss: 215.1408
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 2.8941
                       Mean reward: -187.88
               Mean episode length: 179.49
   Episode\_Reward/progress\_to\_goal: 0.0866
         Episode\_Reward/reach\_goal: 0.0000
          Episode\_Reward/face\_goal: 0.1674
          Episode\_Reward/collision: -7.2873
        Episode\_Reward/action\_rate: -0.0042
        Episode\_Reward/stand\_still: -0.0463
Metrics/target\_pose/position\_error: 3.4898
Metrics/target\_pose/orientation\_error: 1.6750
      Episode\_Termination/time\_out: 1.0000
   Episode\_Termination/base\_height: 0.0000
    Episode\_Termination/reach\_goal: 0.0000
                   Total timesteps: 208896
                    Iteration time: 190.68s
                      Time elapsed: 06:05:41
                               ETA: 08:23:35

################################################################################
                      Learning iteration 138/3000

                       Computation: 8 steps/s (collection: 190.350s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 217.2124
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 2.8936
                       Mean reward: -201.84
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0063
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2195
          Episode_Reward/collision: -6.9436
        Episode_Reward/action_rate: -0.0063
        Episode_Reward/stand_still: -0.0387
Metrics/target_pose/position_error: 5.4595
Metrics/target_pose/orientation_error: 1.9584
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 213504
                    Iteration time: 190.40s
                      Time elapsed: 06:15:17
                               ETA: 08:47:10

################################################################################
                      Learning iteration 139/3000

                       Computation: 8 steps/s (collection: 190.728s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value_function loss: 197.5687
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 2.8935
                       Mean reward: -205.36
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0685
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2088
          Episode_Reward/collision: -6.8472
        Episode_Reward/action_rate: -0.0032
        Episode_Reward/stand_still: -0.0405
Metrics/target_pose/position_error: 3.6808
Metrics/target_pose/orientation_error: 1.3658
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 215040
                    Iteration time: 190.78s
                      Time elapsed: 06:18:28
                               ETA: 08:54:16

################################################################################
                      Learning iteration 140/3000

                       Computation: 8 steps/s (collection: 191.243s, learning 0.050s)
             Mean action noise std: 1.03
          Mean value_function loss: 185.2870
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 2.8929
                       Mean reward: -212.59
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0591
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2270
          Episode_Reward/collision: -7.9149
        Episode_Reward/action_rate: -0.0051
        Episode_Reward/stand_still: -0.0438
Metrics/target_pose/position_error: 4.6950
Metrics/target_pose/orientation_error: 1.0097
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 216576
                    Iteration time: 191.29s
                      Time elapsed: 06:21:39
                               ETA: 09:01:24

################################################################################
                      Learning iteration 141/3000

                       Computation: 8 steps/s (collection: 191.751s, learning 0.050s)
             Mean action noise std: 1.03
          Mean value_function loss: 195.2996
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 2.8918
                       Mean reward: -214.08
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0291
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2967
          Episode_Reward/collision: -9.6406
        Episode_Reward/action_rate: -0.0026
        Episode_Reward/stand_still: -0.0636
Metrics/target_pose/position_error: 4.1770
Metrics/target_pose/orientation_error: 1.4503
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 218112
                    Iteration time: 191.80s
                      Time elapsed: 06:24:51
                               ETA: 09:08:34

################################################################################
                      Learning iteration 142/3000

                       Computation: 8 steps/s (collection: 191.350s, learning 0.055s)
             Mean action noise std: 1.03
          Mean value_function loss: 374.8943
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 2.8912
                       Mean reward: -224.83
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0613
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2328
          Episode_Reward/collision: -7.0113
        Episode_Reward/action_rate: -0.0072
        Episode_Reward/stand_still: -0.0402
Metrics/target_pose/position_error: 4.8148
Metrics/target_pose/orientation_error: 1.4914
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 219648
                    Iteration time: 191.41s
                      Time elapsed: 06:28:02
                               ETA: 09:15:27

################################################################################
                      Learning iteration 143/3000

                       Computation: 7 steps/s (collection: 191.986s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 190.4177
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 2.8910
                       Mean reward: -224.68
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0486
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2193
          Episode_Reward/collision: -6.9002
        Episode_Reward/action_rate: -0.0047
        Episode_Reward/stand_still: -0.0371
Metrics/target_pose/position_error: 4.7807
Metrics/target_pose/orientation_error: 1.1969
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 221184
                    Iteration time: 192.04s
                      Time elapsed: 06:31:14
                               ETA: 09:22:24

################################################################################
                      Learning iteration 144/3000

                       Computation: 7 steps/s (collection: 192.610s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 142.3573
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 2.8909
                       Mean reward: -225.05
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0750
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2092
          Episode_Reward/collision: -4.7344
        Episode_Reward/action_rate: -0.0240
        Episode_Reward/stand_still: -0.0312
Metrics/target_pose/position_error: 4.7745
Metrics/target_pose/orientation_error: 1.7109
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 222720
                    Iteration time: 192.66s
                      Time elapsed: 06:34:27
                               ETA: 09:29:24

################################################################################
                      Learning iteration 145/3000

                       Computation: 7 steps/s (collection: 192.777s, learning 0.050s)
             Mean action noise std: 1.03
          Mean value_function loss: 221.5636
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 2.8909
                       Mean reward: -222.39
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.1014
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2118
          Episode_Reward/collision: -4.9592
        Episode_Reward/action_rate: -0.0048
        Episode_Reward/stand_still: -0.0269
Metrics/target_pose/position_error: 4.9238
Metrics/target_pose/orientation_error: 1.4588
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 224256
                    Iteration time: 192.83s
                      Time elapsed: 06:37:40
                               ETA: 09:36:20

################################################################################
                      Learning iteration 146/3000

                       Computation: 7 steps/s (collection: 193.218s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 198.5242
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 2.8908
                       Mean reward: -216.80
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0837
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2102
          Episode_Reward/collision: -4.2595
        Episode_Reward/action_rate: -0.0057
        Episode_Reward/stand_still: -0.0354
Metrics/target_pose/position_error: 6.4600
Metrics/target_pose/orientation_error: 1.6247
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 225792
                    Iteration time: 193.27s
                      Time elapsed: 06:40:53
                               ETA: 09:43:16

################################################################################
                      Learning iteration 147/3000

                       Computation: 7 steps/s (collection: 193.622s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value_function loss: 147.2096
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.8901
                       Mean reward: -214.48
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0496
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2308
          Episode_Reward/collision: -6.7977
        Episode_Reward/action_rate: -0.0043
        Episode_Reward/stand_still: -0.0426
Metrics/target_pose/position_error: 4.7809
Metrics/target_pose/orientation_error: 0.6728
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 227328
                    Iteration time: 193.68s
                      Time elapsed: 06:44:07
                               ETA: 09:50:12

################################################################################
                      Learning iteration 148/3000

                       Computation: 7 steps/s (collection: 194.042s, learning 0.056s)
             Mean action noise std: 1.03
          Mean value_function loss: 180.1939
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 2.8888
                       Mean reward: -209.87
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0525
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2022
          Episode_Reward/collision: -3.9974
        Episode_Reward/action_rate: -0.0076
        Episode_Reward/stand_still: -0.0366
Metrics/target_pose/position_error: 5.3441
Metrics/target_pose/orientation_error: 2.0505
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 228864
                    Iteration time: 194.10s
                      Time elapsed: 06:47:21
                               ETA: 09:57:07

################################################################################
                      Learning iteration 149/3000

                       Computation: 7 steps/s (collection: 194.759s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 189.2991
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 2.8885
                       Mean reward: -201.42
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0753
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2043
          Episode_Reward/collision: -4.6528
        Episode_Reward/action_rate: -0.0046
        Episode_Reward/stand_still: -0.0317
Metrics/target_pose/position_error: 3.8371
Metrics/target_pose/orientation_error: 1.1886
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 230400
                    Iteration time: 194.81s
                      Time elapsed: 06:50:36
                               ETA: 10:04:08

################################################################################
                      Learning iteration 150/3000

                       Computation: 7 steps/s (collection: 194.751s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value_function loss: 260.6448
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 2.8885
                       Mean reward: -203.23
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0479
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1602
          Episode_Reward/collision: -8.6493
        Episode_Reward/action_rate: -0.0115
        Episode_Reward/stand_still: -0.0655
Metrics/target_pose/position_error: 4.9452
Metrics/target_pose/orientation_error: 2.1633
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 231936
                    Iteration time: 194.80s
                      Time elapsed: 06:53:50
                               ETA: 10:11:01

################################################################################
                      Learning iteration 151/3000

                       Computation: 7 steps/s (collection: 195.402s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value_function loss: 162.6113
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 2.8884
                       Mean reward: -194.47
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0685
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1810
          Episode_Reward/collision: -6.0095
        Episode_Reward/action_rate: -0.0366
        Episode_Reward/stand_still: -0.0589
Metrics/target_pose/position_error: 4.8281
Metrics/target_pose/orientation_error: 1.9804
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 233472
                    Iteration time: 195.45s
                      Time elapsed: 06:57:06
                               ETA: 10:17:57

################################################################################
                      Learning iteration 152/3000

                       Computation: 7 steps/s (collection: 195.771s, learning 0.051s)
             Mean action noise std: 1.02
          Mean value_function loss: 1070.8471
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 2.8864
                       Mean reward: -189.29
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0404
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1904
          Episode_Reward/collision: -6.2969
        Episode_Reward/action_rate: -0.1721
        Episode_Reward/stand_still: -0.0589
Metrics/target_pose/position_error: 2.3132
Metrics/target_pose/orientation_error: 1.4259
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 235008
                    Iteration time: 195.82s
                      Time elapsed: 07:00:22
                               ETA: 10:24:53

################################################################################
                      Learning iteration 153/3000

                       Computation: 7 steps/s (collection: 196.023s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value_function loss: 206.4925
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 2.8858
                       Mean reward: -185.20
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0401
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2020
          Episode_Reward/collision: -6.2344
        Episode_Reward/action_rate: -0.0054
        Episode_Reward/stand_still: -0.0592
Metrics/target_pose/position_error: 2.6645
Metrics/target_pose/orientation_error: 1.3161
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 236544
                    Iteration time: 196.08s
                      Time elapsed: 07:03:38
                               ETA: 10:31:46

################################################################################
                      Learning iteration 154/3000

                       Computation: 7 steps/s (collection: 196.728s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value_function loss: 138.9160
               Mean surrogate loss: 0.0179
                 Mean entropy loss: 2.8859
                       Mean reward: -177.96
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0333
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2607
          Episode_Reward/collision: -7.4688
        Episode_Reward/action_rate: -0.0311
        Episode_Reward/stand_still: -0.0668
Metrics/target_pose/position_error: 5.4538
Metrics/target_pose/orientation_error: 1.9450
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 238080
                    Iteration time: 196.78s
                      Time elapsed: 07:06:54
                               ETA: 10:38:43

################################################################################
                      Learning iteration 155/3000

                       Computation: 7 steps/s (collection: 197.058s, learning 0.053s)
             Mean action noise std: 1.02
          Mean value_function loss: 367.2904
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 2.8860
                       Mean reward: -175.11
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0630
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2088
          Episode_Reward/collision: -3.6293
        Episode_Reward/action_rate: -0.0134
        Episode_Reward/stand_still: -0.0570
Metrics/target_pose/position_error: 5.5674
Metrics/target_pose/orientation_error: 2.4324
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 239616
                    Iteration time: 197.11s
                      Time elapsed: 07:10:12
                               ETA: 10:45:39

################################################################################
                      Learning iteration 156/3000

                       Computation: 7 steps/s (collection: 197.407s, learning 0.054s)
             Mean action noise std: 1.02
          Mean value_function loss: 181.2125
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 2.8860
                       Mean reward: -168.54
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0595
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2055
          Episode_Reward/collision: -2.3785
        Episode_Reward/action_rate: -0.0029
        Episode_Reward/stand_still: -0.0621
Metrics/target_pose/position_error: 5.1961
Metrics/target_pose/orientation_error: 1.2273
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 241152
                    Iteration time: 197.46s
                      Time elapsed: 07:13:29
                               ETA: 10:52:33

################################################################################
                      Learning iteration 157/3000

                       Computation: 7 steps/s (collection: 197.881s, learning 0.058s)
             Mean action noise std: 1.02
          Mean value_function loss: 376.0650
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 2.8858
                       Mean reward: -173.91
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0285
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2064
          Episode_Reward/collision: -4.0512
        Episode_Reward/action_rate: -0.0253
        Episode_Reward/stand_still: -0.0667
Metrics/target_pose/position_error: 3.4629
Metrics/target_pose/orientation_error: 1.9835
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 242688
                    Iteration time: 197.94s
                      Time elapsed: 07:16:47
                               ETA: 10:59:28

################################################################################
                      Learning iteration 158/3000

                       Computation: 7 steps/s (collection: 198.347s, learning 0.053s)
             Mean action noise std: 1.02
          Mean value_function loss: 307.9707
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 2.8857
                       Mean reward: -176.67
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0217
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2613
          Episode_Reward/collision: -7.9210
        Episode_Reward/action_rate: -0.0074
        Episode_Reward/stand_still: -0.0742
Metrics/target_pose/position_error: 3.6383
Metrics/target_pose/orientation_error: 1.8417
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 244224
                    Iteration time: 198.40s
                      Time elapsed: 07:20:05
                               ETA: 11:06:24

################################################################################
                      Learning iteration 159/3000

                       Computation: 7 steps/s (collection: 199.190s, learning 0.061s)
             Mean action noise std: 1.02
          Mean value_function loss: 219.8483
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.8856
                       Mean reward: -163.62
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0189
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1948
          Episode_Reward/collision: -4.2622
        Episode_Reward/action_rate: -0.0102
        Episode_Reward/stand_still: -0.0686
Metrics/target_pose/position_error: 4.4623
Metrics/target_pose/orientation_error: 1.8950
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 245760
                    Iteration time: 199.25s
                      Time elapsed: 07:23:25
                               ETA: 11:13:27

################################################################################
                      Learning iteration 160/3000

                       Computation: 7 steps/s (collection: 198.910s, learning 0.054s)
             Mean action noise std: 1.02
          Mean value_function loss: 195.5270
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 2.8853
                       Mean reward: -160.36
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0143
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1802
          Episode_Reward/collision: -4.7405
        Episode_Reward/action_rate: -0.0179
        Episode_Reward/stand_still: -0.0705
Metrics/target_pose/position_error: 3.2347
Metrics/target_pose/orientation_error: 0.9650
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 247296
                    Iteration time: 198.96s
                      Time elapsed: 07:26:44
                               ETA: 11:20:17

################################################################################
                      Learning iteration 161/3000

                       Computation: 7 steps/s (collection: 200.385s, learning 0.052s)
             Mean action noise std: 1.02
          Mean value_function loss: 188.4519
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 2.8850
                       Mean reward: -157.62
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0356
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2212
          Episode_Reward/collision: -3.1693
        Episode_Reward/action_rate: -0.0055
        Episode_Reward/stand_still: -0.0678
Metrics/target_pose/position_error: 4.9289
Metrics/target_pose/orientation_error: 1.2478
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 248832
                    Iteration time: 200.44s
                      Time elapsed: 07:30:04
                               ETA: 11:27:25

################################################################################
                      Learning iteration 162/3000

                       Computation: 7 steps/s (collection: 200.218s, learning 0.053s)
             Mean action noise std: 1.02
          Mean value_function loss: 197.3117
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 2.8847
                       Mean reward: -155.74
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0308
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2038
          Episode_Reward/collision: -3.4115
        Episode_Reward/action_rate: -0.0057
        Episode_Reward/stand_still: -0.0644
Metrics/target_pose/position_error: 5.6747
Metrics/target_pose/orientation_error: 1.3082
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 250368
                    Iteration time: 200.27s
                      Time elapsed: 07:33:24
                               ETA: 11:34:23

################################################################################
                      Learning iteration 163/3000

                       Computation: 7 steps/s (collection: 200.777s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value_function loss: 348.7559
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 2.8844
                       Mean reward: -148.76
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0402
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2053
          Episode_Reward/collision: -2.7361
        Episode_Reward/action_rate: -0.0029
        Episode_Reward/stand_still: -0.0706
Metrics/target_pose/position_error: 4.5922
Metrics/target_pose/orientation_error: 1.4479
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 251904
                    Iteration time: 200.83s
                      Time elapsed: 07:36:45
                               ETA: 11:41:23

################################################################################
                      Learning iteration 164/3000

                       Computation: 7 steps/s (collection: 201.424s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value_function loss: 206.5960
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 2.8837
                       Mean reward: -138.80
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0240
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2258
          Episode_Reward/collision: -3.8802
        Episode_Reward/action_rate: -0.0089
        Episode_Reward/stand_still: -0.0736
Metrics/target_pose/position_error: 4.9145
Metrics/target_pose/orientation_error: 0.7436
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 253440
                    Iteration time: 201.48s
                      Time elapsed: 07:40:07
                               ETA: 11:48:27

################################################################################
                      Learning iteration 165/3000

                       Computation: 7 steps/s (collection: 202.032s, learning 0.057s)
             Mean action noise std: 1.02
          Mean value_function loss: 158.2065
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 2.8833
                       Mean reward: -133.30
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0401
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1977
          Episode_Reward/collision: -2.7457
        Episode_Reward/action_rate: -0.0054
        Episode_Reward/stand_still: -0.0730
Metrics/target_pose/position_error: 5.0057
Metrics/target_pose/orientation_error: 2.4909
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 254976
                    Iteration time: 202.09s
                      Time elapsed: 07:43:29
                               ETA: 11:55:34

################################################################################
                      Learning iteration 166/3000

                       Computation: 7 steps/s (collection: 202.451s, learning 0.053s)
             Mean action noise std: 1.02
          Mean value_function loss: 192.4541
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 2.8809
                       Mean reward: -129.96
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0271
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2031
          Episode_Reward/collision: -2.7691
        Episode_Reward/action_rate: -0.0129
        Episode_Reward/stand_still: -0.0685
Metrics/target_pose/position_error: 4.7847
Metrics/target_pose/orientation_error: 1.7073
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 256512
                    Iteration time: 202.50s
                      Time elapsed: 07:46:51
                               ETA: 12:02:40

################################################################################
                      Learning iteration 167/3000

                       Computation: 7 steps/s (collection: 202.751s, learning 0.054s)
             Mean action noise std: 1.02
          Mean value_function loss: 191.6106
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 2.8811
                       Mean reward: -129.75
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0096
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2143
          Episode_Reward/collision: -4.0425
        Episode_Reward/action_rate: -0.0028
        Episode_Reward/stand_still: -0.0826
Metrics/target_pose/position_error: 3.2590
Metrics/target_pose/orientation_error: 0.8556
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 258048
                    Iteration time: 202.81s
                      Time elapsed: 07:50:14
                               ETA: 12:09:43

################################################################################
                      Learning iteration 168/3000

                       Computation: 7 steps/s (collection: 203.566s, learning 0.055s)
             Mean action noise std: 1.02
          Mean value_function loss: 166.7135
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 2.8812
                       Mean reward: -116.02
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0135
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2170
          Episode_Reward/collision: -4.2743
        Episode_Reward/action_rate: -0.0039
        Episode_Reward/stand_still: -0.0799
Metrics/target_pose/position_error: 3.8133
Metrics/target_pose/orientation_error: 1.7928
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 259584
                    Iteration time: 203.62s
                      Time elapsed: 07:53:38
                               ETA: 12:16:53

################################################################################
                      Learning iteration 169/3000

                       Computation: 7 steps/s (collection: 204.126s, learning 0.055s)
             Mean action noise std: 1.02
          Mean value_function loss: 112.2900
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 2.8812
                       Mean reward: -114.59
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0106
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2024
          Episode_Reward/collision: -1.4896
        Episode_Reward/action_rate: -0.0025
        Episode_Reward/stand_still: -0.0786
Metrics/target_pose/position_error: 5.9377
Metrics/target_pose/orientation_error: 1.1732
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 261120
                    Iteration time: 204.18s
                      Time elapsed: 07:57:02
                               ETA: 12:24:05

################################################################################
                      Learning iteration 170/3000

                       Computation: 7 steps/s (collection: 204.883s, learning 0.048s)
             Mean action noise std: 1.02
          Mean value_function loss: 154.4755
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 2.8811
                       Mean reward: -98.71
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0222
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2023
          Episode_Reward/collision: -1.3845
        Episode_Reward/action_rate: -0.0129
        Episode_Reward/stand_still: -0.0766
Metrics/target_pose/position_error: 3.5489
Metrics/target_pose/orientation_error: 1.7058
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 262656
                    Iteration time: 204.93s
                      Time elapsed: 08:00:27
                               ETA: 12:31:22

################################################################################
                      Learning iteration 171/3000

                       Computation: 7 steps/s (collection: 205.241s, learning 0.050s)
             Mean action noise std: 1.02
          Mean value_function loss: 123.5481
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 2.8807
                       Mean reward: -95.46
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0263
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1998
          Episode_Reward/collision: -1.5608
        Episode_Reward/action_rate: -0.0079
        Episode_Reward/stand_still: -0.0790
Metrics/target_pose/position_error: 5.3785
Metrics/target_pose/orientation_error: 1.3485
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 264192
                    Iteration time: 205.29s
                      Time elapsed: 08:03:52
                               ETA: 12:38:37

################################################################################
                      Learning iteration 172/3000

                       Computation: 7 steps/s (collection: 206.029s, learning 0.049s)
             Mean action noise std: 1.02
          Mean value_function loss: 229.4024
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 2.8804
                       Mean reward: -94.70
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0091
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1958
          Episode_Reward/collision: -2.5373
        Episode_Reward/action_rate: -0.0047
        Episode_Reward/stand_still: -0.0820
Metrics/target_pose/position_error: 3.8250
Metrics/target_pose/orientation_error: 1.3196
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 265728
                    Iteration time: 206.08s
                      Time elapsed: 08:07:18
                               ETA: 12:45:58

################################################################################
                      Learning iteration 173/3000

                       Computation: 7 steps/s (collection: 206.182s, learning 0.053s)
             Mean action noise std: 1.02
          Mean value_function loss: 190.9935
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 2.8801
                       Mean reward: -106.45
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0239
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2052
          Episode_Reward/collision: -3.4653
        Episode_Reward/action_rate: -0.0172
        Episode_Reward/stand_still: -0.0795
Metrics/target_pose/position_error: 5.9485
Metrics/target_pose/orientation_error: 0.5812
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 267264
                    Iteration time: 206.24s
                      Time elapsed: 08:10:44
                               ETA: 12:53:14

################################################################################
                      Learning iteration 174/3000

                       Computation: 7 steps/s (collection: 206.962s, learning 0.049s)
             Mean action noise std: 1.02
          Mean value_function loss: 231.0305
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 2.8784
                       Mean reward: -100.04
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0225
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1906
          Episode_Reward/collision: -3.0182
        Episode_Reward/action_rate: -0.0049
        Episode_Reward/stand_still: -0.0838
Metrics/target_pose/position_error: 5.8051
Metrics/target_pose/orientation_error: 1.8384
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 268800
                    Iteration time: 207.01s
                      Time elapsed: 08:14:11
                               ETA: 13:00:35

################################################################################
                      Learning iteration 175/3000

                       Computation: 7 steps/s (collection: 207.635s, learning 0.052s)
             Mean action noise std: 1.02
          Mean value_function loss: 188.5330
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 2.8776
                       Mean reward: -97.49
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0066
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2049
          Episode_Reward/collision: -1.2188
        Episode_Reward/action_rate: -0.0094
        Episode_Reward/stand_still: -0.0829
Metrics/target_pose/position_error: 4.4739
Metrics/target_pose/orientation_error: 2.3305
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 270336
                    Iteration time: 207.69s
                      Time elapsed: 08:17:39
                               ETA: 13:07:59

################################################################################
                      Learning iteration 176/3000

                       Computation: 7 steps/s (collection: 208.424s, learning 0.048s)
             Mean action noise std: 1.02
          Mean value_function loss: 274.3923
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 2.8761
                       Mean reward: -96.23
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0119
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1624
          Episode_Reward/collision: -5.1736
        Episode_Reward/action_rate: -0.0065
        Episode_Reward/stand_still: -0.0856
Metrics/target_pose/position_error: 5.6777
Metrics/target_pose/orientation_error: 1.4209
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 271872
                    Iteration time: 208.47s
                      Time elapsed: 08:21:08
                               ETA: 13:15:29

################################################################################
                      Learning iteration 177/3000

                       Computation: 7 steps/s (collection: 208.870s, learning 0.057s)
             Mean action noise std: 1.02
          Mean value_function loss: 249.6835
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 2.8757
                       Mean reward: -94.21
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0249
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1952
          Episode_Reward/collision: -2.6128
        Episode_Reward/action_rate: -0.0034
        Episode_Reward/stand_still: -0.0815
Metrics/target_pose/position_error: 3.4376
Metrics/target_pose/orientation_error: 0.5674
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 273408
                    Iteration time: 208.93s
                      Time elapsed: 08:24:36
                               ETA: 13:22:58

################################################################################
                      Learning iteration 178/3000

                       Computation: 7 steps/s (collection: 209.825s, learning 0.051s)
             Mean action noise std: 1.02
          Mean value_function loss: 182.0123
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 2.8757
                       Mean reward: -96.66
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0070
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1656
          Episode_Reward/collision: -5.0095
        Episode_Reward/action_rate: -0.0048
        Episode_Reward/stand_still: -0.0865
Metrics/target_pose/position_error: 4.0094
Metrics/target_pose/orientation_error: 2.5771
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 274944
                    Iteration time: 209.88s
                      Time elapsed: 08:28:06
                               ETA: 13:30:35

################################################################################
                      Learning iteration 179/3000

                       Computation: 7 steps/s (collection: 210.431s, learning 0.051s)
             Mean action noise std: 1.02
          Mean value_function loss: 147.9455
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 2.8757
                       Mean reward: -90.51
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0104
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2077
          Episode_Reward/collision: -1.6198
        Episode_Reward/action_rate: -0.2027
        Episode_Reward/stand_still: -0.0822
Metrics/target_pose/position_error: 3.0309
Metrics/target_pose/orientation_error: 1.7951
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 276480
                    Iteration time: 210.48s
                      Time elapsed: 08:31:37
                               ETA: 13:38:14

################################################################################
                      Learning iteration 180/3000

                       Computation: 7 steps/s (collection: 210.914s, learning 0.048s)
             Mean action noise std: 1.02
          Mean value_function loss: 115.7454
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.8755
                       Mean reward: -89.49
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0111
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2012
          Episode_Reward/collision: -1.0130
        Episode_Reward/action_rate: -0.0110
        Episode_Reward/stand_still: -0.0862
Metrics/target_pose/position_error: 3.5948
Metrics/target_pose/orientation_error: 2.4430
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 278016
                    Iteration time: 210.96s
                      Time elapsed: 08:35:08
                               ETA: 13:45:54

################################################################################
                      Learning iteration 181/3000

                       Computation: 7 steps/s (collection: 212.518s, learning 0.055s)
             Mean action noise std: 1.02
          Mean value_function loss: 220.9002
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.8749
                       Mean reward: -93.58
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0061
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1948
          Episode_Reward/collision: -2.0451
        Episode_Reward/action_rate: -0.0340
        Episode_Reward/stand_still: -0.0848
Metrics/target_pose/position_error: 5.4606
Metrics/target_pose/orientation_error: 1.4738
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 279552
                    Iteration time: 212.57s
                      Time elapsed: 08:38:40
                               ETA: 13:53:50

################################################################################
                      Learning iteration 182/3000

                       Computation: 7 steps/s (collection: 211.595s, learning 0.049s)
             Mean action noise std: 1.02
          Mean value_function loss: 212.8182
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 2.8748
                       Mean reward: -101.42
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0004
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2572
          Episode_Reward/collision: -5.0642
        Episode_Reward/action_rate: -0.0208
        Episode_Reward/stand_still: -0.0885
Metrics/target_pose/position_error: 3.3867
Metrics/target_pose/orientation_error: 1.7734
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 281088
                    Iteration time: 211.64s
                      Time elapsed: 08:42:12
                               ETA: 14:01:25

################################################################################
                      Learning iteration 183/3000

                       Computation: 7 steps/s (collection: 212.136s, learning 0.049s)
             Mean action noise std: 1.02
          Mean value_function loss: 202.7280
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 2.8750
                       Mean reward: -108.49
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0030
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2636
          Episode_Reward/collision: -6.6632
        Episode_Reward/action_rate: -0.0050
        Episode_Reward/stand_still: -0.0903
Metrics/target_pose/position_error: 4.3225
Metrics/target_pose/orientation_error: 1.5215
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 282624
                    Iteration time: 212.18s
                      Time elapsed: 08:45:44
                               ETA: 14:09:01

################################################################################
                      Learning iteration 184/3000

                       Computation: 7 steps/s (collection: 213.010s, learning 0.050s)
             Mean action noise std: 1.02
          Mean value_function loss: 97.2270
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 2.8752
                       Mean reward: -108.33
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0190
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2019
          Episode_Reward/collision: -1.3872
        Episode_Reward/action_rate: -0.0027
        Episode_Reward/stand_still: -0.0848
Metrics/target_pose/position_error: 4.4051
Metrics/target_pose/orientation_error: 0.9723
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 284160
                    Iteration time: 213.06s
                      Time elapsed: 08:49:17
                               ETA: 14:16:43

################################################################################
                      Learning iteration 185/3000

                       Computation: 7 steps/s (collection: 213.634s, learning 0.050s)
             Mean action noise std: 1.02
          Mean value_function loss: 170.3520
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 2.8741
                       Mean reward: -94.87
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0072
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1976
          Episode_Reward/collision: -2.5764
        Episode_Reward/action_rate: -0.0078
        Episode_Reward/stand_still: -0.0855
Metrics/target_pose/position_error: 4.8742
Metrics/target_pose/orientation_error: 1.3870
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 285696
                    Iteration time: 213.68s
                      Time elapsed: 08:52:51
                               ETA: 14:24:28

################################################################################
                      Learning iteration 186/3000

                       Computation: 7 steps/s (collection: 214.442s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value_function loss: 109.0399
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 2.8741
                       Mean reward: -89.95
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0175
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2030
          Episode_Reward/collision: -1.0868
        Episode_Reward/action_rate: -0.0067
        Episode_Reward/stand_still: -0.0877
Metrics/target_pose/position_error: 6.7983
Metrics/target_pose/orientation_error: 1.0962
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 287232
                    Iteration time: 214.50s
                      Time elapsed: 08:56:25
                               ETA: 14:32:17

################################################################################
                      Learning iteration 187/3000

                       Computation: 7 steps/s (collection: 215.364s, learning 0.048s)
             Mean action noise std: 1.02
          Mean value_function loss: 149.6219
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.8735
                       Mean reward: -84.02
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0032
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2018
          Episode_Reward/collision: -0.5451
        Episode_Reward/action_rate: -0.0040
        Episode_Reward/stand_still: -0.0909
Metrics/target_pose/position_error: 4.4165
Metrics/target_pose/orientation_error: 1.0579
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 288768
                    Iteration time: 215.41s
                      Time elapsed: 09:00:01
                               ETA: 14:40:13

################################################################################
                      Learning iteration 188/3000

                       Computation: 7 steps/s (collection: 215.903s, learning 0.051s)
             Mean action noise std: 1.02
          Mean value_function loss: 105.6016
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 2.8735
                       Mean reward: -82.75
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0048
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2015
          Episode_Reward/collision: -0.8247
        Episode_Reward/action_rate: -0.0105
        Episode_Reward/stand_still: -0.0889
Metrics/target_pose/position_error: 5.0647
Metrics/target_pose/orientation_error: 1.3695
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 290304
                    Iteration time: 215.95s
                      Time elapsed: 09:03:37
                               ETA: 14:48:09

################################################################################
                      Learning iteration 189/3000

                       Computation: 7 steps/s (collection: 216.736s, learning 0.059s)
             Mean action noise std: 1.02
          Mean value_function loss: 125.7905
               Mean surrogate loss: 0.0211
                 Mean entropy loss: 2.8736
                       Mean reward: -81.00
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0035
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1999
          Episode_Reward/collision: -0.8411
        Episode_Reward/action_rate: -0.0119
        Episode_Reward/stand_still: -0.0909
Metrics/target_pose/position_error: 3.8469
Metrics/target_pose/orientation_error: 0.6094
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 291840
                    Iteration time: 216.80s
                      Time elapsed: 09:07:14
                               ETA: 14:56:11

################################################################################
                      Learning iteration 190/3000

                       Computation: 7 steps/s (collection: 217.291s, learning 0.055s)
             Mean action noise std: 1.02
          Mean value_function loss: 81.2855
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 2.8732
                       Mean reward: -76.82
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0037
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2110
          Episode_Reward/collision: -1.7830
        Episode_Reward/action_rate: -0.0028
        Episode_Reward/stand_still: -0.0862
Metrics/target_pose/position_error: 5.4975
Metrics/target_pose/orientation_error: 1.3703
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 293376
                    Iteration time: 217.35s
                      Time elapsed: 09:10:51
                               ETA: 15:04:13

################################################################################
                      Learning iteration 191/3000

                       Computation: 7 steps/s (collection: 217.981s, learning 0.055s)
             Mean action noise std: 1.02
          Mean value_function loss: 137.4052
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 2.8732
                       Mean reward: -75.12
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0001
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1993
          Episode_Reward/collision: -1.2214
        Episode_Reward/action_rate: -0.0049
        Episode_Reward/stand_still: -0.0800
Metrics/target_pose/position_error: 3.3259
Metrics/target_pose/orientation_error: 1.1161
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 294912
                    Iteration time: 218.04s
                      Time elapsed: 09:14:29
                               ETA: 15:12:18

################################################################################
                      Learning iteration 192/3000

                       Computation: 7 steps/s (collection: 218.515s, learning 0.054s)
             Mean action noise std: 1.02
          Mean value_function loss: 187.5173
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 2.8730
                       Mean reward: -78.80
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0020
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2096
          Episode_Reward/collision: -4.3056
        Episode_Reward/action_rate: -0.0090
        Episode_Reward/stand_still: -0.0773
Metrics/target_pose/position_error: 5.3921
Metrics/target_pose/orientation_error: 0.7045
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 296448
                    Iteration time: 218.57s
                      Time elapsed: 09:18:08
                               ETA: 15:20:24

################################################################################
                      Learning iteration 193/3000

                       Computation: 7 steps/s (collection: 218.986s, learning 0.052s)
             Mean action noise std: 1.02
          Mean value_function loss: 125.8908
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 2.8730
                       Mean reward: -84.36
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0171
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2258
          Episode_Reward/collision: -5.5104
        Episode_Reward/action_rate: -0.0426
        Episode_Reward/stand_still: -0.0855
Metrics/target_pose/position_error: 2.2375
Metrics/target_pose/orientation_error: 0.9327
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 297984
                    Iteration time: 219.04s
                      Time elapsed: 09:21:47
                               ETA: 15:28:29

################################################################################
                      Learning iteration 194/3000

                       Computation: 6 steps/s (collection: 219.887s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value_function loss: 105.9311
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 2.8728
                       Mean reward: -72.47
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0051
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2034
          Episode_Reward/collision: -1.6884
        Episode_Reward/action_rate: -0.0222
        Episode_Reward/stand_still: -0.0784
Metrics/target_pose/position_error: 2.3672
Metrics/target_pose/orientation_error: 0.6119
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 299520
                    Iteration time: 219.94s
                      Time elapsed: 09:25:26
                               ETA: 15:36:40

################################################################################
                      Learning iteration 195/3000

                       Computation: 6 steps/s (collection: 220.340s, learning 0.054s)
             Mean action noise std: 1.02
          Mean value_function loss: 144.5467
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 2.8724
                       Mean reward: -70.04
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0074
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2049
          Episode_Reward/collision: -1.6797
        Episode_Reward/action_rate: -0.0252
        Episode_Reward/stand_still: -0.0828
Metrics/target_pose/position_error: 3.2989
Metrics/target_pose/orientation_error: 1.8389
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 301056
                    Iteration time: 220.39s
                      Time elapsed: 09:29:07
                               ETA: 15:44:50

################################################################################
                      Learning iteration 196/3000

                       Computation: 6 steps/s (collection: 220.958s, learning 0.061s)
             Mean action noise std: 1.02
          Mean value_function loss: 99.1128
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 2.8716
                       Mean reward: -71.34
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0066
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2040
          Episode_Reward/collision: -2.2283
        Episode_Reward/action_rate: -0.0104
        Episode_Reward/stand_still: -0.0833
Metrics/target_pose/position_error: 4.3231
Metrics/target_pose/orientation_error: 0.8066
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 302592
                    Iteration time: 221.02s
                      Time elapsed: 09:32:48
                               ETA: 15:53:02

################################################################################
                      Learning iteration 197/3000

                       Computation: 6 steps/s (collection: 221.459s, learning 0.058s)
             Mean action noise std: 1.02
          Mean value_function loss: 73.6938
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 2.8709
                       Mean reward: -69.19
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0065
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2017
          Episode_Reward/collision: -0.8585
        Episode_Reward/action_rate: -0.0067
        Episode_Reward/stand_still: -0.0884
Metrics/target_pose/position_error: 4.8917
Metrics/target_pose/orientation_error: 1.8870
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 304128
                    Iteration time: 221.52s
                      Time elapsed: 09:36:29
                               ETA: 16:01:14

################################################################################
                      Learning iteration 198/3000

                       Computation: 6 steps/s (collection: 222.195s, learning 0.058s)
             Mean action noise std: 1.02
          Mean value_function loss: 66.5542
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 2.8684
                       Mean reward: -73.79
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0062
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2004
          Episode_Reward/collision: -1.0434
        Episode_Reward/action_rate: -0.0045
        Episode_Reward/stand_still: -0.0864
Metrics/target_pose/position_error: 5.4205
Metrics/target_pose/orientation_error: 1.1461
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 305664
                    Iteration time: 222.25s
                      Time elapsed: 09:40:12
                               ETA: 16:09:29

################################################################################
                      Learning iteration 199/3000

                       Computation: 6 steps/s (collection: 222.864s, learning 0.057s)
             Mean action noise std: 1.02
          Mean value_function loss: 84.9180
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 2.8684
                       Mean reward: -74.39
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0156
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2029
          Episode_Reward/collision: -1.2865
        Episode_Reward/action_rate: -0.0032
        Episode_Reward/stand_still: -0.0841
Metrics/target_pose/position_error: 5.5561
Metrics/target_pose/orientation_error: 1.8477
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 307200
                    Iteration time: 222.92s
                      Time elapsed: 09:43:55
                               ETA: 16:17:46

################################################################################
                      Learning iteration 200/3000

                       Computation: 6 steps/s (collection: 223.532s, learning 0.056s)
             Mean action noise std: 1.02
          Mean value_function loss: 122.3830
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 2.8687
                       Mean reward: -79.56
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0042
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1930
          Episode_Reward/collision: -1.7266
        Episode_Reward/action_rate: -0.0476
        Episode_Reward/stand_still: -0.0858
Metrics/target_pose/position_error: 5.4904
Metrics/target_pose/orientation_error: 1.4824
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 308736
                    Iteration time: 223.59s
                      Time elapsed: 09:47:38
                               ETA: 16:26:05

################################################################################
                      Learning iteration 201/3000

                       Computation: 6 steps/s (collection: 224.523s, learning 0.058s)
             Mean action noise std: 1.01
          Mean value_function loss: 160.9875
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 2.8684
                       Mean reward: -75.61
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0064
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2016
          Episode_Reward/collision: -1.2309
        Episode_Reward/action_rate: -0.0028
        Episode_Reward/stand_still: -0.0851
Metrics/target_pose/position_error: 4.4691
Metrics/target_pose/orientation_error: 1.8967
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 310272
                    Iteration time: 224.58s
                      Time elapsed: 09:51:23
                               ETA: 16:34:31

################################################################################
                      Learning iteration 202/3000

                       Computation: 6 steps/s (collection: 224.554s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 184.2780
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 2.8669
                       Mean reward: -79.67
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0098
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2097
          Episode_Reward/collision: -2.2248
        Episode_Reward/action_rate: -0.0046
        Episode_Reward/stand_still: -0.0841
Metrics/target_pose/position_error: 4.2317
Metrics/target_pose/orientation_error: 1.6120
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 311808
                    Iteration time: 224.61s
                      Time elapsed: 09:55:07
                               ETA: 16:42:50

################################################################################
                      Learning iteration 203/3000

                       Computation: 6 steps/s (collection: 224.966s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 79.1388
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 2.8667
                       Mean reward: -86.76
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0014
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2898
          Episode_Reward/collision: -11.4479
        Episode_Reward/action_rate: -0.0034
        Episode_Reward/stand_still: -0.0947
Metrics/target_pose/position_error: 2.6293
Metrics/target_pose/orientation_error: 1.5549
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 313344
                    Iteration time: 225.02s
                      Time elapsed: 09:58:52
                               ETA: 16:51:08

################################################################################
                      Learning iteration 204/3000

                       Computation: 6 steps/s (collection: 225.364s, learning 0.053s)
             Mean action noise std: 1.01
          Mean value_function loss: 120.1869
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 2.8666
                       Mean reward: -80.09
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0029
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1992
          Episode_Reward/collision: -0.9731
        Episode_Reward/action_rate: -0.0142
        Episode_Reward/stand_still: -0.0864
Metrics/target_pose/position_error: 3.4619
Metrics/target_pose/orientation_error: 1.5446
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 314880
                    Iteration time: 225.42s
                      Time elapsed: 10:02:38
                               ETA: 16:59:24

################################################################################
                      Learning iteration 205/3000

                       Computation: 6 steps/s (collection: 226.027s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 171.2545
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 2.8661
                       Mean reward: -70.29
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0020
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2004
          Episode_Reward/collision: -1.2161
        Episode_Reward/action_rate: -0.0068
        Episode_Reward/stand_still: -0.0845
Metrics/target_pose/position_error: 4.0565
Metrics/target_pose/orientation_error: 2.5372
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 316416
                    Iteration time: 226.08s
                      Time elapsed: 10:06:24
                               ETA: 17:07:42

################################################################################
                      Learning iteration 206/3000

                       Computation: 6 steps/s (collection: 226.662s, learning 0.060s)
             Mean action noise std: 1.01
          Mean value_function loss: 168.4369
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 2.8659
                       Mean reward: -85.50
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0046
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2187
          Episode_Reward/collision: -5.1901
        Episode_Reward/action_rate: -0.0112
        Episode_Reward/stand_still: -0.0893
Metrics/target_pose/position_error: 3.8757
Metrics/target_pose/orientation_error: 1.7051
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 317952
                    Iteration time: 226.72s
                      Time elapsed: 10:10:11
                               ETA: 17:16:01

################################################################################
                      Learning iteration 207/3000

                       Computation: 6 steps/s (collection: 227.325s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 124.4555
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 2.8659
                       Mean reward: -86.00
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0044
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2243
          Episode_Reward/collision: -4.8359
        Episode_Reward/action_rate: -0.0166
        Episode_Reward/stand_still: -0.0874
Metrics/target_pose/position_error: 5.6413
Metrics/target_pose/orientation_error: 1.0231
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 319488
                    Iteration time: 227.38s
                      Time elapsed: 10:13:58
                               ETA: 17:24:23

################################################################################
                      Learning iteration 208/3000

                       Computation: 6 steps/s (collection: 227.735s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 133.9922
               Mean surrogate loss: 0.0172
                 Mean entropy loss: 2.8659
                       Mean reward: -81.98
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0026
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2013
          Episode_Reward/collision: -0.8273
        Episode_Reward/action_rate: -0.0362
        Episode_Reward/stand_still: -0.0902
Metrics/target_pose/position_error: 3.5783
Metrics/target_pose/orientation_error: 1.3760
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 321024
                    Iteration time: 227.79s
                      Time elapsed: 10:17:46
                               ETA: 17:32:42

################################################################################
                      Learning iteration 209/3000

                       Computation: 6 steps/s (collection: 228.363s, learning 0.059s)
             Mean action noise std: 1.01
          Mean value_function loss: 123.4456
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 2.8659
                       Mean reward: -92.87
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0016
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2211
          Episode_Reward/collision: -6.4957
        Episode_Reward/action_rate: -0.0259
        Episode_Reward/stand_still: -0.0934
Metrics/target_pose/position_error: 4.5036
Metrics/target_pose/orientation_error: 0.5516
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 322560
                    Iteration time: 228.42s
                      Time elapsed: 10:21:34
                               ETA: 17:41:04

################################################################################
                      Learning iteration 210/3000

                       Computation: 6 steps/s (collection: 229.042s, learning 0.056s)
             Mean action noise std: 1.01
          Mean value_function loss: 149.3275
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 2.8659
                       Mean reward: -90.27
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0039
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1985
          Episode_Reward/collision: -0.9245
        Episode_Reward/action_rate: -0.0056
        Episode_Reward/stand_still: -0.0896
Metrics/target_pose/position_error: 5.7189
Metrics/target_pose/orientation_error: 1.4624
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 324096
                    Iteration time: 229.10s
                      Time elapsed: 10:25:23
                               ETA: 17:49:27

################################################################################
                      Learning iteration 211/3000

                       Computation: 6 steps/s (collection: 229.469s, learning 0.054s)
             Mean action noise std: 1.01
          Mean value_function loss: 156.5874
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 2.8656
                       Mean reward: -92.30
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0013
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1950
          Episode_Reward/collision: -1.3533
        Episode_Reward/action_rate: -0.0115
        Episode_Reward/stand_still: -0.0897
Metrics/target_pose/position_error: 5.0646
Metrics/target_pose/orientation_error: 1.0942
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 325632
                    Iteration time: 229.52s
                      Time elapsed: 10:29:13
                               ETA: 17:57:49

################################################################################
                      Learning iteration 212/3000

                       Computation: 6 steps/s (collection: 230.396s, learning 0.060s)
             Mean action noise std: 1.01
          Mean value_function loss: 231.6102
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 2.8649
                       Mean reward: -91.79
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0009
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2403
          Episode_Reward/collision: -4.4444
        Episode_Reward/action_rate: -0.0353
        Episode_Reward/stand_still: -0.0935
Metrics/target_pose/position_error: 4.3898
Metrics/target_pose/orientation_error: 1.6166
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 327168
                    Iteration time: 230.46s
                      Time elapsed: 10:33:03
                               ETA: 18:06:17

################################################################################
                      Learning iteration 213/3000

                       Computation: 6 steps/s (collection: 230.681s, learning 0.057s)
             Mean action noise std: 1.01
          Mean value_function loss: 142.1556
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 2.8648
                       Mean reward: -96.39
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0016
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1857
          Episode_Reward/collision: -3.3394
        Episode_Reward/action_rate: -0.0208
        Episode_Reward/stand_still: -0.0922
Metrics/target_pose/position_error: 4.7943
Metrics/target_pose/orientation_error: 1.4038
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 328704
                    Iteration time: 230.74s
                      Time elapsed: 10:36:54
                               ETA: 18:14:41

################################################################################
                      Learning iteration 214/3000

                       Computation: 6 steps/s (collection: 231.054s, learning 0.057s)
             Mean action noise std: 1.01
          Mean value_function loss: 164.1736
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 2.8648
                       Mean reward: -95.85
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0020
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2007
          Episode_Reward/collision: -0.6233
        Episode_Reward/action_rate: -0.0031
        Episode_Reward/stand_still: -0.0914
Metrics/target_pose/position_error: 3.4913
Metrics/target_pose/orientation_error: 2.1275
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 330240
                    Iteration time: 231.11s
                      Time elapsed: 10:40:45
                               ETA: 18:23:03

################################################################################
                      Learning iteration 215/3000

                       Computation: 6 steps/s (collection: 231.705s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 156.2262
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 2.8647
                       Mean reward: -97.81
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0022
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2113
          Episode_Reward/collision: -3.5703
        Episode_Reward/action_rate: -0.0135
        Episode_Reward/stand_still: -0.0926
Metrics/target_pose/position_error: 4.6031
Metrics/target_pose/orientation_error: 1.9997
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 331776
                    Iteration time: 231.76s
                      Time elapsed: 10:44:37
                               ETA: 18:31:27

################################################################################
                      Learning iteration 216/3000

                       Computation: 6 steps/s (collection: 232.173s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 153.2163
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 2.8647
                       Mean reward: -94.48
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2039
          Episode_Reward/collision: -0.6597
        Episode_Reward/action_rate: -0.2700
        Episode_Reward/stand_still: -0.0922
Metrics/target_pose/position_error: 4.5001
Metrics/target_pose/orientation_error: 1.3795
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 333312
                    Iteration time: 232.23s
                      Time elapsed: 10:48:29
                               ETA: 18:39:50

################################################################################
                      Learning iteration 217/3000

                       Computation: 6 steps/s (collection: 232.724s, learning 0.059s)
             Mean action noise std: 1.01
          Mean value_function loss: 156.1182
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 2.8646
                       Mean reward: -91.67
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0055
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1944
          Episode_Reward/collision: -1.6797
        Episode_Reward/action_rate: -0.0432
        Episode_Reward/stand_still: -0.0941
Metrics/target_pose/position_error: 5.0195
Metrics/target_pose/orientation_error: 1.0832
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 334848
                    Iteration time: 232.78s
                      Time elapsed: 10:52:22
                               ETA: 18:48:14

################################################################################
                      Learning iteration 218/3000

                       Computation: 6 steps/s (collection: 233.386s, learning 0.056s)
             Mean action noise std: 1.01
          Mean value_function loss: 87.9645
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.8641
                       Mean reward: -87.64
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0021
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1761
          Episode_Reward/collision: -3.1302
        Episode_Reward/action_rate: -0.0180
        Episode_Reward/stand_still: -0.0944
Metrics/target_pose/position_error: 3.8040
Metrics/target_pose/orientation_error: 1.0266
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 336384
                    Iteration time: 233.44s
                      Time elapsed: 10:56:15
                               ETA: 18:56:39

################################################################################
                      Learning iteration 219/3000

                       Computation: 6 steps/s (collection: 234.661s, learning 0.051s)
             Mean action noise std: 1.01
          Mean value_function loss: 97.4445
               Mean surrogate loss: 0.0183
                 Mean entropy loss: 2.8630
                       Mean reward: -78.33
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0070
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2021
          Episode_Reward/collision: -0.4392
        Episode_Reward/action_rate: -0.0183
        Episode_Reward/stand_still: -0.0935
Metrics/target_pose/position_error: 4.0892
Metrics/target_pose/orientation_error: 2.1670
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 337920
                    Iteration time: 234.71s
                      Time elapsed: 11:00:10
                               ETA: 19:05:13

################################################################################
                      Learning iteration 220/3000

                       Computation: 6 steps/s (collection: 234.028s, learning 0.056s)
             Mean action noise std: 1.01
          Mean value_function loss: 71.8921
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 2.8632
                       Mean reward: -74.13
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0000
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2025
          Episode_Reward/collision: -0.4575
        Episode_Reward/action_rate: -0.0155
        Episode_Reward/stand_still: -0.0958
Metrics/target_pose/position_error: 6.7154
Metrics/target_pose/orientation_error: 1.0968
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 339456
                    Iteration time: 234.08s
                      Time elapsed: 11:04:04
                               ETA: 19:13:33

################################################################################
                      Learning iteration 221/3000

                       Computation: 6 steps/s (collection: 235.043s, learning 0.054s)
             Mean action noise std: 1.01
          Mean value_function loss: 77.2402
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 2.8633
                       Mean reward: -64.99
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0005
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2005
          Episode_Reward/collision: -0.5148
        Episode_Reward/action_rate: -0.0469
        Episode_Reward/stand_still: -0.0937
Metrics/target_pose/position_error: 4.1426
Metrics/target_pose/orientation_error: 1.3660
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 340992
                    Iteration time: 235.10s
                      Time elapsed: 11:07:59
                               ETA: 19:21:58

################################################################################
                      Learning iteration 222/3000

                       Computation: 6 steps/s (collection: 236.689s, learning 0.051s)
             Mean action noise std: 1.01
          Mean value_function loss: 69.3797
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 2.8633
                       Mean reward: -62.90
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0048
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1994
          Episode_Reward/collision: -0.6858
        Episode_Reward/action_rate: -0.0372
        Episode_Reward/stand_still: -0.0925
Metrics/target_pose/position_error: 3.6331
Metrics/target_pose/orientation_error: 0.7868
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 342528
                    Iteration time: 236.74s
                      Time elapsed: 11:11:56
                               ETA: 19:30:38

################################################################################
                      Learning iteration 223/3000

                       Computation: 6 steps/s (collection: 239.981s, learning 0.051s)
             Mean action noise std: 1.01
          Mean value_function loss: 90.6038
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 2.8635
                       Mean reward: -63.22
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0003
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1974
          Episode_Reward/collision: -1.0208
        Episode_Reward/action_rate: -0.0686
        Episode_Reward/stand_still: -0.0927
Metrics/target_pose/position_error: 4.7844
Metrics/target_pose/orientation_error: 2.2673
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 344064
                    Iteration time: 240.03s
                      Time elapsed: 11:15:56
                               ETA: 19:39:52

################################################################################
                      Learning iteration 224/3000

                       Computation: 6 steps/s (collection: 237.249s, learning 0.051s)
             Mean action noise std: 1.01
          Mean value_function loss: 139.9786
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.8635
                       Mean reward: -49.74
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0030
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1998
          Episode_Reward/collision: -0.8628
        Episode_Reward/action_rate: -0.0413
        Episode_Reward/stand_still: -0.0925
Metrics/target_pose/position_error: 5.5701
Metrics/target_pose/orientation_error: 1.6694
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 345600
                    Iteration time: 237.30s
                      Time elapsed: 11:19:53
                               ETA: 19:48:24

################################################################################
                      Learning iteration 225/3000

                       Computation: 6 steps/s (collection: 237.347s, learning 0.051s)
             Mean action noise std: 1.01
          Mean value_function loss: 64.0050
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 2.8634
                       Mean reward: -45.90
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0033
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1993
          Episode_Reward/collision: -0.5894
        Episode_Reward/action_rate: -0.0336
        Episode_Reward/stand_still: -0.0935
Metrics/target_pose/position_error: 5.3622
Metrics/target_pose/orientation_error: 1.3664
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 347136
                    Iteration time: 237.40s
                      Time elapsed: 11:23:51
                               ETA: 19:56:52

################################################################################
                      Learning iteration 226/3000

                       Computation: 6 steps/s (collection: 238.126s, learning 0.052s)
             Mean action noise std: 1.01
          Mean value_function loss: 92.9581
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 2.8632
                       Mean reward: -41.61
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0010
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2015
          Episode_Reward/collision: -0.4149
        Episode_Reward/action_rate: -0.1125
        Episode_Reward/stand_still: -0.0936
Metrics/target_pose/position_error: 4.6768
Metrics/target_pose/orientation_error: 1.1813
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 348672
                    Iteration time: 238.18s
                      Time elapsed: 11:27:49
                               ETA: 20:05:22

################################################################################
                      Learning iteration 227/3000

                       Computation: 6 steps/s (collection: 238.692s, learning 0.048s)
             Mean action noise std: 1.01
          Mean value_function loss: 80.1309
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 2.8631
                       Mean reward: -32.91
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0041
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2005
          Episode_Reward/collision: -0.5964
        Episode_Reward/action_rate: -0.0200
        Episode_Reward/stand_still: -0.0937
Metrics/target_pose/position_error: 3.2304
Metrics/target_pose/orientation_error: 0.8259
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 350208
                    Iteration time: 238.74s
                      Time elapsed: 11:31:48
                               ETA: 20:13:53

################################################################################
                      Learning iteration 228/3000

                       Computation: 6 steps/s (collection: 239.579s, learning 0.051s)
             Mean action noise std: 1.01
          Mean value_function loss: 84.5948
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 2.8629
                       Mean reward: -28.47
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: -0.0041
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1959
          Episode_Reward/collision: -1.2101
        Episode_Reward/action_rate: -0.0094
        Episode_Reward/stand_still: -0.0938
Metrics/target_pose/position_error: 4.5816
Metrics/target_pose/orientation_error: 1.9679
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 351744
                    Iteration time: 239.63s
                      Time elapsed: 11:35:47
                               ETA: 20:22:28

################################################################################
                      Learning iteration 229/3000

                       Computation: 6 steps/s (collection: 241.775s, learning 0.055s)
             Mean action noise std: 1.01
          Mean value_function loss: 92.8881
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 2.8626
                       Mean reward: -25.66
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0024
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2022
          Episode_Reward/collision: -0.7101
        Episode_Reward/action_rate: -0.0039
        Episode_Reward/stand_still: -0.0931
Metrics/target_pose/position_error: 5.9981
Metrics/target_pose/orientation_error: 0.6196
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 353280
                    Iteration time: 241.83s
                      Time elapsed: 11:39:49
                               ETA: 20:31:23
