             Mean action noise std: 1.03
          Mean value_function loss: 442.7757
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 2.8853
                       Mean reward: -431.69
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0093
         Episode_Reward/reach_goal: 0.0058
          Episode_Reward/face_goal: 0.1106
          Episode_Reward/collision: -14.4062
        Episode_Reward/action_rate: -0.0402
        Episode_Reward/stand_still: -0.0923
Metrics/target_pose/position_error: 4.2218
Metrics/target_pose/orientation_error: 0.3963
      Episode_Termination/time_out: 0.9779
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0221

---

                   Total timesteps: 28416
                    Iteration time: 76.89s
                      Time elapsed: 00:45:46
                               ETA: 16:30:10

################################################################################
                       Learning iteration 37/2000

                       Computation: 10 steps/s (collection: 76.452s, learning 0.049s)
             Mean action noise std: 1.03
          Mean value_function loss: 353.3964
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 2.8905
                       Mean reward: -432.11
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0103
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2605
          Episode_Reward/collision: -14.7882
        Episode_Reward/action_rate: -0.0227
        Episode_Reward/stand_still: -0.0964
Metrics/target_pose/position_error: 3.5525
Metrics/target_pose/orientation_error: 1.8394
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 29184
                    Iteration time: 76.50s
                      Time elapsed: 00:47:03
                               ETA: 16:30:52

################################################################################
                       Learning iteration 38/2000

                       Computation: 9 steps/s (collection: 76.840s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value_function loss: 285.0776
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 2.8882
                       Mean reward: -433.70
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0047
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1531
          Episode_Reward/collision: -14.8212
        Episode_Reward/action_rate: -0.5342
        Episode_Reward/stand_still: -0.0970
Metrics/target_pose/position_error: 3.5999
Metrics/target_pose/orientation_error: 1.0715
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 29952
                    Iteration time: 76.89s
                      Time elapsed: 00:48:20
                               ETA: 16:31:48

################################################################################
                       Learning iteration 39/2000

                       Computation: 9 steps/s (collection: 77.037s, learning 0.048s)
             Mean action noise std: 1.03
          Mean value_function loss: 363.4871
               Mean surrogate loss: 0.0132
                 Mean entropy loss: 2.8872
                       Mean reward: -432.37
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0308
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2781
          Episode_Reward/collision: -14.3385
        Episode_Reward/action_rate: -0.3199
        Episode_Reward/stand_still: -0.0939
Metrics/target_pose/position_error: 3.3469
Metrics/target_pose/orientation_error: 1.6591
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 30720
                    Iteration time: 77.09s
                      Time elapsed: 00:49:37
                               ETA: 16:32:47

################################################################################
                       Learning iteration 40/2000

                       Computation: 10 steps/s (collection: 76.561s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value_function loss: 302.4570
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 2.8860
                       Mean reward: -431.94
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0190
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2481
          Episode_Reward/collision: -13.6102
        Episode_Reward/action_rate: -0.0207
        Episode_Reward/stand_still: -0.0838
Metrics/target_pose/position_error: 2.8670
Metrics/target_pose/orientation_error: 2.1536
      Episode_Termination/time_out: 0.9753
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0247

---

                   Total timesteps: 31488
                    Iteration time: 76.61s
                      Time elapsed: 00:50:54
                               ETA: 16:33:17

################################################################################
                       Learning iteration 41/2000

                       Computation: 9 steps/s (collection: 76.926s, learning 0.054s)
             Mean action noise std: 1.03
          Mean value_function loss: 272.9548
               Mean surrogate loss: 0.0194
                 Mean entropy loss: 2.8904
                       Mean reward: -432.21
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2221
          Episode_Reward/collision: -14.5911
        Episode_Reward/action_rate: -0.0206
        Episode_Reward/stand_still: -0.0938
Metrics/target_pose/position_error: 3.5596
Metrics/target_pose/orientation_error: 1.9028
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 32256
                    Iteration time: 76.98s
                      Time elapsed: 00:52:11
                               ETA: 16:33:59

################################################################################
                       Learning iteration 42/2000

                       Computation: 9 steps/s (collection: 77.207s, learning 0.047s)
             Mean action noise std: 1.03
          Mean value_function loss: 316.4145
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 2.8938
                       Mean reward: -432.53
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0017
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1692
          Episode_Reward/collision: -14.8750
        Episode_Reward/action_rate: -0.0210
        Episode_Reward/stand_still: -0.0989
Metrics/target_pose/position_error: 3.2984
Metrics/target_pose/orientation_error: 1.1800
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 33024
                    Iteration time: 77.25s
                      Time elapsed: 00:53:28
                               ETA: 16:34:47

################################################################################
                       Learning iteration 43/2000

                       Computation: 9 steps/s (collection: 76.912s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 308.5093
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 2.8956
                       Mean reward: -431.13
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0016
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2816
          Episode_Reward/collision: -13.8785
        Episode_Reward/action_rate: -0.0219
        Episode_Reward/stand_still: -0.0860
Metrics/target_pose/position_error: 3.7765
Metrics/target_pose/orientation_error: 1.6979
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 33792
                    Iteration time: 76.96s
                      Time elapsed: 00:54:45
                               ETA: 16:35:17

################################################################################
                       Learning iteration 44/2000

                       Computation: 9 steps/s (collection: 77.125s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 286.0725
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 2.8983
                       Mean reward: -430.09
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0327
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2519
          Episode_Reward/collision: -14.1424
        Episode_Reward/action_rate: -0.0866
        Episode_Reward/stand_still: -0.0891
Metrics/target_pose/position_error: 3.0338
Metrics/target_pose/orientation_error: 1.6579
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 34560
                    Iteration time: 77.18s
                      Time elapsed: 00:56:02
                               ETA: 16:35:52

################################################################################
                       Learning iteration 45/2000

                       Computation: 9 steps/s (collection: 77.391s, learning 0.047s)
             Mean action noise std: 1.03
          Mean value_function loss: 316.8731
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 2.8988
                       Mean reward: -430.85
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2575
          Episode_Reward/collision: -14.7934
        Episode_Reward/action_rate: -0.1901
        Episode_Reward/stand_still: -0.0971
Metrics/target_pose/position_error: 2.4193
Metrics/target_pose/orientation_error: 1.3504
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 35328
                    Iteration time: 77.44s
                      Time elapsed: 00:57:19
                               ETA: 16:36:33

################################################################################
                       Learning iteration 46/2000

                       Computation: 9 steps/s (collection: 76.996s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value_function loss: 290.2897
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 2.8989
                       Mean reward: -430.46
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0138
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1209
          Episode_Reward/collision: -14.5521
        Episode_Reward/action_rate: -0.3896
        Episode_Reward/stand_still: -0.0955
Metrics/target_pose/position_error: 3.5037
Metrics/target_pose/orientation_error: 1.6162
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 36096
                    Iteration time: 77.05s
                      Time elapsed: 00:58:36
                               ETA: 16:36:52

################################################################################
                       Learning iteration 47/2000

                       Computation: 9 steps/s (collection: 77.527s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value_function loss: 270.2444
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 2.8995
                       Mean reward: -429.83
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0123
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2703
          Episode_Reward/collision: -14.5990
        Episode_Reward/action_rate: -0.2283
        Episode_Reward/stand_still: -0.0962
Metrics/target_pose/position_error: 1.4800
Metrics/target_pose/orientation_error: 1.6339
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 36864
                    Iteration time: 77.58s
                      Time elapsed: 00:59:54
                               ETA: 16:37:29

################################################################################
                       Learning iteration 48/2000

                       Computation: 9 steps/s (collection: 77.608s, learning 0.056s)
             Mean action noise std: 1.03
          Mean value_function loss: 876.7723
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 2.8992
                       Mean reward: -430.31
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0028
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2961
          Episode_Reward/collision: -14.5851
        Episode_Reward/action_rate: -0.2056
        Episode_Reward/stand_still: -0.0942
Metrics/target_pose/position_error: 3.4207
Metrics/target_pose/orientation_error: 1.7962
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 37632
                    Iteration time: 77.66s
                      Time elapsed: 01:01:12
                               ETA: 16:38:05

################################################################################
                       Learning iteration 49/2000

                       Computation: 9 steps/s (collection: 77.384s, learning 0.057s)
             Mean action noise std: 1.03
          Mean value_function loss: 269.6100
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 2.8992
                       Mean reward: -431.11
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0030
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2086
          Episode_Reward/collision: -14.3611
        Episode_Reward/action_rate: -0.8961
        Episode_Reward/stand_still: -0.0933
Metrics/target_pose/position_error: 3.4699
Metrics/target_pose/orientation_error: 1.3484
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 38400
                    Iteration time: 77.44s
                      Time elapsed: 01:02:29
                               ETA: 16:38:28

################################################################################
                       Learning iteration 50/2000

                       Computation: 9 steps/s (collection: 77.201s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value_function loss: 283.5342
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 2.9088
                       Mean reward: -430.41
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0013
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2203
          Episode_Reward/collision: -14.0417
        Episode_Reward/action_rate: -0.1109
        Episode_Reward/stand_still: -0.0881
Metrics/target_pose/position_error: 2.8033
Metrics/target_pose/orientation_error: 1.4163
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 39168
                    Iteration time: 77.26s
                      Time elapsed: 01:03:46
                               ETA: 16:38:39

################################################################################
                       Learning iteration 51/2000

                       Computation: 9 steps/s (collection: 77.644s, learning 0.046s)
             Mean action noise std: 1.04
          Mean value_function loss: 312.2449
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 2.9137
                       Mean reward: -430.13
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0023
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1481
          Episode_Reward/collision: -14.3038
        Episode_Reward/action_rate: -0.0317
        Episode_Reward/stand_still: -0.0930
Metrics/target_pose/position_error: 3.6022
Metrics/target_pose/orientation_error: 2.1667
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 39936
                    Iteration time: 77.69s
                      Time elapsed: 01:05:04
                               ETA: 16:39:04

################################################################################
                       Learning iteration 52/2000

                       Computation: 9 steps/s (collection: 77.786s, learning 0.057s)
             Mean action noise std: 1.04
          Mean value_function loss: 258.1939
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 2.9121
                       Mean reward: -434.69
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0030
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2880
          Episode_Reward/collision: -14.6094
        Episode_Reward/action_rate: -5.1509
        Episode_Reward/stand_still: -0.0943
Metrics/target_pose/position_error: 3.3125
Metrics/target_pose/orientation_error: 1.8635
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 40704
                    Iteration time: 77.84s
                      Time elapsed: 01:06:22
                               ETA: 16:39:30

################################################################################
                       Learning iteration 53/2000

                       Computation: 9 steps/s (collection: 77.319s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value_function loss: 238.0521
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.9118
                       Mean reward: -434.54
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0010
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1744
          Episode_Reward/collision: -14.3212
        Episode_Reward/action_rate: -0.0640
        Episode_Reward/stand_still: -0.0934
Metrics/target_pose/position_error: 3.3483
Metrics/target_pose/orientation_error: 1.8798
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 41472
                    Iteration time: 77.37s
                      Time elapsed: 01:07:39
                               ETA: 16:39:35

################################################################################
                       Learning iteration 54/2000

                       Computation: 9 steps/s (collection: 79.033s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value_function loss: 247.3395
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 2.9164
                       Mean reward: -433.17
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0122
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1270
          Episode_Reward/collision: -14.2830
        Episode_Reward/action_rate: -0.0242
        Episode_Reward/stand_still: -0.0919
Metrics/target_pose/position_error: 4.3544
Metrics/target_pose/orientation_error: 1.5556
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 42240
                    Iteration time: 79.08s
                      Time elapsed: 01:08:58
                               ETA: 16:40:38

################################################################################
                       Learning iteration 55/2000

                       Computation: 9 steps/s (collection: 78.250s, learning 0.051s)
             Mean action noise std: 1.04
          Mean value_function loss: 293.5269
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 2.9193
                       Mean reward: -432.19
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0209
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1572
          Episode_Reward/collision: -14.1111
        Episode_Reward/action_rate: -0.0362
        Episode_Reward/stand_still: -0.0873
Metrics/target_pose/position_error: 4.3616
Metrics/target_pose/orientation_error: 1.4414
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 43008
                    Iteration time: 78.30s
                      Time elapsed: 01:10:17
                               ETA: 16:41:09

################################################################################
                       Learning iteration 56/2000

                       Computation: 9 steps/s (collection: 77.626s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value_function loss: 266.6662
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 2.9191
                       Mean reward: -432.53
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0153
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1777
          Episode_Reward/collision: -14.6059
        Episode_Reward/action_rate: -0.0634
        Episode_Reward/stand_still: -0.0950
Metrics/target_pose/position_error: 3.3610
Metrics/target_pose/orientation_error: 1.2227
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 43776
                    Iteration time: 77.68s
                      Time elapsed: 01:11:34
                               ETA: 16:41:15

################################################################################
                       Learning iteration 57/2000

                       Computation: 9 steps/s (collection: 78.232s, learning 0.051s)
             Mean action noise std: 1.04
          Mean value_function loss: 233.7990
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 2.9170
                       Mean reward: -432.24
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0124
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1146
          Episode_Reward/collision: -14.2535
        Episode_Reward/action_rate: -0.0885
        Episode_Reward/stand_still: -0.0924
Metrics/target_pose/position_error: 3.7217
Metrics/target_pose/orientation_error: 1.0919
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 44544
                    Iteration time: 78.28s
                      Time elapsed: 01:12:53
                               ETA: 16:41:38

################################################################################
                       Learning iteration 58/2000

                       Computation: 9 steps/s (collection: 81.369s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value_function loss: 240.5725
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.9158
                       Mean reward: -432.09
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0497
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2058
          Episode_Reward/collision: -14.1910
        Episode_Reward/action_rate: -0.2137
        Episode_Reward/stand_still: -0.0917
Metrics/target_pose/position_error: 3.7716
Metrics/target_pose/orientation_error: 0.8334
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 45312
                    Iteration time: 81.42s
                      Time elapsed: 01:14:14
                               ETA: 16:43:41

             Mean action noise std: 1.05
          Mean value_function loss: 98759.4723
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 2.9109
                       Mean reward: -71968.82
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0731
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2276
          Episode_Reward/collision: -7.0156
        Episode_Reward/action_rate: -854.4441
        Episode_Reward/stand_still: -0.0607
Metrics/target_pose/position_error: 3.1662
Metrics/target_pose/orientation_error: 1.8875
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 277248
                    Iteration time: 115.23s
                      Time elapsed: 09:21:51
                               ETA: 18:32:31

################################################################################
                      Learning iteration 561/2200

                       Computation: 6 steps/s (collection: 115.302s, learning 0.049s)
             Mean action noise std: 1.04
          Mean value_function loss: 131.4551
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 2.9089
                       Mean reward: -69379.38
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0259
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.3386
          Episode_Reward/collision: -9.9097
        Episode_Reward/action_rate: -0.0243
        Episode_Reward/stand_still: -0.0741
Metrics/target_pose/position_error: 2.6076
Metrics/target_pose/orientation_error: 1.3298
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 278016
                    Iteration time: 115.35s
                      Time elapsed: 09:23:47
                               ETA: 18:32:37

##########################################################Total timesteps: 29952
                    Iteration time: 76.89s
                      Time elapsed: 00:48:20
                               ETA: 16:31:48

################################################################################
                       Learning iteration 39/2000

                       Computation: 9 steps/s (collection: 77.037s, learning 0.048s)
             Mean action noise std: 1.03
          Mean value_function loss: 363.4871
               Mean surrogate loss: 0.0132
                 Mean entropy loss: 2.8872
                       Mean reward: -432.37
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0308
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2781
          Episode_Reward/collision: -14.3385
        Episode_Reward/action_rate: -0.3199
        Episode_Reward/stand_still: -0.0939
Metrics/target_pose/position_error: 3.3469
Metrics/target_pose/orientation_error: 1.6591
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

                   Total timesteps: 30720
                    Iteration time: 77.09s
                      Time elapsed: 00:49:37
                               ETA: 16:32:47

################################################################################
                       Learning iteration 40/2000

                       Computation: 10 steps/s (collection: 76.561s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value_function loss: 302.4570
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 2.8860
                       Mean reward: -431.94
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0190
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2481
          Episode_Reward/collision: -13.6102
        Episode_Reward/action_rate: -0.0207
        Episode_Reward/stand_still: -0.0838
Metrics/target_pose/position_error: 2.8670
Metrics/target_pose/orientation_error: 2.1536
      Episode_Termination/time_out: 0.9753
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0247

                   Total timesteps: 31488
                    Iteration time: 76.61s
                      Time elapsed: 00:50:54
                               ETA: 16:33:17

################################################################################
                       Learning iteration 41/2000

                       Computation: 9 steps/s (collection: 76.926s, learning 0.054s)
             Mean action noise std: 1.03
          Mean value_function loss: 272.9548
               Mean surrogate loss: 0.0194
                 Mean entropy loss: 2.8904
                       Mean reward: -432.21
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0002
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2221
          Episode_Reward/collision: -14.5911
        Episode_Reward/action_rate: -0.0206
        Episode_Reward/stand_still: -0.0938
Metrics/target_pose/position_error: 3.5596
Metrics/target_pose/orientation_error: 1.9028
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 32256
                    Iteration time: 76.98s
                      Time elapsed: 00:52:11
                               ETA: 16:33:59

################################################################################
                       Learning iteration 42/2000

                       Computation: 9 steps/s (collection: 77.207s, learning 0.047s)
             Mean action noise std: 1.03
          Mean value_function loss: 316.4145
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 2.8938
                       Mean reward: -432.53
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0017
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1692
          Episode_Reward/collision: -14.8750
        Episode_Reward/action_rate: -0.0210
        Episode_Reward/stand_still: -0.0989
Metrics/target_pose/position_error: 3.2984
Metrics/target_pose/orientation_error: 1.1800
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 33024
                    Iteration time: 77.25s
                      Time elapsed: 00:53:28
                               ETA: 16:34:47

################################################################################
                       Learning iteration 43/2000

                       Computation: 9 steps/s (collection: 76.912s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 308.5093
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 2.8956
                       Mean reward: -431.13
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0016
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2816
          Episode_Reward/collision: -13.8785
        Episode_Reward/action_rate: -0.0219
        Episode_Reward/stand_still: -0.0860
Metrics/target_pose/position_error: 3.7765
Metrics/target_pose/orientation_error: 1.6979
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 33792
                    Iteration time: 76.96s
                      Time elapsed: 00:54:45
                               ETA: 16:35:17

################################################################################
                       Learning iteration 44/2000

                       Computation: 9 steps/s (collection: 77.125s, learning 0.052s)
             Mean action noise std: 1.03
          Mean value_function loss: 286.0725
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 2.8983
                       Mean reward: -430.09
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0327
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2519
          Episode_Reward/collision: -14.1424
        Episode_Reward/action_rate: -0.0866
        Episode_Reward/stand_still: -0.0891
Metrics/target_pose/position_error: 3.0338
Metrics/target_pose/orientation_error: 1.6579
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 34560
                    Iteration time: 77.18s
                      Time elapsed: 00:56:02
                               ETA: 16:35:52

################################################################################
                       Learning iteration 45/2000

                       Computation: 9 steps/s (collection: 77.391s, learning 0.047s)
             Mean action noise std: 1.03
          Mean value_function loss: 316.8731
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 2.8988
                       Mean reward: -430.85
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0007
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2575
          Episode_Reward/collision: -14.7934
        Episode_Reward/action_rate: -0.1901
        Episode_Reward/stand_still: -0.0971
Metrics/target_pose/position_error: 2.4193
Metrics/target_pose/orientation_error: 1.3504
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 35328
                    Iteration time: 77.44s
                      Time elapsed: 00:57:19
                               ETA: 16:36:33

################################################################################
                       Learning iteration 46/2000

                       Computation: 9 steps/s (collection: 76.996s, learning 0.051s)
             Mean action noise std: 1.03
          Mean value_function loss: 290.2897
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 2.8989
                       Mean reward: -430.46
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0138
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1209
          Episode_Reward/collision: -14.5521
        Episode_Reward/action_rate: -0.3896
        Episode_Reward/stand_still: -0.0955
Metrics/target_pose/position_error: 3.5037
Metrics/target_pose/orientation_error: 1.6162
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 36096
                    Iteration time: 77.05s
                      Time elapsed: 00:58:36
                               ETA: 16:36:52

################################################################################
                       Learning iteration 47/2000

                       Computation: 9 steps/s (collection: 77.527s, learning 0.053s)
             Mean action noise std: 1.03
          Mean value_function loss: 270.2444
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 2.8995
                       Mean reward: -429.83
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0123
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2703
          Episode_Reward/collision: -14.5990
        Episode_Reward/action_rate: -0.2283
        Episode_Reward/stand_still: -0.0962
Metrics/target_pose/position_error: 1.4800
Metrics/target_pose/orientation_error: 1.6339
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 36864
                    Iteration time: 77.58s
                      Time elapsed: 00:59:54
                               ETA: 16:37:29

################################################################################
                       Learning iteration 48/2000

                       Computation: 9 steps/s (collection: 77.608s, learning 0.056s)
             Mean action noise std: 1.03
          Mean value_function loss: 876.7723
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 2.8992
                       Mean reward: -430.31
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0028
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2961
          Episode_Reward/collision: -14.5851
        Episode_Reward/action_rate: -0.2056
        Episode_Reward/stand_still: -0.0942
Metrics/target_pose/position_error: 3.4207
Metrics/target_pose/orientation_error: 1.7962
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 37632
                    Iteration time: 77.66s
                      Time elapsed: 01:01:12
                               ETA: 16:38:05

################################################################################
                       Learning iteration 49/2000

                       Computation: 9 steps/s (collection: 77.384s, learning 0.057s)
             Mean action noise std: 1.03
          Mean value_function loss: 269.6100
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 2.8992
                       Mean reward: -431.11
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0030
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2086
          Episode_Reward/collision: -14.3611
        Episode_Reward/action_rate: -0.8961
        Episode_Reward/stand_still: -0.0933
Metrics/target_pose/position_error: 3.4699
Metrics/target_pose/orientation_error: 1.3484
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 38400
                    Iteration time: 77.44s
                      Time elapsed: 01:02:29
                               ETA: 16:38:28

################################################################################
                       Learning iteration 50/2000

                       Computation: 9 steps/s (collection: 77.201s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value_function loss: 283.5342
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 2.9088
                       Mean reward: -430.41
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0013
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2203
          Episode_Reward/collision: -14.0417
        Episode_Reward/action_rate: -0.1109
        Episode_Reward/stand_still: -0.0881
Metrics/target_pose/position_error: 2.8033
Metrics/target_pose/orientation_error: 1.4163
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 39168
                    Iteration time: 77.26s
                      Time elapsed: 01:03:46
                               ETA: 16:38:39

################################################################################
                       Learning iteration 51/2000

                       Computation: 9 steps/s (collection: 77.644s, learning 0.046s)
             Mean action noise std: 1.04
          Mean value_function loss: 312.2449
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 2.9137
                       Mean reward: -430.13
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0023
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1481
          Episode_Reward/collision: -14.3038
        Episode_Reward/action_rate: -0.0317
        Episode_Reward/stand_still: -0.0930
Metrics/target_pose/position_error: 3.6022
Metrics/target_pose/orientation_error: 2.1667
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 39936
                    Iteration time: 77.69s
                      Time elapsed: 01:05:04
                               ETA: 16:39:04

################################################################################
                       Learning iteration 52/2000

                       Computation: 9 steps/s (collection: 77.786s, learning 0.057s)
             Mean action noise std: 1.04
          Mean value_function loss: 258.1939
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 2.9121
                       Mean reward: -434.69
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0030
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2880
          Episode_Reward/collision: -14.6094
        Episode_Reward/action_rate: -5.1509
        Episode_Reward/stand_still: -0.0943
Metrics/target_pose/position_error: 3.3125
Metrics/target_pose/orientation_error: 1.8635
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 40704
                    Iteration time: 77.84s
                      Time elapsed: 01:06:22
                               ETA: 16:39:30

################################################################################
                       Learning iteration 53/2000

                       Computation: 9 steps/s (collection: 77.319s, learning 0.055s)
             Mean action noise std: 1.04
          Mean value_function loss: 238.0521
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 2.9118
                       Mean reward: -434.54
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0010
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1744
          Episode_Reward/collision: -14.3212
        Episode_Reward/action_rate: -0.0640
        Episode_Reward/stand_still: -0.0934
Metrics/target_pose/position_error: 3.3483
Metrics/target_pose/orientation_error: 1.8798
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 41472
                    Iteration time: 77.37s
                      Time elapsed: 01:07:39
                               ETA: 16:39:35

################################################################################
                       Learning iteration 54/2000

                       Computation: 9 steps/s (collection: 79.033s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value_function loss: 247.3395
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 2.9164
                       Mean reward: -433.17
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: 0.0122
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1270
          Episode_Reward/collision: -14.2830
        Episode_Reward/action_rate: -0.0242
        Episode_Reward/stand_still: -0.0919
Metrics/target_pose/position_error: 4.3544
Metrics/target_pose/orientation_error: 1.5556
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 42240
                    Iteration time: 79.08s
                      Time elapsed: 01:08:58
                               ETA: 16:40:38

################################################################################
                       Learning iteration 55/2000

                       Computation: 9 steps/s (collection: 78.250s, learning 0.051s)
             Mean action noise std: 1.04
          Mean value_function loss: 293.5269
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 2.9193
                       Mean reward: -432.19
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0209
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1572
          Episode_Reward/collision: -14.1111
        Episode_Reward/action_rate: -0.0362
        Episode_Reward/stand_still: -0.0873
Metrics/target_pose/position_error: 4.3616
Metrics/target_pose/orientation_error: 1.4414
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 43008
                    Iteration time: 78.30s
                      Time elapsed: 01:10:17
                               ETA: 16:41:09

################################################################################
                       Learning iteration 56/2000

                       Computation: 9 steps/s (collection: 77.626s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value_function loss: 266.6662
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 2.9191
                       Mean reward: -432.53
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0153
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1777
          Episode_Reward/collision: -14.6059
        Episode_Reward/action_rate: -0.0634
        Episode_Reward/stand_still: -0.0950
Metrics/target_pose/position_error: 3.3610
Metrics/target_pose/orientation_error: 1.2227
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 43776
                    Iteration time: 77.68s
                      Time elapsed: 01:11:34
                               ETA: 16:41:15

################################################################################
                       Learning iteration 57/2000

                       Computation: 9 steps/s (collection: 78.232s, learning 0.051s)
             Mean action noise std: 1.04
          Mean value_function loss: 233.7990
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 2.9170
                       Mean reward: -432.24
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0124
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1146
          Episode_Reward/collision: -14.2535
        Episode_Reward/action_rate: -0.0885
        Episode_Reward/stand_still: -0.0924
Metrics/target_pose/position_error: 3.7217
Metrics/target_pose/orientation_error: 1.0919
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 44544
                    Iteration time: 78.28s
                      Time elapsed: 01:12:53
                               ETA: 16:41:38

################################################################################
                       Learning iteration 58/2000

                       Computation: 9 steps/s (collection: 81.369s, learning 0.052s)
             Mean action noise std: 1.04
          Mean value_function loss: 240.5725
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 2.9158
                       Mean reward: -432.09
               Mean episode length: 179.63
   Episode_Reward/progress_to_goal: -0.0497
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2058
          Episode_Reward/collision: -14.1910
        Episode_Reward/action_rate: -0.2137
        Episode_Reward/stand_still: -0.0917
Metrics/target_pose/position_error: 3.7716
Metrics/target_pose/orientation_error: 0.8334
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 45312
                    Iteration time: 81.42s
                      Time elapsed: 01:14:14
                               ETA: 16:43:41

             Mean action noise std: 1.05
          Mean value_function loss: 98759.4723
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 2.9109
                       Mean reward: -71968.82
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0731
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2276
          Episode_Reward/collision: -7.0156
        Episode_Reward/action_rate: -854.4441
        Episode_Reward/stand_still: -0.0607
Metrics/target_pose/position_error: 3.1662
Metrics/target_pose/orientation_error: 1.8875
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

                   Total timesteps: 277248
                    Iteration time: 115.23s
                      Time elapsed: 09:21:51
                               ETA: 18:32:31

################################################################################
                      Learning iteration 561/2200

                       Computation: 6 steps/s (collection: 115.302s, learning 0.049s)
             Mean action noise std: 1.04
          Mean value_function loss: 131.4551
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 2.9089
                       Mean reward: -69379.38
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0259
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.3386
          Episode_Reward/collision: -9.9097
        Episode_Reward/action_rate: -0.0243
        Episode_Reward/stand_still: -0.0741
Metrics/target_pose/position_error: 2.6076
Metrics/target_pose/orientation_error: 1.3298
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000
######################
                      Learning iteration 562/2200

                       Computation: 6 steps/s (collection: 115.251s, learning 0.049s)
             Mean action noise std: 1.04
          Mean value_function loss: 111536505.6407
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 2.8955
                       Mean reward: -68929.99
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0979
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1931
          Episode_Reward/collision: -4.9080
        Episode_Reward/action_rate: -2694.3447
        Episode_Reward/stand_still: -0.0527
Metrics/target_pose/position_error: 2.8194
Metrics/target_pose/orientation_error: 2.5137
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 278784
                    Iteration time: 115.30s
                      Time elapsed: 09:25:42
                               ETA: 18:32:42

################################################################################
                      Learning iteration 563/2200

                       Computation: 6 steps/s (collection: 115.452s, learning 0.048s)
             Mean action noise std: 1.04
          Mean value_function loss: 620.7846
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 2.8939
                       Mean reward: -68902.04
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0613
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2879
          Episode_Reward/collision: -5.8229
        Episode_Reward/action_rate: -3.9095
        Episode_Reward/stand_still: -0.0688
Metrics/target_pose/position_error: 3.1278
Metrics/target_pose/orientation_error: 1.3813
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 279552
                    Iteration time: 115.50s
                      Time elapsed: 09:27:38
                               ETA: 18:32:47

################################################################################
                      Learning iteration 564/2200

                       Computation: 6 steps/s (collection: 115.679s, learning 0.050s)
             Mean action noise std: 1.04
          Mean value_function loss: 246894058.7559
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 2.8940
                       Mean reward: -68980.71
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0579
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1833
          Episode_Reward/collision: -6.0069
        Episode_Reward/action_rate: -56.1951
        Episode_Reward/stand_still: -0.0549
Metrics/target_pose/position_error: 3.8259
Metrics/target_pose/orientation_error: 2.0368
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 280320
                    Iteration time: 115.73s
                      Time elapsed: 09:29:33
                               ETA: 18:32:53
