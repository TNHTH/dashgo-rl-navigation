             Mean action noise std: 0.73
          Mean value_function loss: 123.7833
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 1.9864
                       Mean reward: -108.97
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.2236
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2212
          Episode_Reward/collision: -1.2465
        Episode_Reward/action_rate: -0.0027
        Episode_Reward/stand_still: -0.0007
Metrics/target_pose/position_error: 4.1855
Metrics/target_pose/orientation_error: 1.2664
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 27648
                    Iteration time: 78.17s
                      Time elapsed: 00:45:11
                               ETA: 17:06:19

################################################################################
                      Learning iteration 636/2600

                       Computation: 9 steps/s (collection: 78.303s, learning 0.048s)
             Mean action noise std: 0.73
          Mean value_function loss: 154.7996
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 1.9841
                       Mean reward: -105.94
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0367
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1894
          Episode_Reward/collision: -5.2743
        Episode_Reward/action_rate: -0.0058
        Episode_Reward/stand_still: -0.0277
Metrics/target_pose/position_error: 5.5947
Metrics/target_pose/orientation_error: 1.6078
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 28416
                    Iteration time: 78.35s
                      Time elapsed: 00:46:29
                               ETA: 17:07:45

################################################################################
                      Learning iteration 637/2600

                       Computation: 9 steps/s (collection: 78.456s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 142.7066
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 1.9832
                       Mean reward: -102.53
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.1187
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1914
          Episode_Reward/collision: -6.3299
        Episode_Reward/action_rate: -0.0030
        Episode_Reward/stand_still: -0.0321
Metrics/target_pose/position_error: 4.7370
Metrics/target_pose/orientation_error: 1.0377
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 29184
                    Iteration time: 78.51s
                      Time elapsed: 00:47:47
                               ETA: 17:09:11

################################################################################
                      Learning iteration 638/2600

                       Computation: 9 steps/s (collection: 78.705s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 118.9003
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 1.9820
                       Mean reward: -106.25
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.1020
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1847
          Episode_Reward/collision: -4.6389
        Episode_Reward/action_rate: -0.0024
        Episode_Reward/stand_still: -0.0238
Metrics/target_pose/position_error: 3.5090
Metrics/target_pose/orientation_error: 1.1087
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 29952
                    Iteration time: 78.76s
                      Time elapsed: 00:49:06
                               ETA: 17:10:41

################################################################################
                      Learning iteration 639/2600

                       Computation: 9 steps/s (collection: 78.750s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 101.9048
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 1.9814
                       Mean reward: -110.60
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.0478
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1615
          Episode_Reward/collision: -7.1910
        Episode_Reward/action_rate: -0.0311
        Episode_Reward/stand_still: -0.0444
Metrics/target_pose/position_error: 5.4313
Metrics/target_pose/orientation_error: 1.8377
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 30720
                    Iteration time: 78.80s
                      Time elapsed: 00:50:25
                               ETA: 17:12:04

################################################################################
                      Learning iteration 640/2600

                       Computation: 9 steps/s (collection: 78.784s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 91.8082
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 1.9777
                       Mean reward: -110.31
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.1060
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2108
          Episode_Reward/collision: -1.0260
        Episode_Reward/action_rate: -0.0146
        Episode_Reward/stand_still: -0.0010
Metrics/target_pose/position_error: 3.1936
Metrics/target_pose/orientation_error: 2.0203
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 31488
                    Iteration time: 78.83s
                      Time elapsed: 00:51:44
                               ETA: 17:13:22

################################################################################
                      Learning iteration 641/2600

                       Computation: 9 steps/s (collection: 79.212s, learning 0.045s)
             Mean action noise std: 0.73
          Mean value_function loss: 140.7682
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 1.9733
                       Mean reward: -115.32
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.1921
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2200
          Episode_Reward/collision: -2.2083
        Episode_Reward/action_rate: -0.0051
        Episode_Reward/stand_still: -0.0023
Metrics/target_pose/position_error: 3.1255
Metrics/target_pose/orientation_error: 0.4553
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 32256
                    Iteration time: 79.26s
                      Time elapsed: 00:53:03
                               ETA: 17:14:51

################################################################################
                      Learning iteration 642/2600

                       Computation: 9 steps/s (collection: 78.842s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 77.2737
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 1.9722
                       Mean reward: -115.58
               Mean episode length: 180.00
   Episode_Reward/progress_to_goal: 0.1297
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2093
          Episode_Reward/collision: -1.5191
        Episode_Reward/action_rate: -0.0040
        Episode_Reward/stand_still: -0.0008
Metrics/target_pose/position_error: 5.5593
Metrics/target_pose/orientation_error: 1.4077
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 33024
                    Iteration time: 78.89s
                      Time elapsed: 00:54:22
                               ETA: 17:15:56

################################################################################
                      Learning iteration 643/2600

                       Computation: 9 steps/s (collection: 79.195s, learning 0.048s)
             Mean action noise std: 0.73
          Mean value_function loss: 202.1717
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 1.9718
                       Mean reward: -111.34
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.1308
         Episode_Reward/reach_goal: 0.0116
          Episode_Reward/face_goal: 0.2552
          Episode_Reward/collision: -4.6545
        Episode_Reward/action_rate: -0.0670
        Episode_Reward/stand_still: -0.0274
Metrics/target_pose/position_error: 4.9230
Metrics/target_pose/orientation_error: 1.4169
      Episode_Termination/time_out: 0.9766
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0234

---

                   Total timesteps: 33792
                    Iteration time: 79.24s
                      Time elapsed: 00:55:41
                               ETA: 17:17:10

################################################################################
                      Learning iteration 644/2600

                       Computation: 9 steps/s (collection: 79.480s, learning 0.046s)
             Mean action noise std: 0.73
          Mean value_function loss: 98.4862
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 1.9715
                       Mean reward: -111.37
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.1238
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2029
          Episode_Reward/collision: -3.1701
        Episode_Reward/action_rate: -0.0183
        Episode_Reward/stand_still: -0.0155
Metrics/target_pose/position_error: 4.2355
Metrics/target_pose/orientation_error: 1.6402
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 34560
                    Iteration time: 79.53s
                      Time elapsed: 00:57:01
                               ETA: 17:18:30

################################################################################
                      Learning iteration 645/2600

                       Computation: 9 steps/s (collection: 79.616s, learning 0.047s)
             Mean action noise std: 0.73
          Mean value_function loss: 124.9114
               Mean surrogate loss: 0.0157
                 Mean entropy loss: 1.9710
                       Mean reward: -111.40
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.3714
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2328
          Episode_Reward/collision: -2.1771
        Episode_Reward/action_rate: -0.0668
        Episode_Reward/stand_still: -0.0009
Metrics/target_pose/position_error: 6.0231
Metrics/target_pose/orientation_error: 2.2086
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 35328
                    Iteration time: 79.66s
                      Time elapsed: 00:58:20
                               ETA: 17:19:48

################################################################################
                      Learning iteration 646/2600

                       Computation: 9 steps/s (collection: 79.731s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 107.8351
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 1.9714
                       Mean reward: -110.94
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.2847
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2251
          Episode_Reward/collision: -1.1667
        Episode_Reward/action_rate: -0.1048
        Episode_Reward/stand_still: -0.0005
Metrics/target_pose/position_error: 4.6352
Metrics/target_pose/orientation_error: 0.9149
      Episode_Termination/time_out: 0.9375
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0625

---

                   Total timesteps: 36096
                    Iteration time: 79.78s
                      Time elapsed: 00:59:40
                               ETA: 17:21:05

################################################################################
                      Learning iteration 647/2600

                       Computation: 9 steps/s (collection: 79.889s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 154.9791
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 1.9745
                       Mean reward: -123.72
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.1202
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2099
          Episode_Reward/collision: -3.1684
        Episode_Reward/action_rate: -0.1268
        Episode_Reward/stand_still: -0.0170
Metrics/target_pose/position_error: 4.6823
Metrics/target_pose/orientation_error: 0.6431
      Episode_Termination/time_out: 0.9766
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0234

---

                   Total timesteps: 36864
                    Iteration time: 79.94s
                      Time elapsed: 01:01:00
                               ETA: 17:22:22

################################################################################
                      Learning iteration 648/2600

                       Computation: 9 steps/s (collection: 80.033s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 79.5865
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 1.9748
                       Mean reward: -128.31
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0724
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.1274
          Episode_Reward/collision: -10.9844
        Episode_Reward/action_rate: -0.0014
        Episode_Reward/stand_still: -0.0710
Metrics/target_pose/position_error: 4.0275
Metrics/target_pose/orientation_error: 1.7334
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 37632
                    Iteration time: 80.09s
                      Time elapsed: 01:02:20
                               ETA: 17:23:38

################################################################################
                      Learning iteration 649/2600

                       Computation: 9 steps/s (collection: 80.195s, learning 0.047s)
             Mean action noise std: 0.73
          Mean value_function loss: 23.3531
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 1.9702
                       Mean reward: -128.17
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.1900
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2160
          Episode_Reward/collision: -0.8681
        Episode_Reward/action_rate: -0.0015
        Episode_Reward/stand_still: -0.0010
Metrics/target_pose/position_error: 4.2357
Metrics/target_pose/orientation_error: 2.2706
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 38400
                    Iteration time: 80.24s
                      Time elapsed: 01:03:40
                               ETA: 17:24:54

################################################################################
                      Learning iteration 650/2600

                       Computation: 9 steps/s (collection: 80.256s, learning 0.046s)
             Mean action noise std: 0.73
          Mean value_function loss: 81.7246
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 1.9682
                       Mean reward: -123.37
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.0301
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2054
          Episode_Reward/collision: -9.9931
        Episode_Reward/action_rate: -0.0050
        Episode_Reward/stand_still: -0.0647
Metrics/target_pose/position_error: 4.6143
Metrics/target_pose/orientation_error: 1.8075
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 39168
                    Iteration time: 80.30s
                      Time elapsed: 01:05:01
                               ETA: 17:26:06

################################################################################
                      Learning iteration 651/2600

                       Computation: 9 steps/s (collection: 80.629s, learning 0.049s)
             Mean action noise std: 0.73
          Mean value_function loss: 126.0588
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 1.9676
                       Mean reward: -110.05
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.1212
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2126
          Episode_Reward/collision: -0.8941
        Episode_Reward/action_rate: -0.0194
        Episode_Reward/stand_still: -0.0004
Metrics/target_pose/position_error: 4.8213
Metrics/target_pose/orientation_error: 0.7565
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 39936
                    Iteration time: 80.68s
                      Time elapsed: 01:06:21
                               ETA: 17:27:26

################################################################################
                      Learning iteration 652/2600

                       Computation: 9 steps/s (collection: 80.766s, learning 0.048s)
             Mean action noise std: 0.73
          Mean value_function loss: 114.8386
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 1.9651
                       Mean reward: -109.91
               Mean episode length: 178.21
   Episode_Reward/progress_to_goal: 0.1389
         Episode_Reward/reach_goal: 0.0000
          Episode_Reward/face_goal: 0.2141
          Episode_Reward/collision: -1.4392
        Episode_Reward/action_rate: -0.0017
        Episode_Reward/stand_still: -0.0013
Metrics/target_pose/position_error: 3.8583
Metrics/target_pose/orientation_error: 1.4291
      Episode_Termination/time_out: 1.0000
   Episode_Termination/base_height: 0.0000
    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 40704
                    Iteration time: 80.81s
                      Time elapsed: 01:07:42
                               ETA: 17:28:45
