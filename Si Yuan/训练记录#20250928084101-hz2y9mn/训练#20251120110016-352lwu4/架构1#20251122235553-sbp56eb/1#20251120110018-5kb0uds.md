 Learning iteration 1482/2000

                       Computation: 27 steps/s (collection: 27.411s, learning 0.052s)

             Mean action noise std: 0.81

          Mean value_function loss: 2933.6208

               Mean surrogate loss: 0.0347

                 Mean entropy loss: 2.4147

                       Mean reward: -706.45

               Mean episode length: 180.00

   Episode_Reward/progress_to_goal: 0.0009

         Episode_Reward/reach_goal: 0.0000

          Episode_Reward/face_goal: 0.1941

          Episode_Reward/collision: -3.2813

        Episode_Reward/action_rate: -15.1244

Metrics/target_pose/position_error: 3.2307

Metrics/target_pose/orientation_error: 1.1666

      Episode_Termination/time_out: 1.0000

   Episode_Termination/base_height: 0.0000

    Episode_Termination/reach_goal: 0.0000

---

                   Total timesteps: 1138944

                    Iteration time: 27.46s

                      Time elapsed: 08:32:01

  ETA: 02:58:50
