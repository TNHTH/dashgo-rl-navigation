根据机器学习的发展脉络，我为你整理了一份从经典到前沿的主流算法清单。这些算法构成了当今人工智能大厦的基石：

### 1. 经典统计学习算法 (Classical Machine Learning)

这些算法通常处理**结构化数据**（如表格数据），在工业界的搜索、推荐和风险控制中依然占据核心地位。

* **线性回归与逻辑回归 (Linear & Logistic Regression):**  基础中的基础，用于预测数值或分类。
* **支持向量机 (SVM):**  通过寻找“最大间隔超平面”进行分类，在小样本高维数据上表现极佳。
* **决策树 (Decision Tree):**  如 ID3, C4.5, CART。易于理解，是集成学习的基础。
* **随机森林 (Random Forest):**  典型的 **Bagging** 算法，通过集成多棵决策树来提高稳定性和准确性。
* **梯度提升决策树 (GBDT/XGBoost/LightGBM):**  典型的 **Boosting** 算法。目前在数据竞赛和金融风控领域，这类模型（尤其是 XGBoost）被公认为处理结构化数据的最强模型。

---

### 2. 经典深度学习架构 (Deep Learning)

深度学习引入了多层神经网络，解决了**非结构化数据**（图像、语音、文本）的特征提取问题。

* **卷积神经网络 (CNN):**  专门用于处理空间信息。

  * **代表作：**  AlexNet, VGG, ResNet（残差网络，解决了深层网络难以训练的问题）。
* **循环神经网络 (RNN) & LSTM:**  专门用于处理序列数据。

  * **代表作：**  LSTM (长短期记忆网络)，曾是机器翻译和语音识别的核心。
* **生成对抗网络 (GAN):**  让两个神经网络（生成器和判别器）互相博弈，生成逼真的图像或音频。

---

### 3. Transformer 与大模型时代 (The Transformer Era)

自 2017 年起，深度学习进入了“大一统”时代，Transformer 架构几乎取代了 RNN。

* **Transformer:**  核心是 **Self-Attention (自注意力机制)** ，允许模型同时关注序列中的所有部分。
* **BERT (Encoder-only):**  由 Google 提出，擅长理解语义，常用于文本分类、问答。
* **GPT 系列 (Decoder-only):**  由 OpenAI 提出，擅长生成文本。目前的 GPT-4、Claude 等大模型大多基于此架构。
* **ViT (Vision Transformer):**  将 Transformer 引入视觉领域，打破了 CNN 在图像领域的垄断。

---

### 4. 现代生成式模型 (Generative Models)

除了文本生成，这些算法在图像、视频生成领域取得了突破。

* **扩散模型 (Diffusion Models):**  \* **原理：**  通过逐渐给图像“去噪”来生成高质量图片。

  * **代表：**  Stable Diffusion, Midjourney, DALL-E 3。
* **混合专家模型 (MoE - Mixture of Experts):**

  * **原理：**  并非每次运行都激活所有参数，而是根据输入只选择部分“专家”神经元工作。
  * **代表：**  GPT-4, Mixtral。这极大地提高了超大规模模型的运行效率。

---

### 5. 强化学习 (Reinforcement Learning)

让机器在与环境的交互中通过“奖励”来学习策略。

* **DQN (Deep Q-Network):**  深度学习与 Q 学习的结合。
* **PPO (Proximal Policy Optimization):**  目前主流的强化学习算法，也是 ChatGPT 进行 **RLHF (人类反馈强化学习)**  的核心技术。

**这一长串名单中，你是否有特别想要深入了解某一个的具体实现原理或应用场景？我可以为你针对性地拆解。**
