# 奖励函数bug验证报告

> **问题来源**: 架构师发现的两个"致命Bug"
> **发现时间**: 2026-01-26 00:00
> **验证时间**: 2026-01-26 00:10
> **状态**: ✅ 已验证 - 架构师完全正确
> **严重程度**: 🔴 严重（影响训练质量）

---

## 问题1: action_smoothness 符号错误

### 架构师的主张

**问题**: 双重负号导致奖励黑客漏洞

**代码分析**:
```python
# 函数定义（dashgo_env_v2.py:563-565）
def reward_action_smoothness(env):
    diff = env.action_manager.action - env.action_manager.prev_action
    return -torch.sum(torch.square(diff), dim=1)  # 返回负值

# 配置（dashgo_env_v2.py:1010）
action_smoothness = RewardTermCfg(
    func=reward_action_smoothness,
    weight=-0.01,  # ❌ 负权重
)
```

**数学后果**:
```
最终奖励 = 负函数 × 负权重
         = -sum(diff²) × (-0.01)
         = +0.01 × sum(diff²)  ❌ 正数（奖励！）
```

### 实际验证

**日志证据**:
```
TensorBoard日志分析:
- action_smoothness平均值: 307
- 最小值: 0.18
- 最大值: 1004.34
- 符号: ✅ 全部为正数（刷分）
```

**反推计算**:
```
最新10个平均值: 257
episode长度: 832步
原始总和: 257 / 0.01 = 25,700
每步原始: 25,700 / 832 = 30.89
每步差异: sqrt(30.89) = 5.56

结论: 每步动作差异5.56 → 严重抖动
```

### 验证结论

✅ **架构师完全正确！**

**问题确认**:
- ❌ 双重负号错误：负函数 × 负权重 = 正奖励
- ❌ 机器人在刷分：动作差异越大，得分越高
- ❌ 奖励黑客漏洞

**影响评估**:
- ❌ 理论上: 机器人通过抖动刷分
- ✅ 实际上: 被alive惩罚抵消（-832），净效果仍是惩罚
- ⚠️ 风险: 如果未来调整权重，错误会放大

---

## 问题2: shaping_distance 梯度消失

### 架构师的主张

**问题**: std=2.0导致远处梯度消失

**代码分析**:
```python
shaping_distance = RewardTermCfg(
    func=reward_position_command_error_tanh,
    weight=0.75,
    params={"std": 2.0, ...}
)
```

**数学验证**:
```
std=2.0:
  距离 2m: reward = 0.179 ✅ 可用
  距离 4m: reward = 0.027 ⚠️ 微弱
  距离 6m: reward = 0.0037 ❌ 几乎为0
  距离 8m: reward = 0.0005 ❌ 梯度消失
```

### 实际验证

**日志证据**:
```
TensorBoard日志分析:
- shaping_distance平均值: 0.007857
- 最小值: 0.000000
- 最大值: 0.329793
- > 0.001 的比例: 7%
```

**统计**:
- 93%的episode: shaping_distance = 0.0000
- 只有7%的episode: 有微小奖励

### 验证结论

✅ **架构师完全正确！**

**问题确认**:
- ❌ std=2.0太小，远处梯度消失
- ❌ 93%的episode奖励为0.0000
- ❌ 机器人大部分时间感受不到引导

**改进方案**（架构师建议）:
```python
params={"std": 4.0, ...}  # 从2.0改为4.0
```

**效果**:
```
std=4.0:
  距离 2m: reward = 0.403 (提升2.2倍)
  距离 4m: reward = 0.179 (提升6.6倍)
  距离 6m: reward = 0.071 (提升19倍)
  距离 8m: reward = 0.027 (提升54倍)
```

---

## 为什么用户感觉"没什么影响"？

### 矛盾分析

**架构师预测**: 严重bug，训练会失败
**用户观察**: 训练看起来正常，机器人能到达目标

### 原因分析

1. **action_smoothness影响被抵消**
   ```
   action_smoothness: +307 (刷分)
   alive惩罚: -832 (832步 × -1)
   净影响: +307 - 832 = -525 (仍然是惩罚)
   ```

2. **reach_goal主导训练**
   ```
   reach_goal: 2000分 (绝对主导)
   其他奖励: 相对较小
   ```

3. **shaping_distance失效但有其他引导**
   - alive惩罚迫使行动
   - 其他dense奖励提供辅助

**结论**:
- 架构师：理论上正确（bug确实存在）
- 用户：实际上影响小（被其他机制补偿）
- 建议：修复（低成本，避免未来问题）

---

## 修复方案

### 修复1: action_smoothness
```python
# dashgo_env_v2.py:1010
weight=0.01  # 从 -0.01 改为 0.01
```

### 修复2: shaping_distance
```python
# dashgo_env_v2.py:978
params={"std": 4.0, ...}  # 从 2.0 改为 4.0
```

---

## 执行记录

**修复时间**: 2026-01-26 00:30
**修复文件**:
- dashgo_env_v2.py (3处修改)
- train_v2.py (新增自动课程学习)
- train_cfg_v2.yaml (配置统一)

**Git提交**: 0d341bf
**GitHub推送**: ✅ 成功

---

**验证时间**: 2026-01-26 00:10
**验证者**: Claude Code AI System
**状态**: ✅ 架构师完全正确
**修复状态**: ✅ 已修复并推送到GitHub

