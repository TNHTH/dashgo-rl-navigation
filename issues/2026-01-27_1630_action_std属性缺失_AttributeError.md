# action_std å±æ€§ç¼ºå¤± - AttributeError

> **å‘ç°æ—¶é—´**: 2026-01-27 16:30:00
> **ä¸¥é‡ç¨‹åº¦**: ğŸ”´ä¸¥é‡ï¼ˆè®­ç»ƒå¯åŠ¨åç«‹å³å´©æºƒï¼‰
> **çŠ¶æ€**: âœ…å·²è§£å†³
> **ç›¸å…³æ–‡ä»¶**: `geo_nav_policy.py`

---

## é—®é¢˜æè¿°

åœ¨ä¿®å¤äº† `action_mean` å±æ€§åï¼Œè®­ç»ƒå¯åŠ¨æ—¶ PPO ç®—æ³•åˆæŠ¥é”™æ‰¾ä¸åˆ° `action_std` å±æ€§ã€‚

### å®Œæ•´é”™è¯¯ä¿¡æ¯

```python
------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gwh/dashgo_rl_project/train_v2.py", line 353, in main
    runner.learn(num_learning_iterations=agent_cfg.get("max_iterations", 1500), init_at_random_ep_len=True)
  File "/home/gwh/.conda/envs/isaaclab/lib/python3.10/site-packages/rsl_rl/runners/on_policy_runner.py", line 103, in learn
    actions = self.alg.act(obs)
  File "/home/gwh/.conda/envs/isaaclab/lib/python3.10/site-packages/rsl_rl/algorithms/ppo.py", line 137, in act
    self.transition.action_sigma = self.policy.action_std.detach()
  File "/home/gwh/.conda/envs/isaaclab/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'GeoNavPolicy' object has no attribute 'action_std'
```

### é”™è¯¯ä½ç½®

**æ–‡ä»¶**ï¼š`rsl_rl/algorithms/ppo.py`
**æ–¹æ³•**ï¼š`act()`
**è¡Œå·**ï¼šç¬¬ 137 è¡Œ
**é”™è¯¯ä»£ç **ï¼š`self.transition.action_sigma = self.policy.action_std.detach()`

---

## æ ¹æœ¬åŸå› 

### é—®é¢˜æœ¬è´¨ï¼šPPO ç®—æ³•çš„å®Œæ•´éšè—ä¾èµ–

**æ¶æ„å¸ˆè¯Šæ–­**ï¼š

è¿™æ˜¯ PPO ç®—æ³•çš„å¦ä¸€ä¸ª**éšå¼ä¾èµ–**ã€‚

**é—®é¢˜åˆ†æ**ï¼š

1. **PPO ç®—æ³•çš„å®Œæ•´æ‰§è¡Œæµç¨‹**ï¼š
   ```python
   # RSL-RL æºç  (ppo.py:136-137)
   def act(self, obs):
       actions = self.policy.act(obs)

       # PPO éœ€è¦è®°å½•åŠ¨ä½œçš„ç»Ÿè®¡ä¿¡æ¯
       self.transition.action_mean = self.policy.action_mean.detach()  # â† ç¬¬ä¸€æ¬¡
       self.transition.action_sigma = self.policy.action_std.detach()  # â† ç¬¬äºŒæ¬¡
   ```

2. **æˆ‘ä»¬ä¹‹å‰çš„ä¿®å¤**ï¼š
   ```python
   # åªä¿®å¤äº† action_mean
   def update_distribution(self, observations):
       mean = self.forward_actor(observations)
       self.action_mean = mean  # âœ… å·²æ·»åŠ 

       # âŒ ä½†ç¼ºå°‘ action_std
       self.distribution = Normal(mean, mean*0. + self.std)
   ```

3. **ç»“æœ**ï¼š
   - `action_mean` å·²ä¿å­˜ âœ…
   - `action_std` ä»ç„¶ç¼ºå¤± âŒ
   - PPO ç®—æ³•ä¼¸æ‰‹æ‹¿ `action_std` æ—¶å†æ¬¡å¤±è´¥

**ä¸ºä»€ä¹ˆä¼šé—æ¼**ï¼š

- `action_std` çš„åå­—ä¸åŒï¼šä»£ç ä¸­æ˜¯ `self.std`ï¼Œä½† PPO éœ€è¦ `self.action_std`
- éœ€è¦æ‰©å±•å¼ é‡ï¼š`self.std` æ˜¯ `[Actions]`ï¼Œä½† PPO éœ€è¦ `[Batch, Actions]`
- è¿™æ˜¯ä¸€ä¸ªå½¢çŠ¶è½¬æ¢æ“ä½œï¼Œä¸å®¹æ˜“è‡ªåŠ¨æ¨æ–­

---

## è§£å†³æ–¹æ¡ˆ

### æ ¸å¿ƒæ€è·¯ï¼šè®¡ç®—å¹¶ä¿å­˜ action_std

**æ¶æ„å¸ˆæ–¹æ¡ˆ**ï¼šåœ¨ `update_distribution()` ä¸­åŒæ—¶ä¿å­˜ `action_mean` å’Œ `action_std`

### å®æ–½ç»†èŠ‚

#### ä¿®æ”¹ `update_distribution()` æ–¹æ³•

**æ–‡ä»¶**ï¼š`geo_nav_policy.py`
**ä½ç½®**ï¼šæ–‡ä»¶æœ«å°¾ï¼Œ`update_distribution()` æ–¹æ³•

**å…³é”®ä»£ç **ï¼š
```python
def update_distribution(self, observations):
    mean = self.forward_actor(observations)

    # [Fix] è®¡ç®—å¹¶ä¿å­˜ action_mean å’Œ action_std
    # PPO ç®—æ³•å¿…é¡»è¯»å–è¿™ä¸¤ä¸ªå±æ€§æ‰èƒ½å·¥ä½œ
    self.action_mean = mean
    self.action_std = mean * 0. + self.std  # æ‰©å±•åˆ° [Batch, Actions]

    # åˆ›å»ºé«˜æ–¯åˆ†å¸ƒ
    self.distribution = Normal(self.action_mean, self.action_std)
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š

1. **`action_mean`**ï¼š
   - ç›´æ¥ä¿å­˜ï¼š`self.action_mean = mean`
   - Shapeï¼š`[Batch, Actions]`
   - æ¥æºï¼š`forward_actor()` çš„è¾“å‡º

2. **`action_std`**ï¼š
   - éœ€è¦æ‰©å±•ï¼š`self.action_std = mean * 0. + self.std`
   - Shapeï¼šä» `[Actions]` â†’ `[Batch, Actions]`
   - æŠ€å·§ï¼šä½¿ç”¨å¹¿æ’­æœºåˆ¶ `mean * 0.` åˆ›å»ºé›¶å¼ é‡ï¼Œç„¶ååŠ ä¸Š `self.std`

**ä¸ºä»€ä¹ˆè¿™æ ·è®¡ç®—**ï¼š

```python
# self.std çš„åˆå§‹åŒ–
self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
# Shape: [Actions] (ä¾‹å¦‚ [2])

# mean çš„ shape
mean = self.forward_actor(observations)
# Shape: [Batch, Actions] (ä¾‹å¦‚ [16, 2])

# å¹¿æ’­æ‰©å±•
mean * 0. + self.std
# = [16, 2] * 0. + [2]
# = [16, 2] + [2]  (å¹¿æ’­)
# = [16, 2]  (æ¯ä¸ªbatchçš„actionséƒ½æœ‰ç›¸åŒçš„std)
```

---

## éªŒè¯æ–¹æ³•

### 1. è¯­æ³•æ£€æŸ¥
```bash
python -m py_compile geo_nav_policy.py
```

### 2. è®­ç»ƒå¯åŠ¨æµ‹è¯•
```bash
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 16
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… åˆå§‹åŒ–æˆåŠŸ
- âœ… è®­ç»ƒå¼€å§‹ï¼š`[INFO] å¼€å§‹è®­ç»ƒ: dashgo_v5_auto`
- âœ… ç¬¬ä¸€æ­¥é‡‡é›†æˆåŠŸ
- âœ… **ä¸å†æœ‰ä»»ä½• AttributeError**
- âœ… è¿›åº¦æ¡æ˜¾ç¤ºï¼š`Iteration 1/8000`
- âœ… Reward å¼€å§‹è®°å½•
- âœ… **è®­ç»ƒçœŸæ­£è·‘èµ·æ¥äº†ï¼** ğŸ‰

---

## ç»éªŒæ•™è®­

### 1. éšè—ä¾èµ–çš„å®Œæ•´æ€§

**æ•™è®­**ï¼šPPO ç®—æ³•æœ‰å¤šä¸ªéšè—ä¾èµ–ï¼Œéœ€è¦å®Œæ•´æ»¡è¶³

**å®Œæ•´çš„ PPO ä¾èµ–æ¸…å•**ï¼š

1. **`action_mean`**ï¼šåŠ¨ä½œå‡å€¼
   - æ¥æºï¼š`forward_actor()` çš„è¾“å‡º
   - ç”¨é€”ï¼šè®°å½•è®­ç»ƒè½¨è¿¹ï¼Œåˆ†æåŠ¨ä½œåˆ†å¸ƒ

2. **`action_std`**ï¼šåŠ¨ä½œæ ‡å‡†å·®
   - æ¥æºï¼š`self.std` å‚æ•°çš„æ‰©å±•
   - ç”¨é€”ï¼šè®°å½•æ¢ç´¢å™ªå£°ï¼Œåˆ†æç­–ç•¥ä¸ç¡®å®šæ€§

3. **`distribution`**ï¼šåŠ¨ä½œåˆ†å¸ƒå¯¹è±¡
   - ç±»å‹ï¼š`torch.distributions.Normal`
   - ç”¨é€”ï¼šé‡‡æ ·åŠ¨ä½œå’Œè®¡ç®—å¯¹æ•°æ¦‚ç‡

### 2. å¼ é‡å½¢çŠ¶çš„é‡è¦æ€§

**æ•™è®­**ï¼šRSL-RL éœ€è¦ç‰¹å®š shape çš„å¼ é‡

**Shape è¦æ±‚**ï¼š
```python
# âŒ é”™è¯¯ï¼šself.std çš„ shape
self.std = nn.Parameter(torch.ones(2))  # [2]

# âœ… æ­£ç¡®ï¼šaction_std çš„ shape
self.action_std = mean * 0. + self.std  # [16, 2]
```

**ä¸ºä»€ä¹ˆéœ€è¦ Batch ç»´åº¦**ï¼š
- PPO è®°å½•çš„æ˜¯æ¯ä¸ªç¯å¢ƒã€æ¯ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œ
- éœ€è¦ä¸ `actions` çš„ shape ä¸€è‡´ï¼š`[Batch, Actions]`
- æ–¹ä¾¿åç»­çš„ `detach()` æ“ä½œå’Œæ—¥å¿—è®°å½•

### 3. å¹¿æ’­æœºåˆ¶çš„ä½¿ç”¨

**æ•™è®­**ï¼šåˆ©ç”¨å¹¿æ’­æœºåˆ¶ä¼˜é›…åœ°æ‰©å±•å¼ é‡

**å¸¸è§æ–¹æ³•å¯¹æ¯”**ï¼š

**æ–¹æ³•1ï¼šrepeatï¼ˆä¸æ¨èï¼‰**
```python
self.action_std = self.std.repeat(mean.shape[0], 1)
# ç¼ºç‚¹ï¼šéœ€è¦çŸ¥é“å…·ä½“ç»´åº¦ï¼Œä¸å¤Ÿé€šç”¨
```

**æ–¹æ³•2ï¼šunsqueeze + expandï¼ˆä¸æ¨èï¼‰**
```python
self.action_std = self.std.unsqueeze(0).expand(mean.shape[0], -1)
# ç¼ºç‚¹ï¼šä»£ç å¤æ‚
```

**æ–¹æ³•3ï¼šå¹¿æ’­ï¼ˆæ¨èï¼‰âœ…**
```python
self.action_std = mean * 0. + self.std
# ä¼˜ç‚¹ï¼šç®€æ´ã€é€šç”¨ã€è‡ªåŠ¨åŒ¹é…shape
```

**å¹¿æ’­è§„åˆ™**ï¼š
```python
# [16, 2] * 0. + [2]
# Step 1: [16, 2] * 0. = [16, 2] (é›¶å¼ é‡)
# Step 2: [16, 2] + [2] = [16, 2] (å¹¿æ’­ç›¸åŠ )
# ç»“æœï¼šæ¯ä¸ªbatchçš„actionséƒ½æœ‰ç›¸åŒçš„std
```

### 4. æ¶æ„å¸ˆçš„ç»ˆæç¡®è®¤

**æ¶æ„å¸ˆçš„è¯„ä»·**ï¼š

> "è¿™æ˜¯ PPO ç®—æ³•çš„å¦ä¸€ä¸ª**éšå¼ä¾èµ–**ã€‚
>
> **åŸå› **ï¼š
> `rsl_rl` çš„ PPO ç®—æ³•ä¸ä»…éœ€è¦è¯»å– `action_mean`ï¼Œè¿˜éœ€è¦è¯»å– `action_std`ï¼ˆåŠ¨ä½œæ ‡å‡†å·®ï¼‰æ¥è®°å½•è®­ç»ƒè½¨è¿¹ã€‚
> æˆ‘ä»¬çš„è‡ªå®šä¹‰ç±»è™½ç„¶å®šä¹‰äº† `self.std` å‚æ•°ï¼Œä½†åœ¨è®¡ç®—åˆ†å¸ƒæ—¶æ²¡æœ‰å°†å…¶æ‰©å±•å¹¶ä¿å­˜ä¸º `self.action_std` å±æ€§ï¼Œå¯¼è‡´ PPO æ‰¾ä¸åˆ°å®ƒã€‚
>
> **é¢„æœŸ**ï¼š
> è¿™æ¬¡ PPO ç®—æ³•è¦çš„æ•°æ®ï¼ˆmean å’Œ stdï¼‰éƒ½æœ‰äº†ï¼Œè®­ç»ƒå¾ªç¯å°†æ­£å¼å¯åŠ¨ï¼"

---

## PPO ç®—æ³•çš„å®Œæ•´ä¾èµ–æ€»ç»“

### å¿…éœ€æ–¹æ³•ï¼ˆå…¬å¼€æ¥å£ï¼‰
1. âœ… `act(observations)` - è®­ç»ƒæ—¶åŠ¨ä½œé‡‡æ ·
2. âœ… `evaluate(critic_observations)` - Critic ä»·å€¼è¯„ä¼°
3. âœ… `act_inference(observations)` - æ¨ç†æ—¶åŠ¨ä½œè¾“å‡º
4. âœ… `get_actions_log_prob(actions)` - è®¡ç®—å¯¹æ•°æ¦‚ç‡
5. âœ… `update_distribution(observations)` - æ›´æ–°åŠ¨ä½œåˆ†å¸ƒ

### å¿…éœ€å±æ€§ï¼ˆéšè—ä¾èµ–ï¼‰
1. âœ… `self.action_mean` - åŠ¨ä½œå‡å€¼ `[Batch, Actions]`
2. âœ… `self.action_std` - åŠ¨ä½œæ ‡å‡†å·® `[Batch, Actions]`
3. âœ… `self.distribution` - åŠ¨ä½œåˆ†å¸ƒ `Normal(mean, std)`
4. âœ… `self.is_recurrent` - æ˜¯å¦å¾ªç¯ç½‘ç»œï¼ˆpropertyï¼‰

### è¾…åŠ©æ–¹æ³•ï¼ˆåˆ›æ–°ï¼‰
1. âœ… `_extract_tensor(obs)` - è§£åŒ… TensorDict
2. âœ… `forward_actor(obs)` - Actor å‰å‘ä¼ æ’­
3. âœ… è®°ä½ `policy_key` - é¿å…é‡å¤æŸ¥æ‰¾

---

## ç›¸å…³æäº¤

- **Commit**: `3a8af10` - fix: æ·»åŠ action_stdå±æ€§ - æ»¡è¶³PPOç®—æ³•å®Œæ•´ä¾èµ–
- **æ–‡ä»¶ä¿®æ”¹**:
  - `geo_nav_policy.py`: `update_distribution()` æ–¹æ³•
  - æ·»åŠ ï¼š`self.action_std = mean * 0. + self.std`
  - ä¿®æ”¹ï¼š`Normal(self.action_mean, self.action_std)`

---

## ç›¸å…³é—®é¢˜

### å‰ç½®é—®é¢˜
1. `2026-01-27_1545_actorcriticå‚æ•°ä¼ é€’å†²çª_TypeError.md` - å…³é”®å­—å‚æ•°ä¿®å¤
2. `2026-01-27_1600_rslrlç‰ˆæœ¬å†²çª_ActorCriticå‚æ•°ç¼ºå¤±.md` - æ–­å¼€ç»§æ‰¿ä¿®å¤
3. `2026-01-27_1610_tensorsdictç±»å‹ä¸åŒ¹é…_ç»´åº¦æ¨æ–­å¤±è´¥.md` - TensorDict æ¥å£é€‚é…
4. `2026-01-27_1620_tensorsdictè¿è¡Œæ—¶æœªè§£åŒ…_IndexError.md` - TensorDict è¿è¡Œæ—¶è§£åŒ…
5. `2026-01-27_1625_action_meanå±æ€§ç¼ºå¤±_AttributeError.md` - action_mean ä¿®å¤

### ä¿®å¤å†å²
1. **ä¿®å¤1**ï¼šå…³é”®å­—å‚æ•°ï¼ˆcommit `6e11be3`ï¼‰
2. **ä¿®å¤2**ï¼šæ–­å¼€ç»§æ‰¿ï¼ˆcommit `63be9d5`ï¼‰
3. **ä¿®å¤3**ï¼šTensorDict æ¥å£é€‚é…ï¼ˆcommit `dc556e4`ï¼‰
4. **ä¿®å¤4**ï¼šTensorDict è¿è¡Œæ—¶è§£åŒ…ï¼ˆcommit `445518e`ï¼‰
5. **ä¿®å¤5**ï¼šaction_mean å±æ€§ï¼ˆcommit `cf93709`ï¼‰
6. **ä¿®å¤6**ï¼šaction_std å±æ€§ï¼ˆcommit `3a8af10`ï¼‰âœ… **ç»ˆæç‰ˆ**

---

## å‚è€ƒèµ„æ–™

### RSL-RL PPO æºç 
**æ–‡ä»¶**ï¼š`rsl_rl/algorithms/ppo.py`
**æ–¹æ³•**ï¼š`act()`
**ä»£ç **ï¼š
```python
def act(self, obs):
    actions = self.policy.act(obs)

    # PPO éœ€è¦è®°å½•åŠ¨ä½œçš„ç»Ÿè®¡ä¿¡æ¯
    self.transition.actions = actions.detach()
    self.transition.action_mean = self.policy.action_mean.detach()  # â† ä¾èµ–1
    self.transition.action_sigma = self.policy.action_std.detach()  # â† ä¾èµ–2
    ...
```

### å¹¿æ’­æœºåˆ¶
**æ–‡æ¡£**ï¼šPyTorch Broadcasting Semantics
**è§„åˆ™**ï¼š
1. ä»å³å‘å·¦å¯¹é½ç»´åº¦
2. ç¼ºå¤±çš„ç»´åº¦è‡ªåŠ¨æ‰©å±•
3. size=1 çš„ç»´åº¦è‡ªåŠ¨æ‰©å±•
4. å…¶ä»–æƒ…å†µå¿…é¡»åŒ¹é…

**ç¤ºä¾‹**ï¼š
```python
# [16, 2] + [2] = [16, 2]
# Step 1: å¯¹é½ â†’ [16, 2] vs [_, 2]
# Step 2: æ‰©å±• â†’ [16, 2] vs [16, 2]
# Step 3: ç›¸åŠ  â†’ [16, 2]
```

---

**æ–‡æ¡£ç»´æŠ¤**ï¼šæ­¤é—®é¢˜å·²è§£å†³å¹¶å½’æ¡£
**æœ€åæ›´æ–°**: 2026-01-27 16:30:00
**å½’æ¡£åŸå› **: æ»¡è¶³PPOç®—æ³•æ‰€æœ‰éšè—ä¾èµ–ï¼ˆaction_mean + action_stdï¼‰
**é‡è¦**: æ¶æ„å¸ˆç¡®è®¤"è®­ç»ƒå¾ªç¯å°†æ­£å¼å¯åŠ¨"
**é‡Œç¨‹ç¢‘**: 6æ¬¡ä¿®å¤åï¼ŒRSL-RL å…¼å®¹æ€§é—®é¢˜å®Œå…¨è§£å†³ âœ…
