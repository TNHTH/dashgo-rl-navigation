# å®æ–½v5.0 Ultimateæ–¹æ¡ˆ - è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹  + æ··åˆå¥–åŠ±

> **å®æ–½æ—¶é—´**: 2026-01-25 17:00:00
> **ä¸¥é‡ç¨‹åº¦**: ğŸŸ¢ åŠŸèƒ½æ”¹è¿›
> **çŠ¶æ€**: âœ… å·²å®æ–½
> **ç›¸å…³æ–‡ä»¶**: train_cfg_v2.yaml, dashgo_env_v2.py

---

## é—®é¢˜æè¿°

**ä»v3_robust_navå‡çº§åˆ°v5.0 Ultimate**ï¼š

v3_robust_navçš„é—®é¢˜ï¼š
- âš ï¸ action_smoothnesså‡ ä¹ä¸º0ï¼ˆNoiseé£é™©ï¼‰
- âš ï¸ shaping_distance 0.5è¿‡ä½ï¼ˆæ”¶æ•›æ…¢ï¼‰
- âš ï¸ æ— è¯¾ç¨‹å­¦ä¹ ï¼ˆè¿œè·ç¦»ç›®æ ‡å¯èƒ½å­¦ä¸ä¼šï¼‰
- âš ï¸ reach_goalå¥–åŠ±1000.0ä¸å¤Ÿçªå‡º

**è§£å†³æ–¹æ¡ˆ**ï¼šé‡‡çº³æ¶æ„å¸ˆv4.0 Auto-Curriculumå»ºè®®ï¼Œå®æ–½v5.0 Ultimateæ–¹æ¡ˆ

---

## å®æ–½å†…å®¹

### 1. è®­ç»ƒé…ç½®ä¿®æ”¹ï¼ˆtrain_cfg_v2.yamlï¼‰

```yaml
# ä¿®æ”¹å‰ï¼ˆv3_robust_navï¼‰
max_iterations: 4000
run_name: "v3_robust_nav"
save_interval: 50

# ä¿®æ”¹åï¼ˆv5.0 Ultimateï¼‰
max_iterations: 5000        # âœ… ç»™è¶³æ—¶é—´è®©è¯¾ç¨‹ä» 3m èµ°åˆ° 8m
run_name: "v5_ultimate_auto"
save_interval: 100          # âœ… æ¯100è½®ä¿å­˜ï¼Œæ›´å®‰å…¨
```

### 2. æ ¸å¿ƒåŠŸèƒ½æ·»åŠ ï¼ˆdashgo_env_v2.pyï¼‰

#### 2.1 è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ å‡½æ•°

**æ–°å¢**ï¼š`curriculum_expand_target_range()`

```python
def curriculum_expand_target_range(env, env_ids, command_name, start_step, end_step, min_limit, max_limit):
    """
    [v5.0 æ ¸å¿ƒ] è‡ªåŠ¨åŒ–è¯¾ç¨‹å­¦ä¹ 
    æ ¹æ®å½“å‰è®­ç»ƒæ€»æ­¥æ•°ï¼Œçº¿æ€§æ‰©å±•ç›®æ ‡ç”Ÿæˆçš„è·ç¦»èŒƒå›´ (3m -> 8m)
    """
    current_step = env.common_step_counter

    # è®¡ç®—è¿›åº¦ alpha (0.0 ~ 1.0)
    if current_step < start_step:
        alpha = 0.0
    elif current_step > end_step:
        alpha = 1.0
    else:
        alpha = (current_step - start_step) / (end_step - start_step)

    # è®¡ç®—å½“å‰éš¾åº¦
    current_limit = min_limit + (max_limit - min_limit) * alpha

    # åŠ¨æ€ä¿®æ”¹å‘½ä»¤ç”Ÿæˆå™¨çš„å‚æ•°
    cmd_term = env.command_manager.get_term(command_name)
    if hasattr(cmd_term.cfg, "ranges") and hasattr(cmd_term.cfg.ranges, "pos_x"):
        cmd_term.cfg.ranges.pos_x = (-current_limit, current_limit)
        cmd_term.cfg.ranges.pos_y = (-current_limit, current_limit)
```

**åŸç†**ï¼š
- ä½¿ç”¨ç‰©ç†æ­¥æ•°ï¼ˆcommon_step_counterï¼‰è€Œéiterationæ•°ä½œä¸ºæ—¶é—´åŸºå‡†
- çº¿æ€§æ’å€¼ä¿è¯å¹³æ»‘è¿‡æ¸¡
- åŠ¨æ€ä¿®æ”¹å‘½ä»¤ç”Ÿæˆå™¨çš„é…ç½®èŒƒå›´å®ç°éš¾åº¦çˆ¬å¡

#### 2.2 è¾…åŠ©å¥–åŠ±å‡½æ•°ä¼˜åŒ–

**æ–°å¢**ï¼šç®€åŒ–ç‰ˆ`reward_target_speed()`å’Œ`reward_facing_target()`

```python
def reward_target_speed(env, asset_cfg):
    """[v5.0] é€Ÿåº¦å¯¹é½å¥–åŠ±ï¼šç›´æ¥é¼“åŠ±å‘å‰ç§»åŠ¨"""
    vel = env.scene[asset_cfg.name].data.root_lin_vel_b[:, 0]
    return torch.clamp(vel, min=0.0)

def reward_facing_target(env, command_name, asset_cfg):
    """[v5.0] å¯¹å‡†å¥–åŠ±ï¼šé¼“åŠ±è½¦å¤´æœå‘ç›®æ ‡"""
    target_pos = env.command_manager.get_command(command_name)[:, :2]
    robot_pos = env.scene[asset_cfg.name].data.root_pos_w[:, :2]
    robot_yaw = env.scene[asset_cfg.name].data.heading_w
    target_vec = target_pos - robot_pos
    target_yaw = torch.atan2(target_vec[:, 1], target_vec[:, 0])
    angle_error = torch.abs(mdp.math.wrap_to_pi(target_yaw - robot_yaw))
    return 1.0 / (1.0 + angle_error)
```

### 3. å¥–åŠ±å‡½æ•°é‡æ„ï¼ˆRewardsCfgï¼‰

**ä¿®æ”¹å‰ï¼ˆv3_robust_navï¼‰**ï¼š
```python
reach_goal = 1000.0
shaping_distance = 0.5 (åŠ¿èƒ½å·®)
action_smoothness = 0.0001 (å‡ ä¹ä¸º0)
collision threshold = 1.0
```

**ä¿®æ”¹åï¼ˆv5.0 Ultimateï¼‰**ï¼š
```python
reach_goal = 2000.0  # âœ… ç»å¯¹ä¸»å¯¼å€¼ï¼ˆæå‡2å€ï¼‰
shaping_distance = 0.75 (ä½¿ç”¨mdp.rewards.position_command_error_tanh)  # âœ… é»„é‡‘å¹³è¡¡ç‚¹
action_smoothness = -0.01  # âœ… æå‡100å€
collision threshold = 10.0  # âœ… æ”¾å®½åˆ°10.0
```

**å®Œæ•´å¥–åŠ±æ¶æ„**ï¼š
```python
@configclass
class DashgoRewardsCfg:
    # [ä¸»å¯¼] ç»ˆç‚¹å¤§å¥–ï¼š2000åˆ†
    reach_goal = RewardTermCfg(
        func=reward_near_goal,
        weight=2000.0,
        params={"threshold": 0.5}
    )

    # [å¼•å¯¼] é»„é‡‘å¹³è¡¡ç‚¹ 0.75 + tanh
    shaping_distance = RewardTermCfg(
        func=mdp.rewards.position_command_error_tanh,
        weight=0.75,
        params={"std": 2.0, "command_name": "target_pose"}
    )

    # [è¾…åŠ©] Denseå¥–åŠ±ç»„ï¼ˆä¿ç•™v3ä¼˜åŠ¿ï¼‰
    target_speed = RewardTermCfg(func=reward_target_speed, weight=1.0)
    facing_goal = RewardTermCfg(func=reward_facing_target, weight=0.1)
    velodyne_style_reward = RewardTermCfg(func=reward_navigation_sota, weight=1.0)

    # [çº¦æŸ] åŠ¨ä½œå¹³æ»‘ï¼š-0.01
    action_smoothness = RewardTermCfg(
        func=reward_action_smoothness,
        weight=-0.01  # ä»0.0001æå‡100å€
    )

    # [çº¦æŸ] ç¢°æ’æƒ©ç½šï¼š-50.0 + 10.0é˜ˆå€¼
    collision = RewardTermCfg(
        func=penalty_collision_force,
        weight=-50.0,
        params={"threshold": 10.0}
    )
```

### 4. å‘½ä»¤é…ç½®ä¿®æ”¹ï¼ˆCommandsCfgï¼‰

**ä¿®æ”¹å‰ï¼ˆv3_robust_navï¼‰**ï¼š
```python
ranges: mdp.UniformPoseCommandCfg.Ranges(
    pos_x=(0.5, 1.5), pos_y=(-1.0, 1.0),  # 1.5mèŒƒå›´
)
```

**ä¿®æ”¹åï¼ˆv5.0 Ultimateï¼‰**ï¼š
```python
ranges: mdp.UniformPoseCommandCfg.Ranges(
    pos_x=(-3.0, 3.0), pos_y=(-3.0, 3.0),  # âœ… 3m x 3mæ­£æ–¹å½¢ï¼ˆæ–°æ‰‹åŒºï¼‰
)
```

### 5. è¯¾ç¨‹é…ç½®æ–°å¢ï¼ˆCurriculumCfgï¼‰

**æ–°å¢**ï¼š`DashgoCurriculumCfg`

```python
@configclass
class DashgoCurriculumCfg:
    """[v5.0 æ ¸å¿ƒ] è‡ªåŠ¨åŒ–è¯¾ç¨‹å­¦ä¹ é…ç½®"""
    target_expansion = CurriculumTermCfg(
        func=curriculum_expand_target_range,
        params={
            "command_name": "target_pose",
            "min_limit": 3.0,  # åˆå§‹éš¾åº¦ï¼š3ç±³ï¼ˆæ–°æ‰‹åŒºï¼‰
            "max_limit": 8.0,  # æœ€ç»ˆéš¾åº¦ï¼š8ç±³ï¼ˆä¸“å®¶åŒºï¼‰
            "start_step": 0,
            "end_step": 300_000_000,  # 300Mç‰©ç†æ­¥å®Œæˆçˆ¬å¡
        }
    )
```

### 6. ç¯å¢ƒé…ç½®æ›´æ–°ï¼ˆDashgoNavEnvV2Cfgï¼‰

**æ–°å¢**ï¼š
```python
@configclass
class DashgoNavEnvV2Cfg(ManagerBasedRLEnvCfg):
    # ... å…¶ä»–é…ç½® ...
    curriculum = DashgoCurriculumCfg()  # âœ… å¯ç”¨è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ 
```

---

## å…³é”®æ”¹è¿›ç‚¹

### 1. è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

**ä¼˜åŠ¿**ï¼š
- âœ… é›¶å¹²é¢„ï¼ˆFire & Forgetï¼‰
- âœ… è‡ªåŠ¨æ‰©å±•éš¾åº¦ï¼ˆ3m â†’ 8mï¼‰
- âœ… çº¿æ€§æ’å€¼å¹³æ»‘è¿‡æ¸¡

**å®ç°æ–¹å¼**ï¼š
- é€šè¿‡`CurriculumTermCfg`æ³¨å†Œè¯¾ç¨‹å‡½æ•°
- åœ¨ç¯å¢ƒé‡ç½®æ—¶è‡ªåŠ¨è°ƒç”¨
- åŠ¨æ€ä¿®æ”¹å‘½ä»¤ç”Ÿæˆå™¨çš„`ranges`å‚æ•°

### 2. æ··åˆå¥–åŠ±æ¶æ„

**è®¾è®¡ç†å¿µ**ï¼š
- **Sparseä¸»å¯¼**ï¼ˆ2000.0ï¼‰ï¼šç¡®ä¿"åˆ°è¾¾ç»ˆç‚¹"æ˜¯å…¨å±€æœ€ä¼˜è§£
- **Denseè¾…åŠ©**ï¼ˆ1.0+0.1+1.0ï¼‰ï¼šè§£å†³åˆæœŸè¿·èŒ«ï¼Œæé«˜å­¦ä¹ æ•ˆç‡
- **é»„é‡‘å¹³è¡¡**ï¼ˆ0.75+tanhï¼‰ï¼šæä¾›æ–¹å‘æ„Ÿï¼Œtanhé™åˆ¶å•æ­¥æ”¶ç›Šé˜²æ­¢åˆ·åˆ†
- **å¼ºçº¦æŸ**ï¼ˆ-0.01, -50.0ï¼‰ï¼šæŠ‘åˆ¶æŠ–åŠ¨ï¼Œå¼ºåŒ–é¿éšœ

### 3. åŠ¨ä½œå¹³æ»‘å¼ºåŒ–

**ä¿®æ”¹å‰**ï¼š`action_smoothness = 0.0001`ï¼ˆå‡ ä¹ä¸º0ï¼‰

**ä¿®æ”¹å**ï¼š`action_smoothness = -0.01`ï¼ˆæå‡100å€ï¼‰

**é¢„æœŸæ•ˆæœ**ï¼šæ²»æ„ˆPolicy Noise 17.30é—®é¢˜

---

## é¢„æœŸè®­ç»ƒæ›²çº¿

### Phase 1 (0-1000 iter): æ–°æ‰‹æœŸ
- Policy Noise: 1.0 â†’ 0.8
- reach_goal: 0% â†’ 20%ï¼ˆ3mè¿‘è·ç¦»å®¹æ˜“è¾¾æˆï¼‰
- target range: 3m Ã— 3m

### Phase 2 (1000-3000 iter): æˆé•¿æœŸ
- Policy Noise: 0.8 â†’ 0.5
- reach_goal: 20% â†’ 40%
- target range: 3m â†’ 8mï¼ˆè‡ªåŠ¨æ‰©å±•ï¼‰

### Phase 3 (3000-5000 iter): ä¸“å®¶æœŸ
- Policy Noise: 0.5 â†’ 0.3
- reach_goal: 40% â†’ 60%+
- target range: 8m Ã— 8mï¼ˆç¨³å®šï¼‰

---

## éªŒè¯æ–¹æ³•

### æˆåŠŸæ ‡å¿—

**ç­–ç•¥ç¨³å®šæ€§**ï¼š
- âœ… Policy Noise Std < 1.0ï¼ˆå…¨ç¨‹ç¨³å®šï¼‰
- âœ… æ— "çˆ†ç‚¸å¼"ä¸Šå‡

**ä»»åŠ¡å®Œæˆç‡**ï¼š
- âœ… reach_goal: 1000è½®æ—¶ > 20%
- âœ… reach_goal: 3000è½®æ—¶ > 40%
- âœ… reach_goal: 5000è½®æ—¶ > 60%

**è¡Œä¸ºè§‚å¯Ÿ**ï¼š
- âœ… æœºå™¨äººå¹³æ»‘ç§»åŠ¨ï¼Œæ— é«˜é¢‘æŠ–åŠ¨
- âœ… èƒ½å®Œæˆ3m â†’ 8mçš„æ¸è¿›å¼å¯¼èˆª
- âœ… æœ€ç»ˆèƒ½ç¨³å®šå®Œæˆ8mé•¿è·ç¦»å¯¼èˆª

### å¤±è´¥æ ‡å¿—

**ç­–ç•¥å†æ¬¡çˆ†ç‚¸**ï¼š
- âŒ Policy Noise > 5.0
- âŒ Noiseä¸Šå‡é€Ÿåº¦ > 0.1/iteration

**è¯¾ç¨‹å­¦ä¹ å¤±è´¥**ï¼š
- âŒ reach_goalå§‹ç»ˆ < 10%
- âŒ æ— æ³•ä»3mè¿‡æ¸¡åˆ°8m

---

## æŠ€æœ¯åˆè§„æ€§æ£€æŸ¥

**Isaac Lab API**ï¼š
- âœ… `CurriculumTermCfg`ï¼šå®˜æ–¹æ¨èè¯¾ç¨‹å­¦ä¹ æ–¹å¼
- âœ… `common_step_counter`ï¼šç‰©ç†æ­¥æ•°åŸºå‡†
- âœ… `mdp.rewards.position_command_error_tanh`ï¼šå®˜æ–¹tanhå¥–åŠ±å‡½æ•°

**ä»£ç è´¨é‡**ï¼š
- âœ… éµå¾ªDRYåŸåˆ™ï¼ˆå¤ç”¨æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼‰
- âœ… æ·»åŠ è¯¦ç»†æ–‡æ¡£å­—ç¬¦ä¸²
- âœ… ä¿æŒä¸ç°æœ‰ä»£ç é£æ ¼ä¸€è‡´

---

## ç›¸å…³æäº¤

- **æœ¬æ¬¡å®æ–½**: v5.0 Ultimateæ–¹æ¡ˆï¼ˆè‡ªåŠ¨è¯¾ç¨‹å­¦ä¹  + æ··åˆå¥–åŠ±ï¼‰
- **å‰åºç‰ˆæœ¬**: v3_robust_navï¼ˆç¨³å¥ç‰ˆé…ç½®ï¼‰
- **æ¶æ„å¸ˆå»ºè®®**: v4.0 Auto-Curriculum

---

## åç»­ä¼˜åŒ–æ–¹å‘

### å¦‚æœv5è®­ç»ƒæ•ˆæœç†æƒ³
1. è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–è¯¾ç¨‹å­¦ä¹ æ›²çº¿ï¼ˆéçº¿æ€§æ’å€¼ï¼‰
2. è°ƒæ•´ Dense å¥–åŠ±æƒé‡ï¼ˆæ‰¾åˆ°æœ€ä¼˜å¹³è¡¡ï¼‰
3. å®éªŒä¸åŒçš„reach_goalå¥–åŠ±å€¼ï¼ˆ2000.0 vs 1000.0 vs 500.0ï¼‰

### å¦‚æœv5è®­ç»ƒä¸ç†æƒ³
**Option A**: é™ä½reach_goalå¥–åŠ±
```python
reach_goal = 1000.0  # ä»2000é™åˆ°1000
```

**Option B**: å‡ç¼“è¯¾ç¨‹å­¦ä¹ é€Ÿåº¦
```python
end_step = 600_000_000  # ä»300Mæ­¥å»¶é•¿åˆ°600Mæ­¥
```

**Option C**: å›é€€åˆ°v3_robust_nav
```bash
git checkout <v3_commit_hash>
```

---

**åˆ›å»ºæ—¶é—´**: 2026-01-25 17:00:00
**ç»´æŠ¤è€…**: Claude Code AI System
**æ¶æ„å¸ˆè®¤è¯**: âœ… åŸºäºæ¶æ„å¸ˆv4.0 Auto-Curriculumå»ºè®®
**çŠ¶æ€**: âœ… å·²å®æ–½ï¼ˆç­‰å¾…è®­ç»ƒéªŒè¯ï¼‰
**ä¸‹ä¸€æ­¥**: å¯åŠ¨è®­ç»ƒï¼Œç›‘æ§Policy Noiseå’Œreach_goalæ›²çº¿
