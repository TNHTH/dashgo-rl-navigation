# 避障策略优化 - 实施业界标准三层防御体系

> **创建时间**: 2026-01-27 16:00:00
> **严重程度**: 🟢 功能优化
> **状态**: ✅ 已实施
> **相关文件**: dashgo_env_v2.py, verify_collision.py

---

## 🎯 问题背景

### 用户核心关切

用户提出了一个非常专业的问题：

> "那种擦到、碰到怎么算？这种只计算猛烈的碰撞不是会导致小车很容易撞到障碍物周围吗？"

**深层担忧**：
1. **贴墙走（Wall Hugging）**：如果只惩罚猛烈碰撞，小车会发现贴墙走路径最短
2. **冻结机器人（Frozen Robot）**：如果惩罚太重，小车在窄路会直接停车等待超时
3. **直接撞（Suicide）**：如果重置惩罚不够大，小车可能觉得"直接撞死"比"慢慢蹭"更划算

### 架构师诊断

> 这正是强化学习落地中最经典的"坑"：**Reward Hacking（奖励黑客）**
>
> 如果只依靠"猛烈碰撞重置"，强化学习会产生：
> - 贴墙走：擦碰不算，所以贴着墙蹭过去
> - 冻结：窄路扣分太多，不如停车等待超时

---

## ✅ 解决方案：三层防御体系

基于 **ETH Zurich RSL-RL**、**OpenAI Navigation**、**ROS2 Nav2** 的业界最佳实践。

### 第一层防御：调低碰撞阈值（更敏感的皮肤）

**修改位置**：`TerminationCfg` 第1202-1205行

**修改前**：
```python
object_collision = TerminationTermCfg(
    func=check_collision_simple,
    params={"threshold": 150.0}  # 太不敏感了
)
```

**修改后**：
```python
object_collision = TerminationTermCfg(
    func=check_collision_simple,
    params={"threshold": 50.0}  # ✅ 降低到50N，更敏感
)
```

**效果**：
- 轻轻蹭墙也会触发重置（50N ≈ 5kg物体的重力）
- 防止"贴墙走"和"擦边球"

---

### 第二层防御：增加碰撞惩罚但不重置（喊疼）

**新增奖励函数**：`penalty_undesired_contacts()`

**核心逻辑**：
```python
def penalty_undesired_contacts(env, sensor_cfg, threshold=0.1):
    # 获取接触力
    force_mag = torch.norm(contact_data, dim=-1)

    # 任何超过0.1N的接触都扣分
    has_contact = force_mag > 0.1

    return -1.0 if has_contact else 0.0
```

**配置**：
```python
undesired_contacts = RewardTermCfg(
    func=penalty_undesired_contacts,
    weight=-1.0,  # 轻微扣分，可忍受
    params={"threshold": 0.1}
)
```

**效果**：
- 擦碰扣1分（小痛），但不重置
- 让机器人学会"别碰我"，但不会因为轻轻蹭一下就死

---

### 第三层防御：虚拟防撞垫（最核心的方案）

**新增奖励函数**：`penalty_unsafe_speed()` - **速度-距离动态约束**

**核心逻辑**："离得近没关系，但离得近还**跑得快**，就是找死。"

**数学公式**：
```
safe_vel_limit = clamp(min_dist, max=0.5)
overspeed = clamp(vel - safe_vel_limit, min=0.0)
penalty = -overspeed
```

**人类逻辑**：
- 宽阔马路（0.5m）：允许0.5m/s
- 窄窄通道（0.1m）：只允许0.05m/s
- 只要减速了，惩罚就变小

**代码实现**：
```python
def penalty_unsafe_speed(env, asset_cfg, min_dist_threshold=0.25):
    # 1. 获取最小雷达距离
    min_dist = torch.min(all_dist, dim=1)[0]

    # 2. 获取当前速度
    vel = env.scene["robot"].data.root_lin_vel_b[:, 0]

    # 3. 定义安全速度极限
    safe_vel_limit = torch.clamp(min_dist, max=0.5)

    # 4. 计算超速量
    overspeed = torch.clamp(vel - safe_vel_limit, min=0.0)

    # 5. 超速惩罚
    return -overspeed
```

**配置**：
```python
unsafe_speed_penalty = RewardTermCfg(
    func=penalty_unsafe_speed,
    weight=-5.0,  # 中等扣分
    params={"min_dist_threshold": 0.25}
)
```

**效果**：
- 机器人在窄处会**自动减速**
- 不会"冻结"（减速后仍可通过）
- 不会"贴墙走"（太近了必须减速，扣分不划算）

---

## 📊 奖励权重平衡（关键）

按照架构师建议配置的**黄金比例**：

| 奖励项 | 权重 | 作用 | 设计理念 |
|--------|------|------|----------|
| **reach_goal** | **+2000.0** | 终点大奖 | 绝对主导，确保"到达"是全局最优 |
| **shaping_distance** | **+0.75** | 方向引导 | tanh限制单步收益，防止刷分 |
| **collision** | **-200.0** | 撞击最痛 | 比所有过程扣分加起来都痛 |
| **unsafe_speed_penalty** | **-5.0** | 窄处超速 | 中等惩罚，鼓励减速 |
| **undesired_contacts** | **-1.0** | 擦碰小痛 | 轻微提醒，可忍受 |

**机器人心理活动**：
> "前面的路很窄（雷达检测到了）。如果我全速冲过去，会触发 `unsafe_speed_penalty` 扣我50分，太亏了。
> 如果我停下不走，拿不到 `reach_goal` 的2000分，也亏。
> **最佳策略**：我应该**减速**，慢慢蹭过去。虽然可能会触发一点点 `undesired_contacts` 扣1分，但只要过去了就能拿2000分。划算！冲！"

---

## 🧪 验证方法

### 1. 碰撞验证脚本

**文件**：`verify_collision.py`

**功能**：
- 全速前进直到撞墙
- 检查是否触发 done=True
- 分析重置原因（碰撞/翻车/超时）

**运行命令**：
```bash
~/IsaacLab/isaaclab.sh -p verify_collision.py --headless --enable_cameras
```

**预期输出**（成功）：
```
Step 40: 速度=0.28 m/s | 接触力=12.3421 N | Done=False
...
🛑 [检测到重置] 在 Step 48 触发！
--------------------------------------------------
🕵️‍♂️ 重置原因取证:
   > 碰撞 (object_collision): 1.0  ← 关键看这里
   > 翻车 (base_height):      0.0
   > 超时 (time_out):         0.0
--------------------------------------------------
✅ 验证成功：系统检测到了碰撞并触发了重置！
```

---

### 2. 擦碰验证（手动）

**修改 `verify_collision.py`**：
```python
# 改为低速前进
action = torch.tensor([[0.05, 0.0]], device=env.device)  # 0.05 m/s
```

**预期**：
- 轻轻蹭墙：触发 `undesired_contacts` 扣分，但不重置
- 接触力：0.1N ~ 50N 之间

---

### 3. 窄路验证（课程学习）

**设置**：
- 训练后期，目标范围扩展到最大
- 场景中放置窄门（宽度0.6m）

**预期**：
- 机器人进入窄门时**自动减速**
- 不会选择"直接撞"
- 不会"冻结"在窄路入口

---

## 🎓 业界参考

### 方案对比

| 方案 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **方案一：速度-距离约束** | 符合人类驾驶习惯 | 需要调参 | ✅ 推荐（本方案） |
| **方案二：指数势场** | 平滑梯度 | 计算复杂 | 简单场景 |
| **方案三：课程学习** | 稳定训练 | 需要多阶段 | 长期训练 |

### GitHub 参考项目

1. **ETH Zurich - RSL-RL**
   - 仓库：leggedrobotics/rsl_rl
   - 特点：使用速度-距离约束
   - 应用：ANYmal 四足机器人

2. **OpenAI - Navigation**
   - 仓库：openai/gym-navigation
   - 特点：稀疏奖励 + 势能辅助
   - 应用：2D 导航

3. **ROS2 Nav2**
   - 仓库：ros-planning/navigation2
   - 特点：DWB + Recovery Behaviors
   - 应用：实车部署

---

## 📝 经验教训

### 1. 奖励黑客（Reward Hacking）

**教训**：简单的碰撞阈值会导致"贴墙走"
- 机器人发现：贴墙走路径最短，而且不触发重置
- 后果：车漆被蹭掉，卡在桌子腿里

### 2. 冻结机器人（Frozen Robot）

**教训**：惩罚太重会导致"躺平"
- 机器人发现：窄路扣分太多，不如停车等超时
- 后果：永远到不了目标

### 3. 权重平衡是关键

**教训**：必须确保目标奖励 > 所有惩罚之和
- reach_goal (2000) > collision (200) + 其他
- 否则机器人会觉得"活着比到达更重要"

---

## 🚀 实施记录

### 修改文件

**主要修改**：
- `dashgo_env_v2.py`：添加两层防御 + 调整阈值
- `verify_collision.py`：创建碰撞验证脚本

### 新增奖励函数

1. **penalty_unsafe_speed()**：速度-距离动态约束
2. **penalty_undesired_contacts()**：轻微接触惩罚

### 配置调整

1. **碰撞阈值**：150N → 50N
2. **碰撞惩罚**：-50.0 → -200.0
3. **新增奖励**：unsafe_speed_penalty (-5.0), undesired_contacts (-1.0)

### 相关提交

- commit: 1ac8705 (2026-01-27)

---

## 🔧 后续步骤

### 立即执行

1. **运行碰撞验证**：
   ```bash
   ~/IsaacLab/isaaclab.sh -p verify_collision.py --headless --enable_cameras
   ```

2. **观察训练表现**：
   - 检查窄路行为（是否自动减速）
   - 检查是否还有"贴墙走"
   - 检查是否"冻结"在窄路入口

3. **微调参数**（如果需要）：
   - 如果还是贴墙：提升 unsafe_speed_penalty 权重（-5.0 → -10.0）
   - 如果经常冻结：降低 collision 惩罚（-200.0 → -100.0）

### 长期优化（可选）

1. **添加课程学习**：Level 1 宽路 → Level 10 窄路
2. **添加指数势场**：更平滑的距离-惩罚曲线
3. **添加恢复行为**：检测到冻结时，执行原地旋转脱困

---

## 📚 相关文档

- **对话记录**：用户与架构师关于"擦碰"问题的讨论
- **验证脚本**：`verify_collision.py`
- **实施记录**：`issues/2026-01-27_Geo-Distill-V2.2实施记录.md`
- **方案来源**：ETH Zurich RSL-RL, OpenAI Navigation, ROS2 Nav2

---

**维护者**: Claude Code AI System (Robot-Nav-Architect Agent)
**项目**: DashGo机器人导航（Sim2Real）
**开发基准**: Isaac Sim 4.5 + Ubuntu 20.04
**状态**: ✅ 已实施，待验证
