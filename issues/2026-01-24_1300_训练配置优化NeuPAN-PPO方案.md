# 训练配置优化：NeuPAN-PPO 方案

> **创建时间**: 2026-01-24
> **问题类型**: 优化
> **优先级**: 高
> **状态**: ⚠️ 已修改，待测试验证

---

## 📋 问题描述

根据 Isaac Sim 架构师的 Sim2Real 部署建议，对训练配置进行全面优化，采用 **NeuPAN-PPO 混合方案**。

**核心策略**：用 PPO 的强健体魄去执行 NeuPAN 的敏捷思想

---

## 🔍 问题诊断

### 当前配置的局限

1. **学习率保守**（1e-4）
   - 收敛速度慢
   - 可能陷入局部最优

2. **采样步数过多**（480）
   - 每个episode过长
   - 可能导致经验过时

3. **缺少自适应学习率**
   - 训练后期可能震荡
   - 无保险丝机制

4. **输入未归一化**
   - 雷达大数值（12.0）可能导致梯度问题

---

## 🎯 解决方案

### 架构师的三大调整

#### 1. 网络加深 (Deep Network)
```yaml
actor_hidden_dims: [512, 256, 128]  # ✅ 更深，拟合复杂避障逻辑
critic_hidden_dims: [512, 256, 128]
```
**收益**：更好地拟合激光雷达点云与避障动作之间的非线性关系

#### 2. 激活函数 ELU
```yaml
activation: 'elu'  # ✅ 比 ReLU 更平滑
```
**收益**：输出平滑，减少电机抖动指令，延长实机寿命

#### 3. 自适应学习率 (Adaptive LR)
```yaml
schedule: "adaptive"  # ✅ 防止训练崩盘的保险丝
```
**收益**：KL散度飙升时自动降低学习率，防止遗忘

---

## ✅ 实施的修改

### 文件：train_cfg_v2.yaml

| 参数 | 修改前 | 修改后 | 变化 | 风险 |
|------|--------|--------|------|------|
| **learning_rate** | 1e-4 | 1e-3 | ⬆️ 10倍 | 🔴 高 |
| **num_steps_per_env** | 480 | 24 | ⬇️ 20倍 | 🔴 高 |
| **entropy_coef** | 0.01 | 0.005 | ⬇️ 2倍 | 🟡 中 |
| **init_noise_std** | 0.8 | 1.0 | ⬆️ 25% | 🟢 低 |
| **empirical_normalization** | False | True | 新增 | 🟢 低 |
| **schedule** | 无 | "adaptive" | 新增 | 🟢 低 |
| **max_iterations** | 10000 | 1500 | ⬇️ 6.7倍 | 🟡 中 |
| **actor_hidden_dims** | [512,256,128] | [512,256,128] | 保持 | - |
| **activation** | 'elu' | 'elu' | 保持 | - |

### 保留内容（用户明确要求）

- ✅ **朝向奖励**：权重保持 0.1（未删除）

---

## ⚠️ 风险评估与缓解

### 风险1：学习率过高（1e-3）

**风险描述**：
- 提高到10倍可能导致训练崩溃
- 梯度爆炸
- 策略震荡

**缓解措施**：
1. **自适应学习率**（已启用）：`schedule: "adaptive"`
   - KL散度超过 0.01 时自动降低学习率
2. **梯度裁剪**（已启用）：`max_grad_norm: 1.0`
   - 防止梯度爆炸
3. **监控指标**：
   ```bash
   # 观察训练日志
   # 如果 reward 持续下降或剧烈震荡 → 立即停止
   ```

**备选方案**：
- 如果训练崩溃，降低到 `5e-4`（折中值）
- 如果仍不稳定，回退到 `1e-4`（原值）

---

### 风险2：采样步数过少（24）

**风险描述**：
- 每个环境只采样24步（约1.6秒@15fps）
- 经验可能不足
- 需要大量并行环境

**采样量计算**：
```
架构师假设：
- num_envs: 4096
- num_steps_per_env: 24
- 总采样：24 × 4096 = 98,304 steps/iteration

当前实际情况：
- num_envs: 80（用户训练命令）
- num_steps_per_env: 24
- 总采样：24 × 80 = 1,920 steps/iteration

差距：98,304 / 1,920 = 51倍！
```

**缓解措施**：
1. **增加并行环境**：
   ```bash
   # 建议至少512个环境
   DISPLAY= ~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 512
   ```

2. **增加采样步数**（如果显存不足）：
   ```yaml
   # 修改 train_cfg_v2.yaml
   num_steps_per_env: 120  # 5倍增加（24 → 120）
   ```

**推荐配置**：
| 并行环境数 | 采样步数 | 总采样/iteration | 显存需求 |
|-----------|----------|------------------|----------|
| 80 | 120 | 9,600 | ~4GB |
| 256 | 60 | 15,360 | ~8GB |
| 512 | 24 | 12,288 | ~12GB |
| 1024 | 24 | 24,576 | ~16GB |
| 4096 | 24 | 98,304 | ~32GB |

---

### 风险3：熵系数降低（0.005）

**风险描述**：
- 探索不足
- 可能陷入局部最优

**缓解措施**：
- 监控训练是否"原地不动"
- 如果不动，提高回 0.01

---

## 🚀 启动命令

### 推荐配置（根据显存）

#### 4GB 显存
```bash
DISPLAY= ~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 128
```

#### 8GB 显存
```bash
DISPLAY= ~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 256
```

#### 12GB+ 显存（架构师推荐）
```bash
DISPLAY= ~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 512
```

#### 32GB+ 显存（完全符合架构师假设）
```bash
DISPLAY= ~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 4096
```

---

## 📊 监控指标

### 训练过程关键指标

1. **reward**：
   - ✅ 期望：从负数逐渐变大，最后稳定在正数
   - ❌ 警告：持续下降或卡在负值不动

2. **episode length**：
   - ✅ 期望：逐渐变长（机器人存活时间变长）
   - ❌ 警告：没有明显增长

3. **KL divergence**：
   - ✅ 期望：在 0.01 附近波动
   - ❌ 警告：持续飙升（说明学习率过高）

4. **learning_rate**：
   - ✅ 期望：根据KL自适应调整
   - ❌ 警告：持续不变（adaptive未生效）

### 异常情况处理

| 现象 | 原因 | 解决方案 |
|------|------|----------|
| Reward持续下降 | 学习率过高 | 降低到 5e-4 或 1e-4 |
| 机器人原地不动 | 探索不足 | 提高 entropy_coef 到 0.01 |
| Reward卡在负值 | 碰撞惩罚过大 | 降低 collision_penalty weight |
| 训练崩溃（NaN） | 梯度爆炸 | 降低学习率，检查 max_grad_norm |

---

## 📝 验证方法

### 1. 语法检查
```bash
python -m py_compile train_cfg_v2.yaml
# 或使用 YAML 验证工具
python -c "import yaml; yaml.safe_load(open('train_cfg_v2.yaml'))"
```

### 2. 配置检查
```bash
# 打印配置确认
python -c "
import yaml
cfg = yaml.safe_load(open('train_cfg_v2.yaml'))
print('learning_rate:', cfg['algorithm']['learning_rate'])
print('num_steps_per_env:', cfg['runner']['num_steps_per_env'])
print('empirical_normalization:', cfg['runner']['empirical_normalization'])
print('schedule:', cfg['algorithm'].get('schedule'))
"
```

### 3. 训练测试
```bash
# 先训练100次迭代快速验证
# 修改 train_cfg_v2.yaml
max_iterations: 100

# 启动训练
DISPLAY= ~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 256

# 观察指标：
# - reward 是否上升？
# - 是否出现崩溃？
# - KL散度是否正常？
```

---

## 🔄 回退方案

### 如果训练失败，逐步回退

#### 步骤1：降低学习率
```yaml
# train_cfg_v2.yaml
learning_rate: 5.0e-4  # 从 1e-3 降低到 5e-4
```

#### 步骤2：增加采样步数
```yaml
# train_cfg_v2.yaml
num_steps_per_env: 120  # 从 24 增加到 120
```

#### 步骤3：完全回退
```yaml
# train_cfg_v2.yaml
learning_rate: 1.0e-4       # 回退到原值
num_steps_per_env: 480      # 回退到原值
entropy_coef: 0.01          # 回退到原值
empirical_normalization: True  # 保留这个改进
schedule: "adaptive"        # 保留这个改进
```

---

## 📚 参考资料

1. **架构师建议文档**：
   - `docs/architect-recommendations/sim2real-deployment-strategy_2026-01-24.md`
   - `docs/architect-recommendations/training-config-comparison_2026-01-24.md`

2. **NeuPAN 论文**：
   - 端到端导航，感知降维
   - 参考其网络设计和奖励函数

3. **RSL-RL 文档**：
   - PPO算法实现
   - 自适应学习率机制

---

## 🎓 经验总结

### 关键要点

1. **采样量比学习率更重要**
   - 24步 × 80环境 = 1920步（太少）
   - 必须增加并行环境或采样步数

2. **自适应学习率是保险丝**
   - KL散度监控至关重要
   - 防止训练后期震荡

3. **输入归一化防止梯度爆炸**
   - 雷达数值(12.0)比其他观测大100倍
   - empirical_normalization 必须开启

4. **网络加深需要更多数据**
   - [512,256,128] 需要 sufficient samples
   - 采样不足会过拟合

### 后续优化

- [ ] 测试不同并行环境数（128, 256, 512, 1024）
- [ ] 测试不同学习率（1e-4, 5e-4, 1e-3）
- [ ] 对比收敛速度和最终性能
- [ ] 记录最佳配置

---

**维护者**: Claude Code AI Assistant
**最后更新**: 2026-01-24
**状态**: ⚠️ 已修改，待训练验证
**下一步**: 启动训练并监控指标
