# "é†‰æ±‰èµ°è·¯"é—®é¢˜è¯Šæ–­æŠ¥å‘Š

> **åˆ›å»ºæ—¶é—´**: 2026-01-26 05:20
> **ä¸¥é‡ç¨‹åº¦**: ğŸ”´ ä¸¥é‡ï¼ˆè®­ç»ƒå®Œæˆçš„æ¨¡å‹è¡Œä¸ºå¼‚å¸¸ï¼‰
> **çŠ¶æ€**: ğŸ” é—®é¢˜å·²å®šä½ï¼Œå¾…æ¶æ„å¸ˆç¡®è®¤ä¿®å¤æ–¹æ¡ˆ
> **æŠ¥å‘Šè€…**: Claude Code AI System

---

## ğŸ¯ é—®é¢˜ç°è±¡

### ç”¨æˆ·æè¿°

è®­ç»ƒå®Œæˆçš„ DashGo æœºå™¨äººå¯¼èˆªç­–ç•¥å‡ºç°ä¸¥é‡çš„"é†‰æ±‰èµ°è·¯"ç°è±¡ï¼š
- æœºå™¨äººä¸€ç›´å¸¦ç€ä¸€å®šè§’é€Ÿåº¦è½¬åœˆï¼Œä¸èµ°ç›´çº¿
- æ„Ÿè§‰ä¸çŸ¥é“ç›®æ ‡åœ¨å“ªï¼Œä¸€ç›´åœ¨ä¹±é€›
- è®­ç»ƒè™½ç„¶æ”¶æ•›ï¼ˆè¾¾åˆ°5000 iterationsï¼‰ï¼Œä½†éƒ¨ç½²æ•ˆæœå®Œå…¨ä¸ç¬¦åˆé¢„æœŸ

### æµ‹è¯•æ•°æ®å¯¹æ¯”

| æµ‹è¯•æ¨¡å‹ | è®­ç»ƒè¿›åº¦ | åŠ¨ä½œèŒƒå›´ï¼ˆç½‘ç»œè¾“å‡ºï¼‰ | æœºå™¨äººè¡Œä¸º |
|---------|---------|---------------------|-----------|
| model_1900.pt | 38% (1900 iter) | v=Â±9.0, w=Â±7.5 | æŒç»­åé€€ï¼Œè¿œç¦»ç›®æ ‡ |
| model_4999.pt | 100% (4999 iter) | v=Â±10, w=Â±13 | **æŒç»­åé€€ï¼Œè¿œç¦»ç›®æ ‡** âŒ |
| **æ­£å¸¸é¢„æœŸ** | - | v=Â±0.3, w=Â±1.0 | æœç›®æ ‡ç§»åŠ¨ï¼Œåˆ°è¾¾ååœæ­¢ |

**å…³é”®å‘ç°**ï¼š
- âœ… ç‰©ç†å‚æ•°æ­£å¸¸ï¼ˆå¼ºåˆ¶èµ°ç›´çº¿æµ‹è¯•é€šè¿‡ï¼‰
- âœ… åæ ‡ç³»è®¡ç®—æ­£å¸¸ï¼ˆç›®æ ‡è§’åº¦ç¬¦å·æ­£ç¡®ï¼‰
- âŒ **è®­ç»ƒæ¨¡å‹å·²æŸå**ï¼ˆæœ€æ–°æ¨¡å‹ä¹Ÿæœ‰åŒæ ·é—®é¢˜ï¼‰
- âŒ æœºå™¨äººå­¦ä¼šäº†"è¿œç¦»ç›®æ ‡"çš„ç­–ç•¥ï¼ˆä¸é¢„æœŸç›¸åï¼‰

---

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### åŸå› 1ï¼šå¥–åŠ±å‡½æ•°è®¾è®¡é—®é¢˜ â­

#### é—®é¢˜ä»£ç ï¼š`reward_target_speed` å‡½æ•°å®šä¹‰å†²çª

**å‘ç°ä½ç½®**: `dashgo_env_v2.py` (ç¬¬1973-1998è¡Œ)

```python
# å®šä¹‰1ï¼ˆæ—§ç‰ˆæœ¬ï¼‰- åªé¼“åŠ±å‰è¿›
def reward_target_speed(env, asset_cfg):
    """
    [v5.0] é€Ÿåº¦å¯¹é½å¥–åŠ±ï¼šé¼“åŠ±ä½¿ç”¨æ¥è¿‘æœ€ä¼˜é€Ÿåº¦çš„é€Ÿåº¦
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥é¼“åŠ±å‘å‰ç§»åŠ¨ï¼Œæ›´ç®€å•ç›´æ¥
    """
    vel = env.scene[asset_cfg.name].data.root_lin_vel_b[:, 0]
    return torch.clamp(vel, min=0.0)  # âœ… åªå¥–åŠ±æ­£é€Ÿåº¦ï¼ˆå‰è¿›ï¼‰

# å®šä¹‰2ï¼ˆæ–°ç‰ˆæœ¬ï¼‰- åŒ¹é…ç›®æ ‡é€Ÿåº¦ï¼ˆä¸ç®¡æ–¹å‘ï¼ï¼‰
def reward_target_speed(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """
    é€Ÿåº¦åŒ¹é…å¥–åŠ±ï¼ˆåæ¥æ·»åŠ çš„ç‰ˆæœ¬ï¼Ÿï¼‰
    """
    robot = env.scene[asset_cfg.name]
    lin_vel_b = torch.nan_to_num(robot.data.root_lin_vel_b[:, 0], nan=0.0, posinf=0.0, neginf=0.0)
    target_vel = 0.25
    speed_match = 1.0 - torch.abs(lin_vel_b - target_vel) / target_vel
    return torch.clamp(speed_match, 0.0, 0.2)  # âŒ å¥–åŠ±ä»»ä½•æ–¹å‘çš„é€Ÿåº¦ï¼
```

**é—®é¢˜åˆ†æ**ï¼š
- ç¬¬äºŒä¸ªå®šä¹‰é¼“åŠ±æœºå™¨äººä»¥ **0.25 m/s** çš„é€Ÿåº¦ç§»åŠ¨
- **ä½†ä¸ç®¡æœå‘**ï¼å¦‚æœæœºå™¨äººæœåç§»åŠ¨ï¼Œä¹Ÿä¼šå¾—åˆ°å¥–åŠ±
- è¿™å¯¼è‡´æœºå™¨äººå­¦ä¼šäº†"å¿«é€Ÿåé€€åˆ·åˆ†"çš„ç­–ç•¥

#### å¥–åŠ±é…ç½®

**å‘ç°ä½ç½®**: `dashgo_env_v2.py` (ç¬¬983-987è¡Œ)

```python
target_speed = RewardTermCfg(
    func=reward_target_speed,  # âš ï¸ ä½¿ç”¨çš„æ˜¯å“ªä¸ªå®šä¹‰ï¼Ÿ
    weight=1.0,  # æƒé‡ä¸ä½ï¼Œä¼šå½±å“ç­–ç•¥
    params={"asset_cfg": SceneEntityCfg("robot")}
)
```

#### å½“å‰å¥–åŠ±æƒé‡åˆ†å¸ƒ

```python
+----------------------------------------+
|          Active Reward Terms           |
+-------+-----------------------+--------+
| Index | Name                  | Weight |
+-------+-----------------------+--------+
|   0   | reach_goal            | 2000.0 |  # ç¨€ç–å¥–åŠ±
|   1   | shaping_distance      |   0.75 |  # Denseå¼•å¯¼
|   2   | target_speed          |    1.0 |  # âš ï¸ é—®é¢˜é¡¹
|   3   | facing_goal           |    0.1 |  # è¾…åŠ©
|   4   | velodyne_style_reward |    1.0 |  # Dense
+-------+-----------------------+--------+
```

**å†²çªåˆ†æ**ï¼š
- `target_speed` (weight=1.0) é¼“åŠ±å¿«é€Ÿç§»åŠ¨ï¼ˆä¸ç®¡æ–¹å‘ï¼‰
- `shaping_distance` (weight=0.75) é¼“åŠ±é è¿‘ç›®æ ‡
- å½“ä¸¤è€…å†²çªæ—¶ï¼Œ`target_speed` å¯èƒ½è·èƒœï¼ˆæ›´å®¹æ˜“åˆ·åˆ†ï¼‰
- ç»“æœï¼šæœºå™¨äººå­¦ä¼š"å¿«é€Ÿåé€€"è€Œä¸æ˜¯"é è¿‘ç›®æ ‡"

---

### åŸå› 2ï¼šè§‚æµ‹å½’ä¸€åŒ–å±‚å¯èƒ½ä¸­æ¯’

#### é—®é¢˜æœºåˆ¶

**è®­ç»ƒåˆæœŸ**ï¼ˆå‰100-200 iterationsï¼‰ï¼š
```python
# æœºå™¨äººç­–ç•¥ï¼šç–¯ç‹‚å‘æŠ–åˆ·åˆ†
Episode 1-10: æœºå™¨äººåŠ¨ä½œ
  lin_vel: [-8.0, +7.5, -9.2, +8.8, ...]  â† æç«¯å€¼
  ang_vel: [-6.5, +7.0, -7.3, +6.9, ...]  â† æç«¯å€¼

# RunningMeanStd è®°å½•äº†è¿™äº›æç«¯ç»Ÿè®¡é‡ï¼š
mean â‰ˆ 0.0 (æ­£è´ŸæŠµæ¶ˆ)
var  â‰ˆ 50.0 (æç«¯æ–¹å·®ï¼)
std  â‰ˆ 7.0 (æç«¯æ ‡å‡†å·®ï¼)
```

**è®­ç»ƒä¸­æœŸ**ï¼ˆ200-1000 iterationsï¼‰ï¼š
```python
# ç­–ç•¥é€æ¸å­¦ä¼šæ­£å¸¸å¯¼èˆª
  lin_vel: [0.2, 0.25, 0.18, ...]  â† æ­£å¸¸å€¼
  ang_vel: [0.5, -0.3, 0.4, ...]   â† æ­£å¸¸å€¼

# ä½†å½’ä¸€åŒ–ç»Ÿè®¡é‡å·²ç»"è„"äº†ï¼š
  var  â‰ˆ 20.0 (ä»ç„¶å¾ˆé«˜ï¼)
  std  â‰ˆ 4.5 (ä»ç„¶å¾ˆé«˜ï¼)
```

**æ¨ç†æ—¶**ï¼ˆå½“å‰ï¼‰ï¼š
```python
# æ­£å¸¸è§‚æµ‹
obs = 0.1  # m/sï¼ˆå®é™…é€Ÿåº¦ï¼‰

# å½’ä¸€åŒ–åï¼ˆè¢«æ±¡æŸ“çš„ç»Ÿè®¡é‡ï¼‰
normalized_obs = (0.1 - 0.0) / 4.5 = 0.022  # â† è¢«ä¸¥é‡ç¼©å°ï¼

# ç¥ç»ç½‘ç»œè®¤ä¸ºæ˜¯"å¼‚å¸¸å°"çš„é€Ÿåº¦
# è¾“å‡ºæç«¯åŠ¨ä½œæ¥"çº æ­£"è¿™ä¸ª"å¼‚å¸¸"
action_v = 9.0  # m/s (è¯•å›¾åŠ é€Ÿï¼)
action_w = -7.5 # rad/s (è¯•å›¾è½¬å‘ï¼)
```

#### æµ‹è¯•éªŒè¯

**no_norm æµ‹è¯•**ï¼ˆå…³é—­å½’ä¸€åŒ–ï¼‰ï¼š
- ç½‘ç»œè¾“å‡ºï¼šv=Â±200, w=Â±150ï¼ˆ**æ›´ä¸¥é‡**ï¼‰
- ç»“è®ºï¼šå½’ä¸€åŒ–å±‚ç¡®å®æœ‰é—®é¢˜ï¼Œä½†**ä¸æ˜¯å”¯ä¸€åŸå› **

---

## ğŸ“Š å®Œæ•´æµ‹è¯•æ•°æ®

### æµ‹è¯•1ï¼šç‰©ç†å‚æ•°éªŒè¯ âœ…

**å‘½ä»¤**: `python debug_play.py --test straight_line`

**ç»“æœ**:
```
Episode 1: yawä¿æŒ 0.00radï¼ˆæ— æŒç»­è½¬å‘ï¼‰
Episode 2: yawä¿æŒ 0.00radï¼ˆæ— æŒç»­è½¬å‘ï¼‰
Episode 3: yawä¿æŒ 0.00radï¼ˆæ— æŒç»­è½¬å‘ï¼‰
```

**ç»“è®º**: ç‰©ç†å‚æ•°æ­£å¸¸ï¼ŒURDF/æ‘©æ“¦åŠ›æ— é—®é¢˜

---

### æµ‹è¯•2ï¼šåæ ‡ç³»éªŒè¯ âœ…

**å‘½ä»¤**: `python debug_play.py --test print_obs`

**ç»“æœ**:
```
[Step 0000] Î¸=0.51rad (å·¦å‰æ–¹) â†’ æœºå™¨äººåœ¨å·¦ä¾§
[Step 0130] Î¸=-3.10rad (æ¥è¿‘Â±Ï€) â†’ æœºå™¨äººè½¬è¿‡è¾¹ç•Œ
[Step 0990] Î¸=3.02rad (æ¥è¿‘Â±Ï€) â†’ æ­£å¸¸çš„æåæ ‡è·³å˜
```

**ç»“è®º**: ç›®æ ‡è§’åº¦è®¡ç®—æ­£å¸¸ï¼Œç¬¦å·æ­£ç¡®

---

### æµ‹è¯•3ï¼šå½’ä¸€åŒ–å½±å“éªŒè¯ âŒ

**å‘½ä»¤**: `python debug_play.py --test no_norm`

**ç»“æœ**:
| æ¨¡å¼ | åŠ¨ä½œ v èŒƒå›´ | åŠ¨ä½œ w èŒƒå›´ | æœºå™¨äººè¡Œä¸º |
|------|------------|------------|-----------|
| å¼€å¯å½’ä¸€åŒ– | v=Â±9.0 | w=Â±7.5 | åé€€ï¼Œè¿œç¦»ç›®æ ‡ |
| å…³é—­å½’ä¸€åŒ– | v=Â±200 | w=Â±150 | **æ›´ä¸¥é‡**ï¼ˆçˆ†ç‚¸ï¼‰ |

**ç»“è®º**:
- âŒ å½’ä¸€åŒ–å±‚å·²ä¸­æ¯’
- âŒ å…³é—­å½’ä¸€åŒ–æ›´ç³Ÿï¼ˆè¯´æ˜ç½‘ç»œä¾èµ–å½’ä¸€åŒ–ï¼‰
- âš ï¸ ä½†**ä¸æ˜¯æ ¹æœ¬åŸå› **ï¼ˆå¥–åŠ±å‡½æ•°é—®é¢˜æ›´ä¸¥é‡ï¼‰

---

### æµ‹è¯•4ï¼šæœ€æ–°æ¨¡å‹éªŒè¯ âŒ

**å‘½ä»¤**: `python debug_play.py --test print_obs --checkpoint logs/model_4999.pt`

**ç»“æœ**:
```
[Step 0010] v=9.71,  w=-6.64  | ç›®æ ‡: d=0.61m
[Step 0590] v=11.19, w=-0.93 | ç›®æ ‡: d=2.24m  â† è¶Šæ¥è¶Šè¿œï¼
[Step 1150] v=-8.64, w=13.83 | ç›®æ ‡: d=2.54m  â† æŒç»­åé€€ï¼
```

**ç»“è®º**:
- âŒ æœ€æ–°æ¨¡å‹ï¼ˆ100%è®­ç»ƒï¼‰ä¹Ÿæœ‰åŒæ ·é—®é¢˜
- âŒ ç­–ç•¥å·²ç»æ”¶æ•›åˆ°"è¿œç¦»ç›®æ ‡"
- âœ… è¯´æ˜ä¸æ˜¯"è®­ç»ƒä¸è¶³"ï¼Œè€Œæ˜¯"å¥–åŠ±å‡½æ•°é”™è¯¯"

---

## ğŸ› ï¸ ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šä¿®å¤å¥–åŠ±å‡½æ•° + é‡æ–°è®­ç»ƒï¼ˆæ¨èï¼‰âœ…

#### æ­¥éª¤1ï¼šä¿®å¤ `reward_target_speed` å‡½æ•°

**æ–‡ä»¶**: `dashgo_env_v2.py`

**ä¿®æ”¹**: åˆ é™¤ç¬¬äºŒä¸ªå®šä¹‰ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª

```python
# ä¿®æ”¹å‰ï¼ˆç¬¬1973-1998è¡Œï¼‰
def reward_target_speed(env, asset_cfg):
    """[v5.0] é€Ÿåº¦å¯¹é½å¥–åŠ±ï¼šé¼“åŠ±ä½¿ç”¨æ¥è¿‘æœ€ä¼˜é€Ÿåº¦çš„é€Ÿåº¦"""
    vel = env.scene[asset_cfg.name].data.root_lin_vel_b[:, 0]
    return torch.clamp(vel, min=0.0)

def reward_target_speed(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """é€Ÿåº¦åŒ¹é…å¥–åŠ±ï¼ˆåæ¥æ·»åŠ çš„ç‰ˆæœ¬ï¼Ÿï¼‰"""
    robot = env.scene[asset_cfg.name]
    lin_vel_b = torch.nan_to_num(robot.data.root_lin_vel_b[:, 0], nan=0.0, posinf=0.0, neginf=0.0)
    target_vel = 0.25
    speed_match = 1.0 - torch.abs(lin_vel_b - target_vel) / target_vel
    return torch.clamp(speed_match, 0.0, 0.2)

# ä¿®æ”¹åï¼ˆåªä¿ç•™ç¬¬ä¸€ä¸ªå®šä¹‰ï¼Œåˆ é™¤ç¬¬äºŒä¸ªï¼‰
def reward_target_speed(env, asset_cfg):
    """
    [v5.0] é€Ÿåº¦å¯¹é½å¥–åŠ±ï¼šé¼“åŠ±ä½¿ç”¨æ¥è¿‘æœ€ä¼˜é€Ÿåº¦çš„é€Ÿåº¦
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥é¼“åŠ±å‘å‰ç§»åŠ¨ï¼Œæ›´ç®€å•ç›´æ¥
    """
    vel = env.scene[asset_cfg.name].data.root_lin_vel_b[:, 0]
    return torch.clamp(vel, min=0.0)  # âœ… åªå¥–åŠ±æ­£é€Ÿåº¦ï¼ˆå‰è¿›ï¼‰
```

**åŸå› **:
- ç¬¬äºŒä¸ªå®šä¹‰å¯èƒ½æ˜¯åœ¨åç»­ç‰ˆæœ¬ä¸­è¯¯æ·»åŠ çš„
- å¯¼è‡´å¥–åŠ±å†²çªï¼Œç­–ç•¥å­¦åˆ°é”™è¯¯è¡Œä¸º

#### æ­¥éª¤2ï¼šæ¸…ç†æ—§è®­ç»ƒæ—¥å¿—

```bash
# å¤‡ä»½æ—§æ¨¡å‹
mv logs logs_old_$(date +%Y%m%d_%H%M)

# æˆ–è€…ç›´æ¥åˆ é™¤
rm -rf logs/*
```

#### æ­¥éª¤3ï¼šä»å¤´è®­ç»ƒ

```bash
python train_v2.py --num_envs 80
```

**é¢„æœŸè®­ç»ƒæ—¶é—´**: 8-12å°æ—¶

**é¢„æœŸç»“æœ**:
- æœºå™¨äººåº”è¯¥å­¦ä¼š"é è¿‘ç›®æ ‡"
- åŠ¨ä½œå€¼åº”è¯¥åœ¨æ­£å¸¸èŒƒå›´ï¼ˆv=Â±0.3, w=Â±1.0ï¼‰
- åˆ°è¾¾ç›®æ ‡ååœæ­¢

---

### æ–¹æ¡ˆBï¼šè°ƒæ•´å¥–åŠ±æƒé‡ï¼ˆå¿«é€ŸéªŒè¯ï¼‰âš ï¸

**ç›®çš„**: åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¿«é€ŸéªŒè¯æ˜¯å¦æ˜¯ `target_speed` çš„é—®é¢˜

**æ–¹æ³•**: æš‚æ—¶ç¦ç”¨ `target_speed` å¥–åŠ±

**ä¿®æ”¹**: `dashgo_env_v2.py`

```python
# ä¿®æ”¹å‰ï¼ˆç¬¬983-987è¡Œï¼‰
target_speed = RewardTermCfg(
    func=reward_target_speed,
    weight=1.0,
    params={"asset_cfg": SceneEntityCfg("robot")}
)

# ä¿®æ”¹åï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰
# target_speed = RewardTermCfg(
#     func=reward_target_speed,
#     weight=1.0,
#     params={"asset_cfg": SceneEntityCfg("robot")}
# )

# æˆ–è€…é™ä½æƒé‡
target_speed = RewardTermCfg(
    func=reward_target_speed,
    weight=0.0,  # âœ… æƒé‡è®¾ä¸º0ï¼Œç›¸å½“äºç¦ç”¨
    params={"asset_cfg": SceneEntityCfg("robot")}
)
```

**ç„¶åé‡æ–°è®­ç»ƒ**:
```bash
python train_v2.py --num_envs 80
```

**ä¼˜ç‚¹**: å¯ä»¥å¿«é€ŸéªŒè¯ `target_speed` æ˜¯å¦æ˜¯é—®é¢˜
**ç¼ºç‚¹**: å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéœ€è¦å†æ¬¡è°ƒæ•´

---

### æ–¹æ¡ˆCï¼šä¿®å¤å‡½æ•°ç­¾åå†²çªï¼ˆæŠ€æœ¯ä¿®å¤ï¼‰ğŸ”§

**é—®é¢˜**: ä¸¤ä¸ª `reward_target_speed` å‡½æ•°ç­¾åä¸åŒï¼ŒPython ä¼šä½¿ç”¨**æœ€åä¸€ä¸ªå®šä¹‰**

**æ–‡ä»¶**: `dashgo_env_v2.py`

**ä¿®æ”¹**: ç»Ÿä¸€å‡½æ•°ç­¾åï¼Œåªä¿ç•™ä¸€ä¸ªå®šä¹‰

```python
# ä¿®æ”¹å‰ï¼ˆä¸¤ä¸ªå‡½æ•°ï¼Œç­¾åä¸åŒï¼‰
def reward_target_speed(env, asset_cfg):  # â† ç­¾å1
    ...

def reward_target_speed(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:  # â† ç­¾å2
    ...

# ä¿®æ”¹åï¼ˆåªä¿ç•™ä¸€ä¸ªå®šä¹‰ï¼Œä½¿ç”¨æ ‡å‡†ç­¾åï¼‰
def reward_target_speed(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """
    [v6.0 ä¿®å¤] é€Ÿåº¦å¥–åŠ±ï¼šåªé¼“åŠ±å‰è¿›ï¼Œä¸é¼“åŠ±åé€€

    å…³é”®ä¿®å¤ï¼šä¸ç®¡å‘åç§»åŠ¨æœ‰å¤šå¿«ï¼Œéƒ½ç»™äºˆ0å¥–åŠ±
    """
    vel = env.scene[asset_cfg.name].data.root_lin_vel_b[:, 0]
    return torch.clamp(vel, min=0.0, max=0.3)  # âœ… åªå¥–åŠ±æ­£é€Ÿåº¦ï¼Œä¸”æœ‰ä¸Šé™
```

**ç„¶åé‡æ–°è®­ç»ƒ**

---

## ğŸ“ ç›¸å…³ä»£ç ç‰‡æ®µ

### ä»£ç 1ï¼šå¥–åŠ±å‡½æ•°å®šä¹‰å†²çª

**æ–‡ä»¶**: `dashgo_env_v2.py` (ç¬¬1973-1998è¡Œ)

```python
# =============================================================================
# [v5.0 Ultimate] è¾…åŠ©å¥–åŠ±å‡½æ•°
# =============================================================================

def reward_target_speed(env, asset_cfg):
    """
    [v5.0] é€Ÿåº¦å¯¹é½å¥–åŠ±ï¼šé¼“åŠ±ä½¿ç”¨æ¥è¿‘æœ€ä¼˜é€Ÿåº¦çš„é€Ÿåº¦

    ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥é¼“åŠ±å‘å‰ç§»åŠ¨ï¼Œæ›´ç®€å•ç›´æ¥
    """
    vel = env.scene[asset_cfg.name].data.root_lin_vel_b[:, 0]
    return torch.clamp(vel, min=0.0)

def reward_facing_target(env, command_name, asset_cfg):
    """
    [v5.0] å¯¹å‡†å¥–åŠ±ï¼šé¼“åŠ±è½¦å¤´æœå‘ç›®æ ‡
    """
    target_pos = env.command_manager.get_command(command_name)[:, :2]
    robot_pos = env.scene[asset_cfg.name].data.root_pos_w[:, :2]
    robot_yaw = env.scene[asset_cfg.name].data.heading_w
    target_vec = target_pos - robot_pos
    target_yaw = torch.atan2(target_vec[:, 1], target_vec[:, 0])
    angle_error = torch.abs(mdp.math.wrap_to_pi(target_yaw - robot_yaw))
    return 1.0 / (1.0 + angle_error)

# =============================================================================
# [v5.0] Velodyne-style dense rewardï¼ˆæ­£å¸¸æ¨¡å¼ä¸“ç”¨ï¼‰
# =============================================================================

def reward_target_speed(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """
    é€Ÿåº¦åŒ¹é…å¥–åŠ±ï¼ˆç¬¬äºŒä¸ªå®šä¹‰ï¼Œç­¾åä¸åŒï¼ï¼‰

    âš ï¸ é—®é¢˜ï¼šè¿™ä¸ªå®šä¹‰ä¼šè¦†ç›–ä¸Šé¢çš„å®šä¹‰ï¼Œå› ä¸ºå®ƒæœ‰å®Œæ•´çš„ç±»å‹æç¤º
    """
    robot = env.scene[asset_cfg.name]
    lin_vel_b = torch.nan_to_num(robot.data.root_lin_vel_b[:, 0], nan=0.0, posinf=0.0, neginf=0.0)
    target_vel = 0.25
    speed_match = 1.0 - torch.abs(lin_vel_b - target_vel) / target_vel
    return torch.clamp(speed_match, 0.0, 0.2)  # âŒ ä¸ç®¡æ–¹å‘ï¼Œåªè¦é€Ÿåº¦å°±å¥–åŠ±
```

### ä»£ç 2ï¼šè·ç¦»å¥–åŠ±å‡½æ•°ï¼ˆæ­£å¸¸ï¼‰

**æ–‡ä»¶**: `dashgo_env_v2.py` (ç¬¬1950-1972è¡Œ)

```python
def reward_position_command_error_tanh(env, std: float, command_name: str, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """
    [v5.0 Hotfix] æ‰‹åŠ¨å®ç°tanhè·ç¦»å¥–åŠ±

    å¥–åŠ±èŒƒå›´: (0, 1]
    é€»è¾‘: è·ç¦»è¶Šè¿‘ï¼Œå¥–åŠ±è¶Šé«˜ï¼ˆæ¥è¿‘1ï¼‰ï¼›è·ç¦»è¶Šè¿œï¼Œå¥–åŠ±è¶Šä½ï¼ˆæ¥è¿‘0ï¼‰

    æ•°å­¦åŸç†:
        reward = 1.0 - tanh(dist / std)
        - å½“ dist = 0, tanh = 0, reward = 1.0ï¼ˆåˆ°è¾¾ç›®æ ‡ï¼‰
        - å½“ dist = std, tanh â‰ˆ 0.76, reward â‰ˆ 0.24ï¼ˆä¸­ç­‰è·ç¦»ï¼‰
        - å½“ dist >> std, tanh â‰ˆ 1.0, reward â‰ˆ 0.0ï¼ˆè¿œè·ç¦»ï¼‰
    """
    # 1. è·å–ç›®æ ‡ä½ç½® (x, y)
    target_pos = env.command_manager.get_command(command_name)[:, :2]

    # 2. è·å–æœºå™¨äººä½ç½® (x, y)
    robot_pos = env.scene[asset_cfg.name].data.root_pos_w[:, :2]

    # 3. è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
    dist = torch.norm(target_pos - robot_pos, dim=1)

    # 4. è®¡ç®—tanhå¥–åŠ±
    return 1.0 - torch.tanh(dist / std)
```

### ä»£ç 3ï¼šå¥–åŠ±é…ç½®

**æ–‡ä»¶**: `dashgo_env_v2.py` (ç¬¬963-992è¡Œ)

```python
# [ç¨€ç–] åˆ°è¾¾å¥–åŠ±ï¼ˆç»å¯¹ä¸»å¯¼ï¼‰
reach_goal = RewardTermCfg(
    func=reward_near_goal,
    weight=2000.0,  # âœ… [v5.0] ç»å¯¹ä¸»å¯¼å€¼ï¼ˆä»1000.0æå‡ï¼‰
    params={
        "command_name": "target_pose",
        "threshold": 0.5,  # âœ… v5.0ä½¿ç”¨0.5mé˜ˆå€¼
        "asset_cfg": SceneEntityCfg("robot")
    }
)

# [å¼•å¯¼] é»„é‡‘å¹³è¡¡ç‚¹ 0.75 + tanh
shaping_distance = RewardTermCfg(
    func=reward_position_command_error_tanh,
    weight=0.75,  # âœ… [v5.0] é»„é‡‘å¹³è¡¡ç‚¹
    params={"std": 4.0, "command_name": "target_pose", "asset_cfg": SceneEntityCfg("robot")}
)

# [è¾…åŠ©] Denseå¥–åŠ±ç»„
target_speed = RewardTermCfg(
    func=reward_target_speed,  # âš ï¸ ä½¿ç”¨å“ªä¸ªå®šä¹‰ï¼Ÿ
    weight=1.0,
    params={"asset_cfg": SceneEntityCfg("robot")}
)
facing_goal = RewardTermCfg(
    func=reward_facing_target,
    weight=0.1,
    params={"command_name": "target_pose", "asset_cfg": SceneEntityCfg("robot")}
)
```

### ä»£ç 4ï¼šåŠ¨ä½œå¤„ç†ï¼ˆæœ‰è£å‰ªï¼Œæ‰€ä»¥å®é™…é€Ÿåº¦æ­£å¸¸ï¼‰

**æ–‡ä»¶**: `dashgo_env_v2.py` (ç¬¬151-160è¡Œ)

```python
# ä»ç¥ç»ç½‘ç»œè¾“å‡ºå¾—åˆ°ç›®æ ‡é€Ÿåº¦
target_v = actions[:, 0] * self.max_lin_vel  # ç½‘ç»œè¾“å‡º * 0.3
target_w = actions[:, 1] * self.max_ang_vel  # ç½‘ç»œè¾“å‡º * 1.0

# è£å‰ªåˆ°ç‰©ç†é™åˆ¶
target_v = torch.clamp(target_v, -self.max_lin_vel, self.max_lin_vel)  # [-0.3, 0.3]
target_w = torch.clamp(target_w, -self.max_ang_vel, self.max_ang_vel)  # [-1.0, 1.0]
```

**è¯´æ˜**:
- ç½‘ç»œè¾“å‡ºï¼šv=Â±10, w=Â±13
- ç»è¿‡è£å‰ªåï¼šv=Â±0.3, w=Â±1.0ï¼ˆå®é™…æ‰§è¡Œå€¼ï¼‰
- æ‰€ä»¥æœºå™¨äººå®é™…é€Ÿåº¦æ˜¯æ­£å¸¸çš„ï¼Œä½†**ç½‘ç»œè¾“å‡ºå¼‚å¸¸**è¯´æ˜è®­ç»ƒå‡ºäº†é—®é¢˜

---

## ğŸš€ æ¶æ„å¸ˆå»ºè®®ï¼ˆè¯·ç¡®è®¤ï¼‰

### æ¨èæ–¹æ¡ˆï¼šæ–¹æ¡ˆAï¼ˆå½»åº•ä¿®å¤ï¼‰âœ…

1. **ä¿®å¤å¥–åŠ±å‡½æ•°å®šä¹‰å†²çª**
   - åˆ é™¤ç¬¬äºŒä¸ª `reward_target_speed` å®šä¹‰
   - åªä¿ç•™ç¬¬ä¸€ä¸ªå®šä¹‰ï¼ˆåªå¥–åŠ±å‰è¿›ï¼‰

2. **æ¸…ç†æ—§è®­ç»ƒæ—¥å¿—**
   - `mv logs logs_old_$(date +%Y%m%d_%H%M)`

3. **ä»å¤´è®­ç»ƒ**
   - `python train_v2.py --num_envs 80`
   - é¢„è®¡æ—¶é—´ï¼š8-12å°æ—¶

### å¤‡é€‰æ–¹æ¡ˆï¼šæ–¹æ¡ˆBï¼ˆå¿«é€ŸéªŒè¯ï¼‰âš ï¸

1. **æš‚æ—¶ç¦ç”¨ target_speed**
   - è®¾ç½® `weight=0.0` æˆ–æ³¨é‡Šæ‰

2. **å¿«é€Ÿè®­ç»ƒéªŒè¯**
   - åªè®­ç»ƒåˆ° 1000 iterationsï¼ˆçº¦2å°æ—¶ï¼‰
   - è§‚å¯Ÿç­–ç•¥æ˜¯å¦æ­£å¸¸

3. **å¦‚æœæ­£å¸¸ï¼Œå†åº”ç”¨æ–¹æ¡ˆA**

---

## ğŸ“š é™„å½•ï¼šæµ‹è¯•æ—¥å¿—

### å®Œæ•´æµ‹è¯•å‘½ä»¤

```bash
# æµ‹è¯•1ï¼šç‰©ç†å‚æ•°éªŒè¯
python debug_play.py --test straight_line --num_envs 1 --num_episodes 3

# æµ‹è¯•2ï¼šåæ ‡ç³»éªŒè¯
python debug_play.py --test print_obs --num_envs 1 --num_episodes 1

# æµ‹è¯•3ï¼šå½’ä¸€åŒ–å½±å“éªŒè¯
python debug_play.py --test no_norm --num_envs 1 --num_episodes 1

# æµ‹è¯•4ï¼šæœ€æ–°æ¨¡å‹éªŒè¯
python debug_play.py --test print_obs --num_envs 1 --num_episodes 1 --checkpoint logs/model_4999.pt
```

### æµ‹è¯•æ•°æ®æ–‡ä»¶

æ‰€æœ‰æµ‹è¯•æ•°æ®å·²ä¿å­˜åœ¨ï¼š
- `issues/2026-01-26_0440_é†‰æ±‰èµ°è·¯é—®é¢˜æ’æŸ¥æŒ‡å—.md`
- `issues/2026-01-26_0450_é†‰æ±‰èµ°è·¯é—®é¢˜æ’æŸ¥è®°å½•.md`
- `issues/2026-01-26_0505_no_normæ¨¡å¼åŠ è½½æ¨¡å‹å¤±è´¥.md`
- `issues/2026-01-26_0520_é†‰æ±‰èµ°è·¯é—®é¢˜è¯Šæ–­æŠ¥å‘Š.md`ï¼ˆæœ¬æ–‡ä»¶ï¼‰

---

**åˆ›å»ºæ—¶é—´**: 2026-01-26 05:20
**æŠ¥å‘Šè€…**: Claude Code AI System
**çŠ¶æ€**: ğŸ” é—®é¢˜å·²å®šä½ï¼Œç­‰å¾…æ¶æ„å¸ˆç¡®è®¤ä¿®å¤æ–¹æ¡ˆ
**ä¸‹ä¸€æ­¥**: æ ¹æ®æ¶æ„å¸ˆåé¦ˆæ‰§è¡Œä¿®å¤
