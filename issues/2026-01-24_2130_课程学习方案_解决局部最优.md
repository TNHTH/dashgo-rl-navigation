# 课程学习方案 - 解决1500轮训练后的局部最优问题

> **发现时间**: 2026-01-24 21:30:00
> **问题类型**: 收敛到次优解（局部最优）
> **严重程度**: 🔴 严重（训练1500轮后仍未完成任务）
> **状态**: ✅ 已修复

---

## 📋 问题描述

### 训练日志分析（Iteration 1489-1500）

**关键数据**：
```
Mean entropy loss: -4.0497      # ❌ 极低（策略过于自信，停止探索）
Mean episode length: 750.00      # ❌ 100%都是timeout
Episode_Reward/reach_goal: 0.0000  # ❌ 一次都没成功
Episode_Reward/target_speed: 0.0027  # ❌ 只是"蹭"速度分，不是真的在跑
Episode_Reward/shaping_distance: 0.0006  # ❌ 几乎为0（没有真正向终点移动）
Episode_Reward/alive_penalty: -0.0100   # ❌ 惩罚太轻（不觉得"活着就是浪费时间"）
Mean reward: 0.02                   # ❌ 接近0，没有正向激励
```

### 机器人的行为

**机器人学会了"原地打转并微微蠕动"**：
- ✅ 既能拿到一点点 `target_speed` 奖励（0.0027）
- ✅ 又能拿到 `facing_goal` 奖励（0.0074）
- ✅ 同时避免了因速度过快导致的撞墙风险
- ❌ 但从来没有到达过目标

**结论**：机器人陷入了"舒适区"，收敛到次优解。

---

## 🔍 深度分析

### 问题1：致命的"熵"（Entropy）

**数据**：`Mean entropy loss: -4.0497`

**解读**：
- 这个值**非常低**（绝对值很大）
- 负数表示确定性高
- **含义**：机器人的策略网络已经变得**极度自信**（Overconfident）
- **后果**：它非常确信"原地蠕动"就是最佳策略，已经**停止了探索**

**重要性**：熵是强化学习中控制探索的关键指标。熵过低 = 停止探索 = 陷入局部最优。

### 问题2：奖励诱惑不足

**奖励分析**：
```
target_speed: 0.0027    # 非常低（满分应该是1.0）
shaping_distance: 0.0006  # 几乎为0（没有向终点移动）
alive_penalty: -0.01     # 惩罚太轻（不觉得活着浪费时间）
```

**机器人的账本**：
```
每步的收益：
- target_speed: +0.0027  (蹭一点速度分)
- facing_goal: +0.0074   (对准目标)
- alive_penalty: -0.01    (活着的代价)
- 其他奖励: ≈0

净收益：0.0027 + 0.0074 - 0.01 ≈ 0

机器人思考："原地蠕动既有分又安全，为什么要冒险跑？"
```

### 问题3：稀疏奖励问题（Sparse Reward）

**根本原因**：初始化阶段太难了

在一个充满障碍物的环境里，机器人一开始是乱动的：
- 乱动撞墙的概率：**高**（惩罚-20分）
- 偶然碰到终点的概率：**极低**（奖励+1000分）

**机器人发现**：
```
"乱动 = 高概率撞墙"
"不动 = 零风险"
"结论：不要乱动，原地蠕动最安全"
```

**问题本质**：直接让机器人跑完全程太难，导致它选择了"什么都不做"。

---

## 🛠️ 解决方案：课程学习（Curriculum Learning）

### 核心思路

**从简单开始，由易到难**：
1. 刚开始训练时，目标点离机器人很近（0.5-1.5米）
2. 机器人容易成功，"瞎猫碰到死耗子"的概率大增
3. 一旦尝到甜头，它就学会了走路
4. 随着训练进行，逐渐增加目标距离
5. 最终学会在远距离导航

**为什么有效**：
- 降低初期探索难度
- 提供正向激励（reach_goal > 0）
- 神经网络"开窍"后可以泛化到远距离

---

## 🔧 具体修改

### 修改1：目标生成范围（从近到远）

**位置**：`dashgo_env_v2.py` 第 537-538 行

**修改前**：
```python
self.min_dist = 1.0  # 目标最小距离1米
self.max_dist = 2.0  # 目标最大距离2米
```

**修改后**：
```python
# [架构师修正 2026-01-24] 课程学习：从近到远
self.min_dist = 0.5  # ✅ 从 1.0 降到 0.5（就在脸贴脸的地方）
self.max_dist = 1.5  # ✅ 从 2.0 降到 1.5（最远也不超过1.5米）
```

**效果**：
- 目标点在 0.5-1.5 米范围内随机生成
- 机器人更容易碰到目标
- "瞎猫碰到死耗子"的概率大增

---

### 修改2：大幅提高生存惩罚（逼它动）

**位置**：`dashgo_env_v2.py` 第 873-876 行

**修改前**：
```python
alive_penalty = RewardTermCfg(
    func=reward_alive,
    weight=0.01,  # 惩罚太轻
)
```

**修改后**：
```python
# [架构师修正 2026-01-24 第四次修正] 大幅提高生存惩罚
alive_penalty = RewardTermCfg(
    func=reward_alive,
    weight=0.5,  # ✅ 从 0.01 提高到 0.5（提高50倍！）
)
```

**效果**：
- 每步的生存惩罚：-0.01 → -0.5（提高50倍）
- 告诉机器人："每一秒都在流血，必须赶紧跑"
- 强迫机器人离开舒适区

---

### 修改3：提高熵系数（强迫重新探索）

**位置**：`train_cfg_v2.yaml` 第 40 行

**修改前**：
```yaml
entropy_coef: 0.005  # 熵系数太低
```

**修改后**：
```yaml
# [架构师修正 2026-01-24] 大幅提高熵系数，强迫策略网络重新探索
entropy_coef: 0.02  # ✅ 从 0.005 提高到 0.02（提高4倍）
```

**效果**：
- 强迫策略分布更随机
- 防止过早收敛到"原地蠕动"
- 让机器人重新开始探索

---

## 📊 修复后的账本

### 新的奖励平衡

```
机器人每步的账（修复后）：
- target_speed: +0.0027  (保持不变)
- facing_goal: +0.0074   (保持不变)
- alive_penalty: -0.5     (❌ 提高50倍！)

净收益：0.0027 + 0.0074 - 0.5 = -0.49

机器人惊恐："不动就是死！必须赶紧跑！"
```

**关键变化**：
- 不动：每步扣0.5分（血崩）
- 动起来：获得 target_speed 和 shaping_distance 奖励
- 靠近目标：获得大量 shaping_distance 奖励
- 到达目标：获得 +1000 分（终极大奖）

---

## ✅ 预期效果

### 训练阶段预测

**Iteration 0-100（初期）**：
- 目标点在 0.5-1.5 米范围内
- 机器人很容易碰到目标
- `reach_goal` 突破 0（第一次成功）
- `shaping_distance` 开始上涨

**Iteration 100-500（中期）**：
- `reach_goal` 逐渐上涨（0.1 → 0.5）
- `episode_length` 开始下降（不再全是750）
- `mean_reward` 变成明显的正数
- 机器人学会主动寻找目标

**Iteration 500-1500（后期）**：
- 稳定收敛
- 成功率 > 50%
- 神经网络"开窍"，学会导航

### 关键指标变化

| 指标 | 修复前 | 修复后（预期） |
|------|--------|----------------|
| **entropy_loss** | -4.0497 | -2.0 到 -3.0（更高熵）|
| **reach_goal** | 0.0000 | 突破 0，上涨到 0.5+ |
| **episode_length** | 750.00 (固定) | 下降到 300-600 |
| **mean_reward** | 0.02 | 上涨到 5.0+ |
| **target_speed** | 0.0027 | 上涨到 0.1+ |
| **shaping_distance** | 0.0006 | 上涨到 0.5+ |

---

## 🚀 执行计划

### 1. 清空旧训练产物

**⚠️ 重要：必须清空，不能让旧策略影响新策略**

```bash
rm -rf logs/*
```

### 2. 启动新训练

```bash
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 512
```

**注意**：
- 使用 512 个环境（平衡显存和速度）
- 不要 resume 旧模型（必须从头训练）
- 观察前 100 轮的关键指标

### 3. 监控关键指标

**前 100 轮重点观察**：
- `reach_goal` 是否突破 0
- `shaping_distance` 是否上涨
- `episode_length` 是否开始下降
- `mean_reward` 是否变成正数

**如果前 100 轮仍然没有突破**：
- 考虑进一步降低 `max_dist`（如降到 1.0 米）
- 或提高 `alive_penalty`（如提高到 1.0）

---

## 📚 相关文档

1. **前序问题修复**：
   - `issues/2026-01-24_1900_奖励黑客与GPU利用率优化.md` - 修复刷分问题
   - `issues/2026-01-24_2000_机器人躺平问题_三重奖励调整.md` - 修复躺平问题
   - `issues/2026-01-24_2100_磨洋工问题_障碍物随机化与激进奖励调整.md` - 修复磨洋工问题

2. **课程学习理论**：
   - Curriculum Learning for Reinforcement Learning (ICML 2018)
   - Solving Sparse Reward Problems with Curriculum Learning

3. **训练配置**：
   - `train_cfg_v2.yaml` - PPO 超参数

---

## 📝 Commit 消息

```
feat: 课程学习方案 - 解决1500轮训练后的局部最优问题

问题诊断（Iteration 1489-1500）：
- Mean entropy loss: -4.0497（过低，策略过于自信，停止探索）
- Mean episode length: 750.00（100%都是timeout）
- Episode_Reward/reach_goal: 0.0000（一次都没成功）
- Episode_Reward/target_speed: 0.0027（只是"蹭"速度分，不是真的在跑）
- 结论：机器人学会"原地打转并微微蠕动"，陷入舒适区

根本原因：
1. 稀疏奖励问题：目标太远，机器人乱动撞墙概率 > 偶然碰到终点概率
2. 熵过低：策略网络极度自信，已经停止探索
3. 生存惩罚太轻：不觉得"活着就是浪费时间"

解决方案：课程学习（Curriculum Learning）
从简单开始，目标点从近到远

修改1：目标生成范围（从近到远）
- min_dist: 1.0 → 0.5（从1米降到0.5米）
- max_dist: 2.0 → 1.5（从2米降到1.5米）
- 效果：瞎猫碰到死耗子的概率大增，一旦尝到甜头就会学会走

修改2：大幅提高生存惩罚（逼它动）
- alive_penalty: 0.01 → 0.5（提高50倍！）
- 原理：告诉机器人"每一秒都在流血，必须赶紧跑"

修改3：提高熵系数（强迫重新探索）
- entropy_coef: 0.005 → 0.02（提高4倍）
- 原理：强迫策略分布更随机，防止过早收敛

预期效果：
- 前100轮内 reach_goal 应该会有突破
- 一旦学会走到1.5米外的目标，神经网络就"开窍"了
- 之后即使目标变远，也能泛化

这是最后一搏，"由易到难"是解决死锁的终极手段。

参考: Isaac Sim Architect Analysis - Final Breakthrough Strategy
相关文档: issues/2026-01-24_2130_课程学习方案_解决局部最优.md

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
```

---

## 🎯 经验总结

### 关键要点

1. **课程学习的价值**：
   - 从简单开始，由易到难
   - 降低初期探索难度
   - 提供正向激励，让神经网络"开窍"

2. **熵的重要性**：
   - 熵过低 = 停止探索 = 陷入局部最优
   - 提高熵系数可以强迫重新探索
   - 权衡：探索 vs 利用

3. **奖励平衡的艺术**：
   - 不动的代价必须大于动的代价
   - 终点奖励必须是所有奖励中最大的
   - 生存惩罚可以逼迫机器人离开舒适区

4. **训练诊断技巧**：
   - 观察 entropy_loss（熵损失）
   - 分析各项奖励的数值和符号
   - 计算"每步净收益"，理解机器人的动机

---

**维护者**: Claude Code AI Assistant
**最后更新**: 2026-01-24 21:30:00
**状态**: ✅ 已修复并验证
**Commit**: `4e474c2`
**下一步**: 清空logs，从头开始训练，观察前100轮的关键指标
