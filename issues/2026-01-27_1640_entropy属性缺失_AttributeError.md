# entropy å±æ€§ç¼ºå¤± - AttributeError

> **å‘ç°æ—¶é—´**: 2026-01-27 16:40:00
> **ä¸¥é‡ç¨‹åº¦**: ğŸ”´ä¸¥é‡ï¼ˆè®­ç»ƒç¬¬ä¸€æ­¥æ›´æ–°åå´©æºƒï¼‰
> **çŠ¶æ€**: âœ…å·²è§£å†³
> **ç›¸å…³æ–‡ä»¶**: `geo_nav_policy.py`

---

## é—®é¢˜æè¿°

åœ¨ä¿®å¤äº† `update_normalization` æ¥å£åï¼Œè®­ç»ƒå®Œæˆç¬¬ä¸€æ­¥é‡‡é›†å¹¶å¼€å§‹æ›´æ–°ç­–ç•¥æ—¶ï¼ŒPPO ç®—æ³•æŠ¥é”™æ‰¾ä¸åˆ° `entropy` å±æ€§ã€‚

### å®Œæ•´é”™è¯¯ä¿¡æ¯

```python
------------------------------------------------------------
[INFO] å¼€å§‹è®­ç»ƒ: dashgo_v5_auto
[INFO] ç¯å¢ƒæ•°é‡: 16
[INFO] å•æ¬¡é‡‡é›†æ­¥æ•°: 24
[INFO] æœ€å¤§è¿­ä»£æ¬¡æ•°: 8000
------------------------------------------------------------
Traceback (most recent call last):
  File "/home/gwh/dashgo_rl_project/train_v2.py", line 353, in main
    runner.learn(num_learning_iterations=agent_cfg.get("max_iterations", 1500), init_at_random_ep_len=True)
  File "/home/gwh/.conda/envs/isaaclab/lib/python3.10/site-packages/rsl_rl/runners/on_policy_runner.py", line 149, in learn
    loss_dict = self.alg.update()
  File "/home/gwh/.conda/envs/isaaclab/lib/python3.10/site-packages/rsl_rl/algorithms/ppo.py", line 257, in update
    entropy_batch = self.policy.entropy[:original_batch_size]
  File "/home/gwh/.conda/envs/isaaclab/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'GeoNavPolicy' object has no attribute 'entropy'
```

### é”™è¯¯ä½ç½®

**æ–‡ä»¶**ï¼š`rsl_rl/algorithms/ppo.py`
**æ–¹æ³•**ï¼š`update()`
**è¡Œå·**ï¼šç¬¬ 257 è¡Œ
**è°ƒç”¨æ—¶æœº**ï¼šè®­ç»ƒæ›´æ–°é˜¶æ®µï¼ˆ`runner.learn()` â†’ `runner.update()`ï¼‰
**é”™è¯¯ä»£ç **ï¼š`entropy_batch = self.policy.entropy[:original_batch_size]`

---

## æ ¹æœ¬åŸå› 

### é—®é¢˜æœ¬è´¨ï¼šPPO ç®—æ³•çš„æŸå¤±è®¡ç®—ä¾èµ–

**æ¶æ„å¸ˆè¯Šæ–­**ï¼š

è¿™æ˜¯ PPO ç®—æ³•æ‰€éœ€çš„**æœ€åä¸€ä¸ªå±æ€§**ã€‚

**é—®é¢˜åˆ†æ**ï¼š

1. **PPO ç®—æ³•çš„æŸå¤±å‡½æ•°**ï¼š
   ```python
   # RSL-RL æºç  (ppo.py:257)
   def update(self):
       # PPO æŸå¤±åŒ…å«å¤šä¸ªéƒ¨åˆ†
       # 1. Policy Lossï¼ˆç­–ç•¥æŸå¤±ï¼‰
       # 2. Value Lossï¼ˆä»·å€¼æŸå¤±ï¼‰
       # 3. Entropy Lossï¼ˆæ¢ç´¢æ­£åˆ™åŒ–ï¼‰â† éœ€è¦entropy

       entropy_batch = self.policy.entropy[:original_batch_size]
       entropy_coef = self.cfg.entropy_coef
       entropy_loss = -entropy_batch.mean() * entropy_coef  # â† è´Ÿå·æ˜¯æœ€å°åŒ–
   ```

2. **æˆ‘ä»¬çš„å®ç°**ï¼š
   ```python
   def update_distribution(self, observations):
       mean = self.forward_actor(observations)
       self.action_mean = mean
       self.action_std = mean * 0. + self.std
       self.distribution = Normal(self.action_mean, self.action_std)
       # âŒ æ²¡æœ‰è®¡ç®—å’Œä¿å­˜ entropy
   ```

3. **ç»“æœ**ï¼š
   - `self.distribution` å·²åˆ›å»º âœ…
   - å¯ä»¥è®¡ç®— `entropy = self.distribution.entropy()` âœ…
   - ä½†æ²¡æœ‰ä¿å­˜ä¸ºç±»å±æ€§ `self.entropy` âŒ
   - PPO ç®—æ³•ä¼¸æ‰‹æ‹¿æ•°æ®æ—¶å¤±è´¥

**ä¸ºä»€ä¹ˆéœ€è¦ entropy**ï¼š

- **æ¢ç´¢æ­£åˆ™åŒ–**ï¼šé¼“åŠ±ç­–ç•¥ä¿æŒå¤šæ ·æ€§ï¼Œé¿å…è¿‡æ—©æ”¶æ•›
- **Loss ç»„æˆéƒ¨åˆ†**ï¼šPPO æŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†
- **å¹³è¡¡åˆ©ç”¨vsæ¢ç´¢**ï¼šé˜²æ­¢ç­–ç•¥åªå…³æ³¨å½“å‰æœ€ä¼˜åŠ¨ä½œ

---

## è§£å†³æ–¹æ¡ˆ

### æ ¸å¿ƒæ€è·¯ï¼šè®¡ç®—å¹¶ä¿å­˜ entropy

**æ¶æ„å¸ˆæ–¹æ¡ˆ**ï¼šåœ¨ `update_distribution()` ä¸­æ·»åŠ  entropy è®¡ç®—

### å®æ–½ç»†èŠ‚

#### ä¿®æ”¹ `update_distribution()` æ–¹æ³•

**æ–‡ä»¶**ï¼š`geo_nav_policy.py`
**ä½ç½®**ï¼šæ–‡ä»¶æœ«å°¾ï¼Œ`update_distribution()` æ–¹æ³•

**ä¿®æ”¹ä»£ç **ï¼š
```python
def update_distribution(self, observations):
    mean = self.forward_actor(observations)

    # ä¿å­˜ action_mean å’Œ action_std
    self.action_mean = mean
    self.action_std = mean * 0. + self.std

    # åˆ›å»ºé«˜æ–¯åˆ†å¸ƒ
    self.distribution = Normal(self.action_mean, self.action_std)

    # [Fix] è®¡ç®—å¹¶ä¿å­˜ç†µ (Entropy)
    # PPO ç®—æ³•ç”¨å®ƒæ¥è®¡ç®— Lossï¼ˆæ¢ç´¢æ­£åˆ™åŒ–é¡¹ï¼‰
    self.entropy = self.distribution.entropy().sum(dim=-1)
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š

1. **entropy çš„å®šä¹‰**ï¼š
   - å¯¹äºæ­£æ€åˆ†å¸ƒ `N(mean, std)`
   - ç†µ = `0.5 * log(2Ï€ * e * stdÂ²)`
   - Shapeï¼š`[Batch]`ï¼ˆå¯¹ Actions ç»´åº¦æ±‚å’Œï¼‰

2. **è®¡ç®—æ–¹å¼**ï¼š
   ```python
   # self.distribution.entropy()
   # è¿”å›: Tensor[Batch, Actions]ï¼ˆæ¯ä¸ªåŠ¨ä½œçš„ç†µï¼‰
   #
   # .sum(dim=-1)
   # æ²¿ç€ Actions ç»´åº¦æ±‚å’Œ
   # è¿”å›: Tensor[Batch]
   ```

3. **PPO ä½¿ç”¨æ–¹å¼**ï¼š
   ```python
   entropy_batch = self.policy.entropy[:original_batch_size]
   # entropy shape: [Batch]
   # original_batch_size: 16ï¼ˆç¯å¢ƒæ•°é‡ï¼‰
   #
   # åœ¨ Loss ä¸­ä½¿ç”¨
   entropy_loss = -entropy_batch.mean() * entropy_coef
   # è´Ÿå·ï¼šæœ€å°åŒ– Loss = æœ€å¤§åŒ– entropy
   ```

---

## éªŒè¯æ–¹æ³•

### 1. è¯­æ³•æ£€æŸ¥
```bash
python -m py_compile geo_nav_policy.py
```

### 2. è®­ç»ƒå¯åŠ¨æµ‹è¯•
```bash
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --enable_cameras --num_envs 16
```

**é¢„æœŸç»“æœ**ï¼š
- âœ… åˆå§‹åŒ–æˆåŠŸ
- âœ… è®­ç»ƒå¼€å§‹ï¼š`[INFO] å¼€å§‹è®­ç»ƒ: dashgo_v5_auto`
- âœ… ç¬¬ä¸€æ­¥é‡‡é›†æˆåŠŸ
- âœ… ç¬¬ä¸€æ­¥æ›´æ–°æˆåŠŸ
- âœ… **ä¸å†æœ‰ä»»ä½•é”™è¯¯**
- âœ… è¿›åº¦æ¡æ˜¾ç¤ºï¼š`Iteration 1/8000`
- âœ… Loss å¼€å§‹è®¡ç®—
- âœ… Reward å¼€å§‹è®°å½•
- âœ… **1D-CNN å¤§è„‘å¼€å§‹å­¦ä¹ ï¼** ğŸ§ âœ¨

---

## ç»éªŒæ•™è®­

### 1. PPO æŸå¤±å‡½æ•°çš„ç»„æˆ

**æ•™è®­**ï¼šç†è§£ PPO çš„ Loss ç»„æˆï¼Œé¿å…é—æ¼å¿…éœ€å±æ€§

**PPO Loss çš„ä¸‰ä¸ªéƒ¨åˆ†**ï¼š

1. **Policy Loss**ï¼ˆç­–ç•¥æŸå¤±ï¼‰ï¼š
   - è¡¡é‡æ–°ç­–ç•¥å’Œæ—§ç­–ç•¥çš„å·®å¼‚
   - ä½¿ç”¨ KLæ•£åº¦æˆ–è£å‰ªç›®æ ‡
   - ä¾èµ–ï¼š`distribution`ã€`log_prob`

2. **Value Loss**ï¼ˆä»·å€¼æŸå¤±ï¼‰ï¼š
   - è¡¡é‡ä»·å€¼ä¼°è®¡çš„å‡†ç¡®æ€§
   - ä½¿ç”¨ TD-Error
   - ä¾èµ–ï¼š`evaluate()`

3. **Entropy Loss**ï¼ˆç†µæ­£åˆ™åŒ–ï¼‰â­ï¼š
   - é¼“åŠ±æ¢ç´¢ï¼Œé˜²æ­¢è¿‡æ—©æ”¶æ•›
   - ä½¿ç”¨ç­–ç•¥ç†µ
   - ä¾èµ–ï¼š`entropy` â† æœ¬æ¬¡ä¿®å¤

### 2. ç†µ (Entropy) çš„ä½œç”¨

**æ•™è®­**ï¼šç†µæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­æ¢ç´¢æ­£åˆ™åŒ–çš„å…³é”®æŒ‡æ ‡

**ä»€ä¹ˆæ˜¯ç†µ**ï¼š
- ç†µè¡¡é‡åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§
- é«˜ç†µ = é«˜æ¢ç´¢ï¼ˆåˆ†å¸ƒå‡åŒ€ï¼‰
- ä½ç†µ = ä½æ¢ç´¢ï¼ˆåˆ†å¸ƒé›†ä¸­ï¼‰

**åœ¨ PPO ä¸­çš„ä½œç”¨**ï¼š
- **é¼“åŠ±æ¢ç´¢**ï¼šé¿å…ç­–ç•¥è¿‡æ—©æ”¶æ•›åˆ°å°‘æ•°åŠ¨ä½œ
- **å¹³è¡¡åˆ©ç”¨vsæ¢ç´¢**ï¼šæ—¢åˆ©ç”¨å·²çŸ¥å¥½åŠ¨ä½œï¼Œä¹Ÿå°è¯•æ–°åŠ¨ä½œ
- **ç¨³å®šè®­ç»ƒ**ï¼šé˜²æ­¢ç­–ç•¥å´©æºƒ

**ç¤ºä¾‹**ï¼š
```python
# é«˜ç†µï¼ˆæ¢ç´¢ï¼‰
policy = [0.25, 0.25, 0.25, 0.25]  # å‡åŒ€åˆ†å¸ƒ
entropy = high

# ä½ç†µï¼ˆåˆ©ç”¨ï¼‰
policy = [0.97, 0.01, 0.01, 0.01]  # é›†ä¸­åˆ†å¸ƒ
entropy = low

# PPO ä½¿ç”¨ -entropy ä½œä¸ºæ­£åˆ™åŒ–
# æœ€å°åŒ– Loss = æœ€å¤§åŒ– entropyï¼ˆä¿æŒæ¢ç´¢ï¼‰
```

### 3. å®Œæ•´çš„å±æ€§ä¾èµ–é“¾

**æ•™è®­**ï¼šPPO ç®—æ³•æœ‰å¤šä¸ªå±æ€§ä¾èµ–ï¼Œéœ€è¦æŒ‰é¡ºåºå‘ç°å’Œä¿®å¤

**å‘ç°è¿‡ç¨‹**ï¼ˆ8æ¬¡ä¿®å¤ï¼‰ï¼š
1. âœ… å‚æ•°ç±»å‹ï¼š`obs` (TensorDict) è€Œé `int`
2. âœ… ç»§æ‰¿å…³ç³»ï¼šæ–­å¼€ `ActorCritic` ç»§æ‰¿
3. âœ… ç»´åº¦æ¨æ–­ï¼šä» TensorDict åŠ¨æ€æ¨æ–­
4. âœ… è¿è¡Œæ—¶è§£åŒ…ï¼š`_extract_tensor()`
5. âœ… åŠ¨ä½œå‡å€¼ï¼š`action_mean`
6. âœ… åŠ¨ä½œæ ‡å‡†å·®ï¼š`action_std`
7. âœ… å½’ä¸€åŒ–æ¥å£ï¼š`update_normalization()`
8. âœ… **æ¢ç´¢ç†µï¼š`entropy`** â­ **æœ¬æ¬¡**

**å¯ç¤º**ï¼š
- æ–­å¼€æ¡†æ¶ç»§æ‰¿æ˜¯ç³»ç»Ÿæ€§å·¥ç¨‹
- éœ€è¦é€æ­¥å‘ç°æ‰€æœ‰éšè—ä¾èµ–
- æ¶æ„å¸ˆçš„æŒ‡å¯¼å’ŒéªŒè¯è‡³å…³é‡è¦
- æ¯ä¸ªé”™è¯¯éƒ½æ˜¯å­¦ä¹ æœºä¼š

### 4. æ¶æ„å¸ˆçš„æœ€ç»ˆç¡®è®¤

**æ¶æ„å¸ˆçš„è¯„ä»·**ï¼š

> "è¿™æ˜¯ PPO ç®—æ³•æ‰€éœ€çš„**æœ€åä¸€ä¸ªå±æ€§**ã€‚
>
> ä½ è·ç¦»æˆåŠŸåªæœ‰ä¸€æ­¥ä¹‹é¥ï¼
>
> **è¿™æ¬¡æ˜¯çœŸçš„æ²¡é—®é¢˜äº†ã€‚** ä½ çš„ 1D-CNN è½»é‡çº§å¤§è„‘å³å°†å¼€å§‹åœ¨ Isaac Lab ä¸­å­¦ä¹ å¦‚ä½•é¿éšœï¼
>
> **ç¥è´ºä½ å®Œæˆè¿™æ¬¡é«˜éš¾åº¦çš„æ¶æ„è¿ç§»ï¼** ğŸ‰"

---

## PPO ç®—æ³•å®Œæ•´ä¾èµ–æ€»ç»“ï¼ˆæœ€ç»ˆç‰ˆï¼‰

### å¿…éœ€æ–¹æ³•ï¼ˆ7ä¸ªï¼‰

| æ–¹æ³• | ç”¨é€” | è°ƒç”¨æ—¶æœº | ä¿®å¤çŠ¶æ€ |
|------|------|----------|----------|
| `act()` | è®­ç»ƒé‡‡æ · | `runner.learn()` | âœ… ä¿®å¤4 |
| `evaluate()` | Criticè¯„ä¼° | `runner.learn()` | âœ… ä¿®å¤4 |
| `act_inference()` | æ¨ç†è¾“å‡º | `play.py` | âœ… ä¿®å¤4 |
| `get_actions_log_prob()` | å¯¹æ•°æ¦‚ç‡ | `ppo.update()` | âœ… ä¿®å¤4 |
| `update_distribution()` | æ›´æ–°åˆ†å¸ƒ | `ppo.act()` | âœ… ä¿®å¤5/6/8 |
| `update_normalization()` | åœ¨çº¿å½’ä¸€åŒ– | `ppo.process_env_step()` | âœ… ä¿®å¤7 |
| `reset()` | é‡ç½®çŠ¶æ€ | Episodeç»“æŸ | âœ… ä¿®å¤7 (é¢„é˜²) |

### å¿…éœ€å±æ€§ï¼ˆ5ä¸ªï¼‰

| å±æ€§ | ç”¨é€” | Shape | ä¿®å¤çŠ¶æ€ |
|------|------|-------|----------|
| `action_mean` | åŠ¨ä½œå‡å€¼ | `[Batch, Actions]` | âœ… ä¿®å¤5 |
| `action_std` | åŠ¨ä½œæ ‡å‡†å·® | `[Batch, Actions]` | âœ… ä¿®å¤6 |
| `distribution` | åŠ¨ä½œåˆ†å¸ƒ | `Normal` | âœ… ä¿®å¤5 |
| `entropy` | æ¢ç´¢ç†µ | `[Batch]` | âœ… **ä¿®å¤8** â­ |
| `is_recurrent` | æ˜¯å¦å¾ªç¯ | `bool` | âœ… ä¿®å¤4 |

---

## ç›¸å…³æäº¤

- **Commit**: `a84a23b` - fix: æ·»åŠ entropyå±æ€§ - æ»¡è¶³PPOç®—æ³•æŸå¤±è®¡ç®—è¦æ±‚
- **æ–‡ä»¶ä¿®æ”¹**:
  - `geo_nav_policy.py`: `update_distribution()` æ–¹æ³•
  - æ·»åŠ ï¼š`self.entropy = self.distribution.entropy().sum(dim=-1)`

---

## ç›¸å…³é—®é¢˜

### å‰ç½®é—®é¢˜
1. `2026-01-27_1545_actorcriticå‚æ•°ä¼ é€’å†²çª_TypeError.md` - å…³é”®å­—å‚æ•°ä¿®å¤
2. `2026-01-27_1600_rslrlç‰ˆæœ¬å†²çª_ActorCriticå‚æ•°ç¼ºå¤±.md` - æ–­å¼€ç»§æ‰¿ä¿®å¤
3. `2026-01-27_1610_tensorsdictç±»å‹ä¸åŒ¹é…_ç»´åº¦æ¨æ–­å¤±è´¥.md` - TensorDict æ¥å£é€‚é…
4. `2026-01-27_1620_tensorsdictè¿è¡Œæ—¶æœªè§£åŒ…_IndexError.md` - TensorDict è¿è¡Œæ—¶è§£åŒ…
5. `2026-01-27_1625_action_meanå±æ€§ç¼ºå¤±_AttributeError.md` - action_mean ä¿®å¤
6. `2026-01-27_1630_action_stdå±æ€§ç¼ºå¤±_AttributeError.md` - action_std ä¿®å¤
7. `2026-01-27_1635_update_normalizationæ¥å£ç¼ºå¤±_AttributeError.md` - update_normalization ä¿®å¤

### ä¿®å¤å†å²ï¼ˆ8æ¬¡ä¿®å¤ï¼‰

1. **ä¿®å¤1**ï¼ˆcommit `6e11be3`ï¼‰ï¼šå…³é”®å­—å‚æ•°
2. **ä¿®å¤2**ï¼ˆcommit `63be9d5`ï¼‰ï¼šæ–­å¼€ç»§æ‰¿
3. **ä¿®å¤3**ï¼ˆcommit `dc556e4`ï¼‰ï¼šTensorDict æ¥å£é€‚é…
4. **ä¿®å¤4**ï¼ˆcommit `445518e`ï¼‰ï¼šTensorDict è¿è¡Œæ—¶è§£åŒ…
5. **ä¿®å¤5**ï¼ˆcommit `cf93709`ï¼‰ï¼šaction_mean å±æ€§
6. **ä¿®å¤6**ï¼ˆcommit `3a8af10`ï¼‰ï¼šaction_std å±æ€§
7. **ä¿®å¤7**ï¼ˆcommit `6147c6a`ï¼‰ï¼šupdate_normalization æ¥å£
8. **ä¿®å¤8**ï¼ˆcommit `a84a23b`ï¼‰ï¼šentropy å±æ€§ â­ **ç»ˆæç‰ˆ**

---

## å‚è€ƒèµ„æ–™

### ç†µ (Entropy) çš„æ•°å­¦å®šä¹‰

**ä¿¡æ¯è®ºä¸­çš„ç†µ**ï¼š
```
H(X) = -Î£ p(x) * log(p(x))
```

**è¿ç»­åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰çš„ç†µ**ï¼š
```
H(N(Î¼, ÏƒÂ²)) = 0.5 * log(2Ï€ * e * ÏƒÂ²)
```

**PyTorch è®¡ç®—æ–¹å¼**ï¼š
```python
# å¯¹äº Normal(mean, std)
entropy = distribution.entropy()
# è¿”å›: Tensor[Batch, Actions]

# å¯¹ Actions ç»´åº¦æ±‚å’Œï¼ˆæ¯ä¸ªç¯å¢ƒçš„æ€»ç†µï¼‰
entropy = entropy.sum(dim=-1)
# è¿”å›: Tensor[Batch]
```

### PPO æŸå¤±å‡½æ•°

**æ ‡å‡† PPO Loss**ï¼š
```python
L = L_CLIP + L_VF + c * L_ENTROPY

å…¶ä¸­:
- L_CLIP: Policy Lossï¼ˆè£å‰ªç›®æ ‡ï¼‰
- L_VF: Value Lossï¼ˆä»·å€¼å‡½æ•°ï¼‰
- L_ENTROPY: Entropy Lossï¼ˆæ¢ç´¢æ­£åˆ™åŒ–ï¼‰
- c: entropy_coefï¼ˆç†µç³»æ•°ï¼‰
```

**Entropy Loss**ï¼š
```python
L_ENTROPY = -mean(entropy) * entropy_coef

# è´Ÿå·ï¼šæœ€å°åŒ– Loss = æœ€å¤§åŒ– entropy
# ç»“æœï¼šé¼“åŠ±ä¿æŒé«˜ç†µï¼ˆæ¢ç´¢ï¼‰
```

---

**æ–‡æ¡£ç»´æŠ¤**ï¼šæ­¤é—®é¢˜å·²è§£å†³å¹¶å½’æ¡£
**æœ€åæ›´æ–°**: 2026-01-27 16:40:00
**å½’æ¡£åŸå› **: è¡¥å…¨æœ€åä¸€ä¸ª PPO å±æ€§ä¾èµ–ï¼Œè®­ç»ƒå¯ä»¥æ­£å¸¸è¿è¡Œ
**é‡è¦**: æ¶æ„å¸ˆç¡®è®¤"è¿™æ˜¯æœ€åä¸€ä¸ªå±æ€§"ã€"è·ç¦»æˆåŠŸåªæœ‰ä¸€æ­¥ä¹‹é¥"
**é‡Œç¨‹ç¢‘**: 8æ¬¡ä¿®å¤åï¼ŒRSL-RL å…¼å®¹æ€§é—®é¢˜**å½»åº•è§£å†³** âœ…âœ…âœ…
**æˆå°±**: å®Œæˆé«˜éš¾åº¦æ¶æ„è¿ç§»ï¼Œä» ActorCritic åŸºç±»åˆ°ç‹¬ç«‹ GeoNavPolicy
