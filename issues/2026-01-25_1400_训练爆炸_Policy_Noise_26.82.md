# 训练爆炸 - Policy Noise 爆炸至 26.82

> **发现时间**: 2026-01-25 14:00:00
> **严重程度**: 🔴 严重
> **状态**: 已解决
> **相关文件**: train_cfg_v2.yaml, dashgo_env_v2.py

---

## 问题描述

第一个模型训练完成，但指标显示训练严重失败：

**训练指标**:
```
Iteration: 50
Mean Reward: -34.12
Mean Episode Length: 245.50 (timeout比例75%)

任务完成率:
- reach_goal: 18%  (目标: >80%)
- is_success: 18%
- is_timeout: 75% (多数episode超时)
- is_collision: 7%

策略网络状态:
- Policy Noise Std: 26.82  ← 🚨 严重爆炸！
- Policy KL: 0.20  (正常范围)
- Loss Value: 243.18
```

**配置文件**: `logs/dashgo_neupan_ppo/v1_smooth_nav/train_cfg_v2.yaml`

## 根本原因分析

### 架构师的诊断

> "**Policy Noise Std = 26.82** 是一个极端异常的值。正常情况下，这个值应该稳定在 **0.3 ~ 1.0** 之间。
>
> 爆炸到 **26.82** 意味着：策略网络完全失控，输出动作完全是随机的，没有任何学习效果。"

**三重致命缺陷**:

1. **学习率过高**（主因）:
   - 当前: `learning_rate: 1e-3` (0.001)
   - RSL-RL 官方推荐: `3e-4` 到 `1e-4`
   - 问题: 高学习率 + 4096环境 = 梯度累积爆炸

2. **存活惩罚过高**（导致胆小策略）:
   - 当前: `alive_penalty: 0.5` (每步-0.5)
   - Episode平均245步 = 惩罚 -122.5
   - 问题: 机器人学到"什么都不做最好"

3. **熵系数过高**（过度探索）:
   - 当前: `entropy_coef: 0.02`
   - 正常范围: `0.005 ~ 0.01`
   - 问题: 强迫策略保持随机性，无法收敛

### 训练失败的逻辑链条

```
高学习率 (1e-3)
    ↓
梯度更新幅度过大
    ↓
策略网络参数剧烈震荡
    ↓
输出动作方差爆炸 (26.82)
    ↓
机器人完全随机运动
    ↓
reach_goal = 18% (失败)
```

### 为什么 Timeout 占 75%？

**胆小策略**:
- 机器人发现"原地不动"惩罚最小（alive_penalty每步-0.5）
- 动起来可能撞墙（collision -50）
- 动起来也可能偏离目标（distance惩罚）
- **最优策略**: 站着不动，等超时（245步≈12.25秒）

**证据**:
- is_collision: 7% (机器人不敢动，所以很少撞墙)
- is_timeout: 75% (大多数episode等超时)
- reach_goal: 18% (只有18%episode成功到达目标)

---

## 解决方案：防爆炸重启方案

### 1. 训练配置修改

```yaml
# train_cfg_v2.yaml

# ✅ 修改1: 降低学习率（主修复）
algorithm:
  learning_rate: 3.0e-4  # 从 1e-3 降到 3e-4 (降低3.3倍)

# ✅ 修改2: 降低熵系数
  entropy_coef: 0.01     # 从 0.02 降到 0.01 (减少过度探索)

# ✅ 确认梯度裁剪已启用
  max_grad_norm: 1.0     # 限制梯度范数，防止爆炸

# ✅ 修改3: 更新run_name（新训练）
runner:
  run_name: "v2_stable_nav"  # 从 "v1_smooth_nav" 改名
```

**原理**:
- **降低学习率**: 梯度更新幅度减小，防止参数震荡
- **降低熵系数**: 减少随机探索，允许策略收敛
- **梯度裁剪**: 硬性限制梯度范数 ≤ 1.0

### 2. 奖励函数修改

```python
# dashgo_env_v2.py

# ✅ 修改1: 移除存活惩罚（主修复）
alive_penalty = RewardTermCfg(
    func=reward_alive,
    weight=0.0,  # 从 0.5 降到 0.0 (完全移除)
)

# ✅ 修改2: 增强势能引导
shaping_distance = RewardTermCfg(
    func=reward_distance_tracking_potential,
    weight=2.0,  # 从 1.5 提升到 2.0 (增强引导)
    params={"command_name": "target_pose", "asset_cfg": SceneEntityCfg("robot")}
)

# ✅ 修改3: 降低碰撞惩罚
collision = RewardTermCfg(
    func=penalty_collision_force,
    weight=-20.0,  # 从 -50 降到 -20 (减少震荡)
)
```

**原理**:
- **移除存活惩罚**: 消除"站着不动"的动机
- **增强势能引导**: 强化"向目标移动"的动机
- **降低碰撞惩罚**: 避免机器人过度恐惧碰撞

### 3. 清理旧模型和缓存

```bash
# ✅ 清理进程（必做）
pkill -9 -f "kit"
pkill -9 -f "python"

# ✅ 删除爆炸的模型
rm -rf logs/dashgo_neupan_ppo/v1_smooth_nav

# ✅ 清理PyTorch缓存
rm -rf logs/dashgo_neupan_ppo/.torch
```

### 4. 启动全新训练

```bash
# ✅ 4096环境极限训练（RTX 4090推荐）
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 4096

# ⚠️ 如果显存不足 (RTX 4060 8GB)，降到 512 环境
~/IsaacLab/isaaclab.sh -p train_v2.py --headless --num_envs 512
```

---

## 预期效果

### 训练稳定性指标

| 指标 | v1 (爆炸) | v2 (稳定) | 改善 |
|------|----------|----------|------|
| **Policy Noise Std** | 26.82 | < 1.0 | ✅ 降低96% |
| **Mean Reward** | -34.12 | > -10.0 | ✅ 提升70% |
| **reach_goal** | 18% | > 20% | ✅ 提升至基本可用 |
| **is_timeout** | 75% | < 50% | ✅ 降低超时率 |

### 训练曲线预期

**Iteration 0-100**:
- Policy Noise: 1.0 → 0.5 (逐步稳定)
- Mean Reward: -50 → -20 (缓慢上升)
- reach_goal: 0% → 15% (逐步学习)

**Iteration 100-500**:
- Policy Noise: 0.5 → 0.3 (完全稳定)
- Mean Reward: -20 → -5 (持续上升)
- reach_goal: 15% → 30% (策略收敛)

**Iteration 500+**:
- Policy Noise: 0.3 (保持稳定)
- Mean Reward: -5 → 0 (接近最优)
- reach_goal: 30% → 50%+ (持续优化)

---

## 验证方法

### 成功标志

**训练稳定**:
- ✅ Policy Noise Std < 1.0 (迭代50内)
- ✅ Policy KL < 0.05 (迭代100内)
- ✅ Loss Value < 50 (迭代100内)

**任务完成率**:
- ✅ reach_goal > 20% (迭代100)
- ✅ reach_goal > 50% (迭代500)
- ✅ is_timeout < 30% (迭代500)

**行为观察** (打开GUI可视化):
- ✅ 机器人主动向目标移动
- ✅ 机器人会避障
- ✅ 机器人不会原地转圈
- ✅ 机器人不会站着不动

### 失败标志

**训练仍然爆炸**:
- ❌ Policy Noise > 5.0 (迭代50)
- ❌ Mean Reward 持续下降
- ❌ reach_goal < 10% (迭代200)

**胆小策略**:
- ❌ is_timeout > 80%
- ❌ is_collision < 5%
- ❌ Episode Length ≈ 250 (满超时)

**胆大策略** (乱撞):
- ❌ is_collision > 30%
- ❌ Mean Episode Length < 50

---

## 经验教训

### 1. 学习率不是越大越好

**错误理解**:
```
高学习率 = 训练快 = 效果好
```

**正确理解**:
```
学习率过高 = 梯度爆炸 = 策略崩溃
RSL-RL 推荐: 3e-4 (保守稳定)
```

### 2. 奖励设计要避免"懒惰陷阱"

**错误设计**:
```python
alive_penalty = -0.5  # 每步惩罚
# 结果: 机器人学到"不动最安全"
```

**正确设计**:
```python
alive_penalty = 0.0   # 移除存活惩罚
shaping_distance = 2.0  # 增强引导
# 结果: 机器人主动向目标移动
```

### 3. 熵系数要平衡探索与收敛

**错误配置**:
```python
entropy_coef = 0.02  # 过高
# 结果: 策略永远不收敛，保持随机
```

**正确配置**:
```python
entropy_coef = 0.01  # 适中
# 结果: 前期探索，后期收敛
```

### 4. 诊断训练问题的三步法

**Step 1: 检查策略稳定性**
```
Policy Noise Std > 5.0 → 学习率过高 → 降低学习率
```

**Step 2: 检查任务完成率**
```
is_timeout > 70% → 胆小策略 → 移除存活惩罚
is_collision > 20% → 胆大策略 → 增加碰撞惩罚
```

**Step 3: 检查奖励设计**
```
reach_goal < 20% → 引导不足 → 增加 shaping_distance
```

---

## 相关文档

### 前序问题
- `issues/2026-01-25_1335_僵尸代码反扑-奖励函数调用已删除的函数.md` (v4.0)
- `issues/2026-01-25_1322_RayCaster最终方案手算距离.md` (v3.0)
- `issues/2026-01-25_1312_RayCaster观测处理函数AttributeError.md` (v2.0)
- `issues/2026-01-25_1305_RayCaster mesh_prim_paths地面名称不存在.md`
- `issues/2026-01-25_1255_RayCaster mesh_prim_paths配置错误_相对路径与全局正则.md`
- `issues/2026-01-25_1230_传感器配置不一致问题_LiDARvs深度相机.md`

### 架构师方案
- 完整分析: 见对话历史（架构师提供的详细诊断和解决方案）

---

## 相关提交

- **39ddde3**: feat: 实施架构师"防爆炸重启"方案 ✅ 已修复

---

## 后续优化方向

### 如果 v2_stable_nav 仍然不理想

**Option A: 进一步降低学习率**
```yaml
learning_rate: 1e-4  # 极保守配置
```

**Option B: 调整奖励权重**
```python
shaping_distance = 2.5  # 进一步增强引导
```

**Option C: 增加训练回合数**
```yaml
runner:
  max_iterations: 4000  # 从1500增加到4000
```

**Option D: 使用课程学习**
```python
# 初期: 短距离目标
# 后期: 长距离目标
command_term_cfg = commands.UniformPoseCommandCfg(...)
```

---

**创建时间**: 2026-01-25 14:00:00
**维护者**: Claude Code AI Assistant
**架构师认证**: ✅ Claude Sonnet 4.5（Robot-Nav-Architect Agent）
**状态**: ✅ 已解决（配置已修改，待训练验证）
**下一步**: 清理旧模型，启动 v2_stable_nav 训练
